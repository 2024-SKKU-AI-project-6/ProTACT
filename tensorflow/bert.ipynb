{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e9b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from custom_layers.zeromasking import ZeroMaskedEntries\n",
    "from custom_layers.attention import Attention\n",
    "from custom_layers.multiheadattention_pe import MultiHeadAttention_PE\n",
    "from custom_layers.multiheadattention import MultiHeadAttention\n",
    "\n",
    "def correlation_coefficient(trait1, trait2):\n",
    "    x = trait1\n",
    "    y = trait2\n",
    "    \n",
    "    # maksing if either x or y is a masked value\n",
    "    mask_value = -0.\n",
    "    mask_x = K.cast(K.not_equal(x, mask_value), K.floatx())\n",
    "    mask_y = K.cast(K.not_equal(y, mask_value), K.floatx())\n",
    "    \n",
    "    mask = mask_x * mask_y\n",
    "    x_masked, y_masked = x * mask, y * mask\n",
    "    \n",
    "    mx = K.sum(x_masked) / K.sum(mask) # ignore the masked values when obtaining the mean\n",
    "    my = K.sum(y_masked) / K.sum(mask) # ignore the masked values when obtaining the mean\n",
    "    \n",
    "    xm, ym = (x_masked-mx) * mask, (y_masked-my) * mask # maksing the masked values\n",
    "    \n",
    "    r_num = K.sum(xm * ym)\n",
    "    r_den = K.sqrt(K.sum(K.square(xm)) * K.sum(K.square(ym)))\n",
    "    r = 0.\n",
    "    r = tf.cond(r_den > 0, lambda: r_num / (r_den), lambda: r+0)\n",
    "    return r\n",
    "\n",
    "def cosine_sim(trait1, trait2):\n",
    "    x = trait1\n",
    "    y = trait2\n",
    "    \n",
    "    mask_value = 0.\n",
    "    mask_x = K.cast(K.not_equal(x, mask_value), K.floatx())\n",
    "    mask_y = K.cast(K.not_equal(y, mask_value), K.floatx())\n",
    "    \n",
    "    mask = mask_x * mask_y\n",
    "    x_masked, y_masked = x*mask, y*mask\n",
    "    \n",
    "    normalize_x = tf.nn.l2_normalize(x_masked,0) * mask # mask 값 반영     \n",
    "    normalize_y = tf.nn.l2_normalize(y_masked,0) * mask # mask 값 반영\n",
    "        \n",
    "    cos_similarity = tf.reduce_sum(tf.multiply(normalize_x, normalize_y))\n",
    "    return cos_similarity\n",
    "    \n",
    "def trait_sim_loss(y_true, y_pred):\n",
    "    mask_value = -1\n",
    "    mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "    \n",
    "    # masking\n",
    "    y_trans = tf.transpose(y_true * mask)\n",
    "    y_pred_trans = tf.transpose(y_pred * mask)\n",
    "    \n",
    "    sim_loss = 0.0\n",
    "    cnt = 0.0\n",
    "    ts_loss = 0.\n",
    "    #trait_num = y_true.shape[1]\n",
    "    trait_num = 9\n",
    "    print('trait num: ', trait_num)\n",
    "    \n",
    "    # start from idx 1, since we ignore the overall score \n",
    "    for i in range(1, trait_num):\n",
    "        for j in range(i+1, trait_num):\n",
    "            corr = correlation_coefficient(y_trans[i], y_trans[j])\n",
    "            sim_loss = tf.cond(corr>=0.7, lambda: tf.add(sim_loss, 1-cosine_sim(y_pred_trans[i], y_pred_trans[j])), \n",
    "                            lambda: tf.add(sim_loss, 0))\n",
    "            cnt = tf.cond(corr>=0.7, lambda: tf.add(cnt, 1), \n",
    "                            lambda: tf.add(cnt, 0))\n",
    "    ts_loss = tf.cond(cnt > 0, lambda: sim_loss/cnt, lambda: ts_loss+0)\n",
    "    return ts_loss\n",
    "    \n",
    "def masked_loss_function(y_true, y_pred):\n",
    "    mask_value = -1\n",
    "    mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "    mse = keras.losses.MeanSquaredError()\n",
    "    return mse(y_true * mask, y_pred * mask)\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    alpha = 0.7\n",
    "    mse_loss = masked_loss_function(y_true, y_pred)\n",
    "    ts_loss = trait_sim_loss(y_true, y_pred)\n",
    "    return alpha * mse_loss + (1-alpha) * ts_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ProTACT(pos_vocab_size, vocab_size, maxnum, maxlen, readability_feature_count,\n",
    "              linguistic_feature_count, configs, output_dim, num_heads, embedding_weights):\n",
    "    embedding_dim = configs.EMBEDDING_DIM # 50\n",
    "    dropout_prob = configs.DROPOUT #  0.5\n",
    "    cnn_filters = configs.CNN_FILTERS # 100\n",
    "    cnn_kernel_size = configs.CNN_KERNEL_SIZE # 5\n",
    "    lstm_units = configs.LSTM_UNITS # 100\n",
    "    num_heads = num_heads # 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 1. Essay Representation\n",
    "    pos_input = layers.Input(shape=(maxnum*maxlen,), dtype='int32', name='pos_input')\n",
    "    # pos_x = layers.Embedding(output_dim=embedding_dim, input_dim=pos_vocab_size, input_length=maxnum*maxlen,\n",
    "    #                          weights=None, mask_zero=True, name='pos_x')(pos_input)\n",
    "    # pos_x_maskedout = ZeroMaskedEntries(name='pos_x_maskedout')(pos_x)\n",
    "    # pos_drop_x = layers.Dropout(dropout_prob, name='pos_drop_x')(pos_x_maskedout)\n",
    "    # pos_resh_W = layers.Reshape((maxnum, maxlen, embedding_dim), name='pos_resh_W')(pos_drop_x) # (97, 50, 50)\n",
    "    # pos_zcnn = layers.TimeDistributed(layers.Conv1D(cnn_filters, cnn_kernel_size, padding='valid'), name='pos_zcnn')(pos_resh_W)\n",
    "    # pos_avg_zcnn = layers.TimeDistributed(Attention(), name='pos_avg_zcnn')(pos_zcnn)\n",
    "\n",
    "    linguistic_input = layers.Input((linguistic_feature_count,), name='linguistic_input')\n",
    "    readability_input = layers.Input((readability_feature_count,), name='readability_input')\n",
    "\n",
    "    pos_MA_list = [MultiHeadAttention(100,num_heads)(pos_avg_zcnn) for _ in range(output_dim)]\n",
    "    pos_MA_lstm_list = [layers.LSTM(lstm_units, return_sequences=True)(pos_MA) for pos_MA in pos_MA_list] \n",
    "    pos_avg_MA_lstm_list = [Attention()(pos_hz_lstm) for pos_hz_lstm in pos_MA_lstm_list] \n",
    "\n",
    "    ### 2. Prompt Representation\n",
    "    # word embedding\n",
    "    prompt_word_input = layers.Input(shape=(maxnum*maxlen,), dtype='int32', name='prompt_word_input')\n",
    "    # prompt = layers.Embedding(output_dim=embedding_dim, input_dim=vocab_size, input_length=maxnum*maxlen,\n",
    "    #                          weights=embedding_weights, mask_zero=True, name='prompt')(prompt_word_input)\n",
    "    # prompt_maskedout = ZeroMaskedEntries(name='prompt_maskedout')(prompt)\n",
    "\n",
    "    # # pos embedding\n",
    "    # prompt_pos_input = layers.Input(shape=(maxnum*maxlen,), dtype='int32', name='prompt_pos_input')\n",
    "    # prompt_pos = layers.Embedding(output_dim=embedding_dim, input_dim=pos_vocab_size, input_length=maxnum*maxlen,\n",
    "    #                          weights=None, mask_zero=True, name='pos_prompt')(prompt_pos_input)\n",
    "    # prompt_pos_maskedout = ZeroMaskedEntries(name='prompt_pos_maskedout')(prompt_pos) \n",
    "    \n",
    "    # # add word + pos embedding\n",
    "    # prompt_emb = tf.keras.layers.Add()([prompt_maskedout, prompt_pos_maskedout])\n",
    "    \n",
    "    # prompt_drop_x = layers.Dropout(dropout_prob, name='prompt_drop_x')(prompt_emb)\n",
    "    # prompt_resh_W = layers.Reshape((maxnum, maxlen, embedding_dim), name='prompt_resh_W')(prompt_drop_x)\n",
    "    # prompt_zcnn = layers.TimeDistributed(layers.Conv1D(cnn_filters, cnn_kernel_size, padding='valid'), name='prompt_zcnn')(prompt_resh_W)\n",
    "    # prompt_avg_zcnn = layers.TimeDistributed(Attention(), name='prompt_avg_zcnn')(prompt_zcnn)\n",
    "    \n",
    "    prompt_MA_list = MultiHeadAttention(100, num_heads)(prompt_avg_zcnn)\n",
    "    prompt_MA_lstm_list = layers.LSTM(lstm_units, return_sequences=True)(prompt_MA_list) \n",
    "    prompt_avg_MA_lstm_list = Attention()(prompt_MA_lstm_list)\n",
    "    \n",
    "    query = prompt_avg_MA_lstm_list\n",
    "\n",
    "    # 여기에서 합쳐진다.\n",
    "    es_pr_MA_list = [MultiHeadAttention_PE(100,num_heads)(pos_avg_MA_lstm_list[i], query) for i in range(output_dim)]\n",
    "    es_pr_MA_lstm_list = [layers.LSTM(lstm_units, return_sequences=True)(pos_hz_MA) for pos_hz_MA in es_pr_MA_list]\n",
    "    es_pr_avg_lstm_list = [Attention()(pos_hz_lstm) for pos_hz_lstm in es_pr_MA_lstm_list]\n",
    "    es_pr_feat_concat = [layers.Concatenate()([rep, linguistic_input, readability_input]) # concatenate with non-prompt-specific features\n",
    "                                 for rep in es_pr_avg_lstm_list]\n",
    "    pos_avg_hz_lstm = tf.concat([layers.Reshape((1, lstm_units + linguistic_feature_count + readability_feature_count))(rep)\n",
    "                                 for rep in es_pr_feat_concat], axis=-2)\n",
    "\n",
    "    # 1-9 까지의 traits 을 중심으로 점수 예측을 한다.\n",
    "    final_preds = []\n",
    "    for index, rep in enumerate(range(output_dim)):\n",
    "        mask = np.array([True for _ in range(output_dim)])\n",
    "        mask[index] = False\n",
    "        non_target_rep = tf.boolean_mask(pos_avg_hz_lstm, mask, axis=-2)\n",
    "        target_rep = pos_avg_hz_lstm[:, index:index+1]\n",
    "        att_attention = layers.Attention()([target_rep, non_target_rep])\n",
    "        attention_concat = tf.concat([target_rep, att_attention], axis=-1)\n",
    "        attention_concat = layers.Flatten()(attention_concat)\n",
    "        final_pred = layers.Dense(units=1, activation='sigmoid')(attention_concat)\n",
    "        final_preds.append(final_pred)\n",
    "\n",
    "    y = layers.Concatenate()([pred for pred in final_preds])\n",
    "\n",
    "    model = keras.Model(inputs=[pos_input, prompt_word_input, prompt_pos_input, linguistic_input, readability_input], outputs=y)\n",
    "    model.summary()\n",
    "    model.compile(loss=total_loss, optimizer='rmsprop')\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
