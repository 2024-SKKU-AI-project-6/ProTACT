{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/joohwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joohwan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload behavior\n",
    "%autoreload 2\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models.ProTACT import ProTACT\n",
    "from models.Loss import LossFunctions\n",
    "from configs.configs import Configs\n",
    "from utils.read_data_pr import read_pos_vocab, read_word_vocab, read_prompts_we, read_essays_prompts, read_prompts_pos\n",
    "from utils.general_utils import get_scaled_down_scores, pad_hierarchical_text_sequences, get_attribute_masks, load_word_embedding_dict, build_embedd_table\n",
    "from evaluators.multitask_evaluator_all_attributes import Evaluator as AllAttEvaluator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Test prompt id is 1 of type <class 'int'>\n",
      "Seed: 1\n",
      "Numhead :  2  | Features :  ../data/hand_crafted_v3.csv  | Pos_emb :  50\n"
     ]
    }
   ],
   "source": [
    "# print(\"main started\")\n",
    "# parser = argparse.ArgumentParser(description=\"ProTACT model\")\n",
    "# parser.add_argument('--test_prompt_id', type=int,\n",
    "#                     default=1, help='prompt id of test essay set')\n",
    "# parser.add_argument('--seed', type=int, default=12, help='set random seed')\n",
    "# parser.add_argument('--model_name', type=str,\n",
    "#                     choices=['ProTACT'],\n",
    "#                     help='name of model')\n",
    "# parser.add_argument('--num_heads', type=int, default=2,\n",
    "#                     help='set the number of heads in Multihead Attention')\n",
    "# parser.add_argument('--features_path', type=str,\n",
    "#                     default='data/hand_crafted_v3.csv')\n",
    "test_prompt_id = 1\n",
    "seed = 1\n",
    "num_heads = 2\n",
    "features_path = '../data/hand_crafted_v3.csv'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Test prompt id is {} of type {}\".format(\n",
    "    test_prompt_id, type(test_prompt_id)))\n",
    "print(\"Seed: {}\".format(seed))\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "data_path = configs.DATA_PATH\n",
    "train_path = data_path + str(test_prompt_id) + '/train.pk'\n",
    "dev_path = data_path + str(test_prompt_id) + '/dev.pk'\n",
    "test_path = data_path + str(test_prompt_id) + '/test.pk'\n",
    "pretrained_embedding = configs.PRETRAINED_EMBEDDING\n",
    "embedding_path = configs.EMBEDDING_PATH\n",
    "readability_path = configs.READABILITY_PATH\n",
    "prompt_path = configs.PROMPT_PATH\n",
    "vocab_size = configs.VOCAB_SIZE\n",
    "epochs = configs.EPOCHS\n",
    "batch_size = configs.BATCH_SIZE\n",
    "print(\"Numhead : \", num_heads, \" | Features : \",\n",
    "      features_path, \" | Pos_emb : \", configs.EMBEDDING_DIM)\n",
    "\n",
    "read_configs = {\n",
    "    'train_path': train_path,\n",
    "    'dev_path': dev_path,\n",
    "    'test_path': test_path,\n",
    "    'features_path': features_path,\n",
    "    'readability_path': readability_path,\n",
    "    'vocab_size': vocab_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prompt_pos size: 8\n",
      " prompt_words size: 8\n",
      " pos_x size: 9513\n",
      " readability_x size: 9513\n",
      " pos_x size: 1680\n",
      " readability_x size: 1680\n",
      " pos_x size: 1783\n",
      " readability_x size: 1783\n",
      "Loading GloVe ...\n",
      "OOV number =189, OOV ratio = 0.047262\n"
     ]
    }
   ],
   "source": [
    "# read POS for prompts\n",
    "pos_vocab = read_pos_vocab(read_configs)\n",
    "prompt_pos_data = read_prompts_pos(\n",
    "    prompt_path, pos_vocab)  # for prompt POS embedding\n",
    "\n",
    "# read words for prompts\n",
    "word_vocab = read_word_vocab(read_configs)\n",
    "# for prompt word embedding\n",
    "prompt_data = read_prompts_we(prompt_path, word_vocab)\n",
    "\n",
    "train_data, dev_data, test_data = read_essays_prompts(\n",
    "    read_configs, prompt_data, prompt_pos_data, pos_vocab)\n",
    "\n",
    "if pretrained_embedding:\n",
    "    embedd_dict, embedd_dim, _ = load_word_embedding_dict(embedding_path)\n",
    "    embedd_matrix = build_embedd_table(\n",
    "        word_vocab, embedd_dict, embedd_dim, caseless=True)\n",
    "    embed_table = embedd_matrix\n",
    "else:\n",
    "    embed_table = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sent length: 50\n",
      "max sent num: 97\n",
      "max prompt sent length: 18\n",
      "max prompt sent num: 8\n",
      "================================\n",
      "X_train_pos:  (9513, 4850)\n",
      "X_train_prompt_words:  (9513, 4850)\n",
      "X_train_prompt_pos:  (9513, 4850)\n",
      "X_train_readability:  (9513, 35)\n",
      "X_train_ling:  (9513, 51)\n",
      "X_train_attribute_rel:  (9513, 9)\n",
      "Y_train:  (9513, 9)\n",
      "================================\n",
      "X_dev_pos:  (1680, 4850)\n",
      "X_dev_prompt_words:  (1680, 4850)\n",
      "X_dev_prompt_pos:  (1680, 4850)\n",
      "X_dev_readability:  (1680, 35)\n",
      "X_dev_ling:  (1680, 51)\n",
      "X_dev_attribute_rel:  (1680, 9)\n",
      "Y_dev:  (1680, 9)\n",
      "================================\n",
      "X_test_pos:  (1783, 4850)\n",
      "X_test_prompt_words:  (1783, 4850)\n",
      "X_test_prompt_pos:  (1783, 4850)\n",
      "X_test_readability:  (1783, 35)\n",
      "X_test_ling:  (1783, 51)\n",
      "X_test_attribute_rel:  (1783, 9)\n",
      "Y_test:  (1783, 9)\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "max_sentlen = max(train_data['max_sentlen'],\n",
    "                  dev_data['max_sentlen'], test_data['max_sentlen'])\n",
    "max_sentnum = max(train_data['max_sentnum'],\n",
    "                  dev_data['max_sentnum'], test_data['max_sentnum'])\n",
    "prompt_max_sentlen = prompt_data['max_sentlen']\n",
    "prompt_max_sentnum = prompt_data['max_sentnum']\n",
    "\n",
    "print('max sent length: {}'.format(max_sentlen))\n",
    "print('max sent num: {}'.format(max_sentnum))\n",
    "print('max prompt sent length: {}'.format(prompt_max_sentlen))\n",
    "print('max prompt sent num: {}'.format(prompt_max_sentnum))\n",
    "\n",
    "train_data['y_scaled'] = get_scaled_down_scores(\n",
    "    train_data['data_y'], train_data['prompt_ids'])\n",
    "dev_data['y_scaled'] = get_scaled_down_scores(\n",
    "    dev_data['data_y'], dev_data['prompt_ids'])\n",
    "test_data['y_scaled'] = get_scaled_down_scores(\n",
    "    test_data['data_y'], test_data['prompt_ids'])\n",
    "\n",
    "X_train_pos = pad_hierarchical_text_sequences(\n",
    "    train_data['pos_x'], max_sentnum, max_sentlen)\n",
    "X_dev_pos = pad_hierarchical_text_sequences(\n",
    "    dev_data['pos_x'], max_sentnum, max_sentlen)\n",
    "X_test_pos = pad_hierarchical_text_sequences(\n",
    "    test_data['pos_x'], max_sentnum, max_sentlen)\n",
    "\n",
    "X_train_pos = X_train_pos.reshape(\n",
    "    (X_train_pos.shape[0], X_train_pos.shape[1] * X_train_pos.shape[2]))\n",
    "X_dev_pos = X_dev_pos.reshape(\n",
    "    (X_dev_pos.shape[0], X_dev_pos.shape[1] * X_dev_pos.shape[2]))\n",
    "X_test_pos = X_test_pos.reshape(\n",
    "    (X_test_pos.shape[0], X_test_pos.shape[1] * X_test_pos.shape[2]))\n",
    "\n",
    "X_train_prompt = pad_hierarchical_text_sequences(\n",
    "    train_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "X_dev_prompt = pad_hierarchical_text_sequences(\n",
    "    dev_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "X_test_prompt = pad_hierarchical_text_sequences(\n",
    "    test_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "\n",
    "X_train_prompt = X_train_prompt.reshape(\n",
    "    (X_train_prompt.shape[0], X_train_prompt.shape[1] * X_train_prompt.shape[2]))\n",
    "X_dev_prompt = X_dev_prompt.reshape(\n",
    "    (X_dev_prompt.shape[0], X_dev_prompt.shape[1] * X_dev_prompt.shape[2]))\n",
    "X_test_prompt = X_test_prompt.reshape(\n",
    "    (X_test_prompt.shape[0], X_test_prompt.shape[1] * X_test_prompt.shape[2]))\n",
    "\n",
    "X_train_prompt_pos = pad_hierarchical_text_sequences(\n",
    "    train_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "X_dev_prompt_pos = pad_hierarchical_text_sequences(\n",
    "    dev_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "X_test_prompt_pos = pad_hierarchical_text_sequences(\n",
    "    test_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "\n",
    "X_train_prompt_pos = X_train_prompt_pos.reshape(\n",
    "    (X_train_prompt_pos.shape[0], X_train_prompt_pos.shape[1] * X_train_prompt_pos.shape[2]))\n",
    "X_dev_prompt_pos = X_dev_prompt_pos.reshape(\n",
    "    (X_dev_prompt_pos.shape[0], X_dev_prompt_pos.shape[1] * X_dev_prompt_pos.shape[2]))\n",
    "X_test_prompt_pos = X_test_prompt_pos.reshape(\n",
    "    (X_test_prompt_pos.shape[0], X_test_prompt_pos.shape[1] * X_test_prompt_pos.shape[2]))\n",
    "\n",
    "X_train_linguistic_features = np.array(train_data['features_x'])\n",
    "X_dev_linguistic_features = np.array(dev_data['features_x'])\n",
    "X_test_linguistic_features = np.array(test_data['features_x'])\n",
    "\n",
    "X_train_readability = np.array(train_data['readability_x'])\n",
    "X_dev_readability = np.array(dev_data['readability_x'])\n",
    "X_test_readability = np.array(test_data['readability_x'])\n",
    "\n",
    "Y_train = np.array(train_data['y_scaled'])\n",
    "Y_dev = np.array(dev_data['y_scaled'])\n",
    "Y_test = np.array(test_data['y_scaled'])\n",
    "\n",
    "X_train_attribute_rel = get_attribute_masks(Y_train)\n",
    "X_dev_attribute_rel = get_attribute_masks(Y_dev)\n",
    "X_test_attribute_rel = get_attribute_masks(Y_test)\n",
    "\n",
    "print('================================')\n",
    "print('X_train_pos: ', X_train_pos.shape)\n",
    "print('X_train_prompt_words: ', X_train_prompt.shape)\n",
    "print('X_train_prompt_pos: ', X_train_prompt_pos.shape)\n",
    "print('X_train_readability: ', X_train_readability.shape)\n",
    "print('X_train_ling: ', X_train_linguistic_features.shape)\n",
    "print('X_train_attribute_rel: ', X_train_attribute_rel.shape)\n",
    "print('Y_train: ', Y_train.shape)\n",
    "\n",
    "print('================================')\n",
    "print('X_dev_pos: ', X_dev_pos.shape)\n",
    "print('X_dev_prompt_words: ', X_dev_prompt.shape)\n",
    "print('X_dev_prompt_pos: ', X_dev_prompt_pos.shape)\n",
    "print('X_dev_readability: ', X_dev_readability.shape)\n",
    "print('X_dev_ling: ', X_dev_linguistic_features.shape)\n",
    "print('X_dev_attribute_rel: ', X_dev_attribute_rel.shape)\n",
    "print('Y_dev: ', Y_dev.shape)\n",
    "\n",
    "print('================================')\n",
    "print('X_test_pos: ', X_test_pos.shape)\n",
    "print('X_test_prompt_words: ', X_test_prompt.shape)\n",
    "print('X_test_prompt_pos: ', X_test_prompt_pos.shape)\n",
    "print('X_test_readability: ', X_test_readability.shape)\n",
    "print('X_test_ling: ', X_test_linguistic_features.shape)\n",
    "print('X_test_attribute_rel: ', X_test_attribute_rel.shape)\n",
    "print('Y_test: ', Y_test.shape)\n",
    "print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to torch tensor\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train_pos),\n",
    "    torch.from_numpy(X_train_prompt),\n",
    "    torch.from_numpy(X_train_prompt_pos),\n",
    "    torch.from_numpy(X_train_linguistic_features),\n",
    "    torch.from_numpy(X_train_readability),\n",
    "    torch.from_numpy(Y_train)\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_dev_pos),\n",
    "    torch.from_numpy(X_dev_prompt),\n",
    "    torch.from_numpy(X_dev_prompt_pos),\n",
    "    torch.from_numpy(X_dev_linguistic_features),\n",
    "    torch.from_numpy(X_dev_readability),\n",
    "    torch.from_numpy(Y_dev)\n",
    ")\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "dev_features_list = [\n",
    "    torch.from_numpy(X_dev_pos),\n",
    "    torch.from_numpy(X_dev_prompt),\n",
    "    torch.from_numpy(X_dev_prompt_pos),\n",
    "    torch.from_numpy(X_dev_linguistic_features),\n",
    "    torch.from_numpy(X_dev_readability)\n",
    "]\n",
    "test_features_list = [\n",
    "    torch.from_numpy(X_test_pos),\n",
    "    torch.from_numpy(X_test_prompt),\n",
    "    torch.from_numpy(X_test_prompt_pos),\n",
    "    torch.from_numpy(X_test_linguistic_features),\n",
    "    torch.from_numpy(X_test_readability)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.essay_pos_attention_list ModuleList(\n",
      "  (0-96): 97 x Attention()\n",
      ")\n",
      "Layer: essay_pos_embedding.weight | Size: torch.Size([36, 50])\n",
      "Layer: essay_pos_conv.weight | Size: torch.Size([100, 50, 5])\n",
      "Layer: essay_pos_conv.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.0.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.0.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.1.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.1.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.2.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.2.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.3.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.3.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.4.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.4.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.5.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.5.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.6.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.6.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.7.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.7.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.8.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.8.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.9.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.9.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.10.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.10.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.11.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.11.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.12.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.12.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.13.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.13.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.14.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.14.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.15.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.15.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.16.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.16.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.17.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.17.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.18.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.18.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.19.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.19.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.20.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.20.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.21.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.21.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.22.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.22.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.23.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.23.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.24.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.24.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.25.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.25.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.26.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.26.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.27.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.27.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.28.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.28.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.29.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.29.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.30.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.30.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.31.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.31.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.32.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.32.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.33.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.33.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.34.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.34.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.35.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.35.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.36.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.36.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.37.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.37.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.38.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.38.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.39.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.39.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.40.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.40.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.41.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.41.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.42.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.42.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.43.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.43.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.44.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.44.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.45.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.45.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.46.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.46.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.47.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.47.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.48.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.48.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.49.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.49.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.50.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.50.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.51.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.51.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.52.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.52.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.53.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.53.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.54.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.54.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.55.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.55.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.56.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.56.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.57.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.57.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.58.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.58.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.59.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.59.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.60.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.60.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.61.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.61.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.62.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.62.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.63.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.63.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.64.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.64.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.65.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.65.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.66.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.66.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.67.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.67.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.68.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.68.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.69.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.69.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.70.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.70.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.71.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.71.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.72.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.72.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.73.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.73.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.74.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.74.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.75.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.75.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.76.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.76.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.77.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.77.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.78.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.78.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.79.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.79.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.80.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.80.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.81.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.81.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.82.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.82.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.83.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.83.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.84.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.84.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.85.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.85.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.86.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.86.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.87.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.87.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.88.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.88.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.89.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.89.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.90.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.90.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.91.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.91.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.92.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.92.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.93.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.93.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.94.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.94.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.95.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.95.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_attention_list.96.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_attention_list.96.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.0.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.0.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.0.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.0.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.0.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.0.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.0.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.0.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.1.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.1.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.1.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.1.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.1.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.1.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.1.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.1.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.2.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.2.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.2.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.2.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.2.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.2.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.2.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.2.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.3.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.3.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.3.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.3.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.3.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.3.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.3.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.3.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.4.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.4.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.4.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.4.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.4.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.4.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.4.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.4.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.5.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.5.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.5.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.5.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.5.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.5.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.5.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.5.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.6.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.6.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.6.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.6.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.6.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.6.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.6.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.6.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.7.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.7.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.7.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.7.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.7.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.7.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.7.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.7.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.8.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.8.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.8.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.8.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.8.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.8.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA.8.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_MA.8.dense.bias | Size: torch.Size([100])\n",
      "Layer: essay_pos_MA_LSTM.0.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.0.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.0.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.0.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.1.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.1.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.1.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.1.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.2.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.2.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.2.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.2.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.3.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.3.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.3.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.3.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.4.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.4.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.4.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.4.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.5.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.5.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.5.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.5.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.6.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.6.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.6.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.6.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.7.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.7.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.7.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.7.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.8.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.8.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: essay_pos_MA_LSTM.8.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_MA_LSTM.8.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: essay_pos_avg_MA_LSTM.0.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.0.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.1.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.1.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.2.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.2.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.3.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.3.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.4.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.4.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.5.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.5.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.6.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.6.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.7.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.7.att_W | Size: torch.Size([100, 100])\n",
      "Layer: essay_pos_avg_MA_LSTM.8.att_V | Size: torch.Size([100])\n",
      "Layer: essay_pos_avg_MA_LSTM.8.att_W | Size: torch.Size([100, 100])\n",
      "Layer: prompt_embedding.weight | Size: torch.Size([4000, 50])\n",
      "Layer: prompt_pos_embedding.weight | Size: torch.Size([36, 50])\n",
      "Layer: prompt_conv.weight | Size: torch.Size([100, 50, 5])\n",
      "Layer: prompt_conv.bias | Size: torch.Size([100])\n",
      "Layer: prompt_attention.att_V | Size: torch.Size([100])\n",
      "Layer: prompt_attention.att_W | Size: torch.Size([100, 100])\n",
      "Layer: prompt_MA.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: prompt_MA.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: prompt_MA.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: prompt_MA.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: prompt_MA.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: prompt_MA.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: prompt_MA.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: prompt_MA.dense.bias | Size: torch.Size([100])\n",
      "Layer: prompt_MA_lstm.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: prompt_MA_lstm.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: prompt_MA_lstm.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: prompt_MA_lstm.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: prompt_avg_MA_lstm.att_V | Size: torch.Size([100])\n",
      "Layer: prompt_avg_MA_lstm.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.0.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.0.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.0.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.0.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.0.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.0.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.0.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.0.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.1.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.1.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.1.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.1.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.1.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.1.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.1.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.1.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.2.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.2.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.2.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.2.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.2.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.2.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.2.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.2.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.3.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.3.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.3.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.3.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.3.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.3.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.3.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.3.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.4.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.4.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.4.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.4.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.4.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.4.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.4.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.4.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.5.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.5.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.5.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.5.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.5.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.5.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.5.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.5.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.6.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.6.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.6.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.6.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.6.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.6.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.6.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.6.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.7.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.7.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.7.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.7.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.7.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.7.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.7.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.7.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.8.query_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.8.query_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.8.key_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.8.key_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.8.value_dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.8.value_dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_list.8.dense.weight | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_MA_list.8.dense.bias | Size: torch.Size([100])\n",
      "Layer: es_pr_MA_lstm_list.0.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.0.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.0.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.0.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.1.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.1.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.1.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.1.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.2.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.2.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.2.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.2.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.3.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.3.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.3.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.3.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.4.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.4.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.4.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.4.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.5.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.5.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.5.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.5.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.6.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.6.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.6.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.6.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.7.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.7.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.7.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.7.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.8.weight_ih_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.8.weight_hh_l0 | Size: torch.Size([400, 100])\n",
      "Layer: es_pr_MA_lstm_list.8.bias_ih_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_MA_lstm_list.8.bias_hh_l0 | Size: torch.Size([400])\n",
      "Layer: es_pr_avg_lstm_list.0.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.0.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.1.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.1.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.2.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.2.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.3.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.3.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.4.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.4.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.5.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.5.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.6.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.6.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.7.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.7.att_W | Size: torch.Size([100, 100])\n",
      "Layer: es_pr_avg_lstm_list.8.att_V | Size: torch.Size([100])\n",
      "Layer: es_pr_avg_lstm_list.8.att_W | Size: torch.Size([100, 100])\n",
      "Layer: att_attention_list.0.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.0.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.0.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.0.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.1.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.1.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.1.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.1.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.2.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.2.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.2.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.2.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.3.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.3.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.3.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.3.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.4.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.4.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.4.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.4.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.5.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.5.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.5.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.5.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.6.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.6.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.6.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.6.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.7.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.7.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.7.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.7.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: att_attention_list.8.in_proj_weight | Size: torch.Size([558, 186])\n",
      "Layer: att_attention_list.8.in_proj_bias | Size: torch.Size([558])\n",
      "Layer: att_attention_list.8.out_proj.weight | Size: torch.Size([186, 186])\n",
      "Layer: att_attention_list.8.out_proj.bias | Size: torch.Size([186])\n",
      "Layer: final_dense_list.0.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.0.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.1.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.1.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.2.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.2.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.3.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.3.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.4.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.4.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.5.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.5.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.6.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.6.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.7.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.7.bias | Size: torch.Size([1])\n",
      "Layer: final_dense_list.8.weight | Size: torch.Size([1, 372])\n",
      "Layer: final_dense_list.8.bias | Size: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = ProTACT(\n",
    "    len(pos_vocab), len(word_vocab), max_sentnum, max_sentlen,\n",
    "    X_train_readability.shape[1], X_train_linguistic_features.shape[1],\n",
    "    configs, Y_train.shape[1], num_heads, embed_table\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = LossFunctions(alpha=0.7)\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    model.parameters(), lr=configs.LEARNING_RATE)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_drop_x:  torch.Size([1680, 4850, 50])\n",
      "pos_resh_W torch.Size([1680, 97, 50, 50])\n",
      "pos_zcnn torch.Size([1680, 97, 46, 100])\n",
      "pos_avg_zcnn torch.Size([1680, 97, 100])\n",
      "prompt_zcnn torch.Size([1680, 97, 46, 100])\n",
      "prompt_avg_zcnn torch.Size([1680, 97, 100])\n",
      "prompt_MA_lstm shape:  torch.Size([1680, 97, 100])\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0233, -0.0094,  0.0141,  ..., -0.1457, -0.0378, -0.0204],\n",
      "          [-0.0453, -0.0844, -0.0589,  ..., -0.0302,  0.0033,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0232, -0.0092,  0.0138,  ..., -0.1458, -0.0377, -0.0206],\n",
      "          [-0.0454, -0.0845, -0.0590,  ..., -0.0301,  0.0033,  0.0022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0231, -0.0092,  0.0136,  ..., -0.1459, -0.0377, -0.0208],\n",
      "          [-0.0454, -0.0843, -0.0591,  ..., -0.0301,  0.0033,  0.0023]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0230, -0.0092,  0.0135,  ..., -0.1458, -0.0378, -0.0208],\n",
      "          [-0.0454, -0.0843, -0.0591,  ..., -0.0301,  0.0033,  0.0024]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0231, -0.0092,  0.0134,  ..., -0.1458, -0.0379, -0.0207],\n",
      "          [-0.0454, -0.0843, -0.0591,  ..., -0.0302,  0.0031,  0.0024]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0231, -0.0092,  0.0136,  ..., -0.1457, -0.0378, -0.0207],\n",
      "          [-0.0454, -0.0846, -0.0590,  ..., -0.0301,  0.0032,  0.0023]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0873, -0.0715, -0.0726,  ...,  0.0146, -0.0316,  0.0203],\n",
      "          [ 0.0428, -0.0760, -0.0031,  ..., -0.0159, -0.0196,  0.0816]]],\n",
      "\n",
      "\n",
      "        [[[-0.0872, -0.0717, -0.0724,  ...,  0.0144, -0.0316,  0.0202],\n",
      "          [ 0.0429, -0.0759, -0.0031,  ..., -0.0159, -0.0195,  0.0816]]],\n",
      "\n",
      "\n",
      "        [[[-0.0869, -0.0717, -0.0721,  ...,  0.0140, -0.0317,  0.0201],\n",
      "          [ 0.0429, -0.0754, -0.0031,  ..., -0.0160, -0.0192,  0.0816]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0869, -0.0719, -0.0721,  ...,  0.0139, -0.0317,  0.0201],\n",
      "          [ 0.0431, -0.0754, -0.0031,  ..., -0.0159, -0.0193,  0.0817]]],\n",
      "\n",
      "\n",
      "        [[[-0.0870, -0.0718, -0.0722,  ...,  0.0140, -0.0316,  0.0201],\n",
      "          [ 0.0431, -0.0756, -0.0030,  ..., -0.0159, -0.0194,  0.0816]]],\n",
      "\n",
      "\n",
      "        [[[-0.0871, -0.0717, -0.0722,  ...,  0.0142, -0.0317,  0.0201],\n",
      "          [ 0.0429, -0.0758, -0.0031,  ..., -0.0160, -0.0194,  0.0816]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0422,  0.0960, -0.0448,  ..., -0.0634, -0.0926,  0.0798],\n",
      "          [-0.0989,  0.0867, -0.0789,  ...,  0.0343, -0.0361, -0.0286]]],\n",
      "\n",
      "\n",
      "        [[[-0.0420,  0.0959, -0.0447,  ..., -0.0634, -0.0921,  0.0798],\n",
      "          [-0.0987,  0.0865, -0.0786,  ...,  0.0345, -0.0363, -0.0284]]],\n",
      "\n",
      "\n",
      "        [[[-0.0417,  0.0957, -0.0444,  ..., -0.0634, -0.0919,  0.0799],\n",
      "          [-0.0987,  0.0864, -0.0783,  ...,  0.0347, -0.0364, -0.0284]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0416,  0.0958, -0.0444,  ..., -0.0634, -0.0918,  0.0800],\n",
      "          [-0.0986,  0.0863, -0.0782,  ...,  0.0346, -0.0365, -0.0284]]],\n",
      "\n",
      "\n",
      "        [[[-0.0418,  0.0959, -0.0446,  ..., -0.0633, -0.0918,  0.0798],\n",
      "          [-0.0986,  0.0864, -0.0784,  ...,  0.0345, -0.0365, -0.0284]]],\n",
      "\n",
      "\n",
      "        [[[-0.0419,  0.0960, -0.0447,  ..., -0.0634, -0.0918,  0.0798],\n",
      "          [-0.0986,  0.0865, -0.0786,  ...,  0.0344, -0.0365, -0.0285]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0592, -0.0545, -0.0323,  ..., -0.1098,  0.0357,  0.1012],\n",
      "          [-0.0440, -0.0767, -0.0451,  ...,  0.1001,  0.0995,  0.0379]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0593, -0.0541, -0.0321,  ..., -0.1093,  0.0361,  0.1010],\n",
      "          [-0.0444, -0.0769, -0.0450,  ...,  0.0999,  0.0996,  0.0376]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0595, -0.0539, -0.0323,  ..., -0.1093,  0.0362,  0.1011],\n",
      "          [-0.0445, -0.0769, -0.0447,  ...,  0.0996,  0.0997,  0.0376]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0595, -0.0538, -0.0323,  ..., -0.1092,  0.0363,  0.1011],\n",
      "          [-0.0445, -0.0770, -0.0447,  ...,  0.0996,  0.0997,  0.0375]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0594, -0.0536, -0.0322,  ..., -0.1092,  0.0362,  0.1011],\n",
      "          [-0.0446, -0.0769, -0.0448,  ...,  0.0997,  0.0997,  0.0375]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0593, -0.0539, -0.0321,  ..., -0.1092,  0.0361,  0.1012],\n",
      "          [-0.0446, -0.0768, -0.0450,  ...,  0.0997,  0.0996,  0.0375]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-8.5791e-02, -3.8997e-02,  1.5583e-02,  ..., -9.9002e-02,\n",
      "            4.5663e-02, -5.4879e-02],\n",
      "          [ 2.0730e-04, -9.0048e-02, -8.3656e-03,  ..., -1.1729e-01,\n",
      "           -7.3293e-02, -9.5353e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.5737e-02, -3.8944e-02,  1.5343e-02,  ..., -9.8961e-02,\n",
      "            4.5625e-02, -5.4803e-02],\n",
      "          [ 1.7659e-04, -9.0102e-02, -8.7556e-03,  ..., -1.1744e-01,\n",
      "           -7.3266e-02, -9.6764e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.5538e-02, -3.8920e-02,  1.5039e-02,  ..., -9.9092e-02,\n",
      "            4.5757e-02, -5.4706e-02],\n",
      "          [ 5.2628e-05, -9.0066e-02, -8.8817e-03,  ..., -1.1733e-01,\n",
      "           -7.3261e-02, -9.6246e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.5401e-02, -3.8906e-02,  1.4975e-02,  ..., -9.9115e-02,\n",
      "            4.5815e-02, -5.4680e-02],\n",
      "          [ 7.3713e-05, -9.0185e-02, -8.9730e-03,  ..., -1.1737e-01,\n",
      "           -7.3216e-02, -9.6986e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.5448e-02, -3.8995e-02,  1.5086e-02,  ..., -9.9167e-02,\n",
      "            4.5723e-02, -5.4736e-02],\n",
      "          [ 1.6021e-04, -9.0073e-02, -8.8595e-03,  ..., -1.1748e-01,\n",
      "           -7.3282e-02, -9.6893e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.5499e-02, -3.8943e-02,  1.5202e-02,  ..., -9.9052e-02,\n",
      "            4.5657e-02, -5.4804e-02],\n",
      "          [ 2.9684e-04, -8.9991e-02, -8.7904e-03,  ..., -1.1753e-01,\n",
      "           -7.3337e-02, -9.5903e-03]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0143, -0.0622,  0.0689,  ..., -0.0669,  0.0021, -0.0413],\n",
      "          [ 0.0859, -0.0306,  0.0307,  ...,  0.0068,  0.0396, -0.0017]]],\n",
      "\n",
      "\n",
      "        [[[-0.0146, -0.0620,  0.0687,  ..., -0.0668,  0.0014, -0.0413],\n",
      "          [ 0.0861, -0.0308,  0.0304,  ...,  0.0068,  0.0397, -0.0017]]],\n",
      "\n",
      "\n",
      "        [[[-0.0149, -0.0620,  0.0687,  ..., -0.0668,  0.0010, -0.0413],\n",
      "          [ 0.0863, -0.0306,  0.0301,  ...,  0.0069,  0.0398, -0.0017]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0149, -0.0619,  0.0687,  ..., -0.0669,  0.0009, -0.0413],\n",
      "          [ 0.0864, -0.0306,  0.0300,  ...,  0.0069,  0.0399, -0.0016]]],\n",
      "\n",
      "\n",
      "        [[[-0.0149, -0.0619,  0.0687,  ..., -0.0670,  0.0011, -0.0412],\n",
      "          [ 0.0863, -0.0307,  0.0301,  ...,  0.0068,  0.0398, -0.0017]]],\n",
      "\n",
      "\n",
      "        [[[-0.0147, -0.0621,  0.0687,  ..., -0.0669,  0.0013, -0.0412],\n",
      "          [ 0.0862, -0.0308,  0.0302,  ...,  0.0068,  0.0397, -0.0016]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0185, -0.0433, -0.1315,  ...,  0.0321,  0.0694,  0.0866],\n",
      "          [ 0.1395,  0.0462, -0.0423,  ...,  0.1286,  0.1165,  0.0780]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0181, -0.0432, -0.1317,  ...,  0.0322,  0.0690,  0.0870],\n",
      "          [ 0.1396,  0.0464, -0.0427,  ...,  0.1285,  0.1166,  0.0778]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0177, -0.0432, -0.1316,  ...,  0.0321,  0.0691,  0.0870],\n",
      "          [ 0.1394,  0.0462, -0.0428,  ...,  0.1284,  0.1165,  0.0777]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0176, -0.0432, -0.1316,  ...,  0.0321,  0.0691,  0.0870],\n",
      "          [ 0.1394,  0.0462, -0.0429,  ...,  0.1284,  0.1166,  0.0776]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0179, -0.0432, -0.1317,  ...,  0.0322,  0.0690,  0.0871],\n",
      "          [ 0.1395,  0.0462, -0.0428,  ...,  0.1285,  0.1167,  0.0778]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0180, -0.0432, -0.1317,  ...,  0.0322,  0.0689,  0.0872],\n",
      "          [ 0.1396,  0.0463, -0.0427,  ...,  0.1286,  0.1166,  0.0778]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0158,  0.0930, -0.1310,  ..., -0.0087,  0.0329,  0.0902],\n",
      "          [-0.1252,  0.0620,  0.0606,  ...,  0.0339,  0.1032,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0159,  0.0927, -0.1306,  ..., -0.0088,  0.0332,  0.0901],\n",
      "          [-0.1253,  0.0621,  0.0604,  ...,  0.0335,  0.1032,  0.0036]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0160,  0.0929, -0.1304,  ..., -0.0086,  0.0331,  0.0902],\n",
      "          [-0.1253,  0.0621,  0.0603,  ...,  0.0334,  0.1033,  0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0162,  0.0928, -0.1303,  ..., -0.0087,  0.0332,  0.0902],\n",
      "          [-0.1254,  0.0621,  0.0602,  ...,  0.0334,  0.1032,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0161,  0.0928, -0.1305,  ..., -0.0088,  0.0331,  0.0902],\n",
      "          [-0.1254,  0.0620,  0.0601,  ...,  0.0335,  0.1033,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0160,  0.0927, -0.1306,  ..., -0.0088,  0.0330,  0.0902],\n",
      "          [-0.1254,  0.0620,  0.0602,  ...,  0.0335,  0.1032,  0.0037]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0209, -0.0474,  0.1120,  ...,  0.0324,  0.0980, -0.0178],\n",
      "          [ 0.0242,  0.0187,  0.0206,  ..., -0.0210,  0.0523,  0.0161]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0210, -0.0474,  0.1119,  ...,  0.0326,  0.0979, -0.0180],\n",
      "          [ 0.0241,  0.0185,  0.0205,  ..., -0.0206,  0.0522,  0.0157]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0211, -0.0474,  0.1120,  ...,  0.0328,  0.0981, -0.0180],\n",
      "          [ 0.0239,  0.0184,  0.0204,  ..., -0.0206,  0.0523,  0.0157]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0211, -0.0474,  0.1119,  ...,  0.0330,  0.0980, -0.0181],\n",
      "          [ 0.0240,  0.0184,  0.0204,  ..., -0.0205,  0.0522,  0.0156]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0211, -0.0474,  0.1118,  ...,  0.0329,  0.0979, -0.0181],\n",
      "          [ 0.0241,  0.0185,  0.0204,  ..., -0.0206,  0.0521,  0.0156]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0210, -0.0474,  0.1118,  ...,  0.0327,  0.0980, -0.0180],\n",
      "          [ 0.0241,  0.0185,  0.0205,  ..., -0.0206,  0.0522,  0.0156]]]])\n",
      "########################\n",
      "################################\n",
      "es_pr_MA_list: \n",
      "[tensor([[[ 0.1124,  0.0497,  0.0944,  ..., -0.1089,  0.0704, -0.0186]],\n",
      "\n",
      "        [[ 0.1123,  0.0497,  0.0943,  ..., -0.1090,  0.0702, -0.0185]],\n",
      "\n",
      "        [[ 0.1124,  0.0498,  0.0943,  ..., -0.1090,  0.0703, -0.0185]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1124,  0.0498,  0.0943,  ..., -0.1090,  0.0703, -0.0185]],\n",
      "\n",
      "        [[ 0.1125,  0.0497,  0.0943,  ..., -0.1091,  0.0702, -0.0185]],\n",
      "\n",
      "        [[ 0.1124,  0.0497,  0.0943,  ..., -0.1091,  0.0702, -0.0186]]]), tensor([[[ 0.0170,  0.0665, -0.0628,  ..., -0.0951,  0.0374, -0.0086]],\n",
      "\n",
      "        [[ 0.0171,  0.0666, -0.0628,  ..., -0.0951,  0.0373, -0.0087]],\n",
      "\n",
      "        [[ 0.0170,  0.0666, -0.0629,  ..., -0.0952,  0.0373, -0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0170,  0.0666, -0.0630,  ..., -0.0952,  0.0372, -0.0089]],\n",
      "\n",
      "        [[ 0.0170,  0.0666, -0.0630,  ..., -0.0952,  0.0372, -0.0088]],\n",
      "\n",
      "        [[ 0.0169,  0.0666, -0.0628,  ..., -0.0952,  0.0372, -0.0087]]]), tensor([[[ 0.0785, -0.0223, -0.0115,  ...,  0.0975, -0.0560, -0.0168]],\n",
      "\n",
      "        [[ 0.0785, -0.0222, -0.0115,  ...,  0.0973, -0.0560, -0.0168]],\n",
      "\n",
      "        [[ 0.0785, -0.0223, -0.0115,  ...,  0.0972, -0.0558, -0.0169]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0784, -0.0223, -0.0115,  ...,  0.0972, -0.0558, -0.0169]],\n",
      "\n",
      "        [[ 0.0785, -0.0222, -0.0115,  ...,  0.0973, -0.0559, -0.0168]],\n",
      "\n",
      "        [[ 0.0784, -0.0222, -0.0115,  ...,  0.0973, -0.0559, -0.0167]]]), tensor([[[-0.0324,  0.0211, -0.0667,  ...,  0.0559,  0.0324,  0.0417]],\n",
      "\n",
      "        [[-0.0323,  0.0211, -0.0666,  ...,  0.0558,  0.0326,  0.0416]],\n",
      "\n",
      "        [[-0.0321,  0.0211, -0.0665,  ...,  0.0558,  0.0326,  0.0415]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0321,  0.0211, -0.0665,  ...,  0.0558,  0.0326,  0.0416]],\n",
      "\n",
      "        [[-0.0322,  0.0211, -0.0665,  ...,  0.0559,  0.0327,  0.0415]],\n",
      "\n",
      "        [[-0.0323,  0.0211, -0.0666,  ...,  0.0558,  0.0326,  0.0415]]]), tensor([[[-0.0826,  0.0299, -0.1437,  ...,  0.0016, -0.0887,  0.0835]],\n",
      "\n",
      "        [[-0.0827,  0.0298, -0.1436,  ...,  0.0016, -0.0888,  0.0836]],\n",
      "\n",
      "        [[-0.0827,  0.0298, -0.1436,  ...,  0.0016, -0.0889,  0.0837]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0827,  0.0298, -0.1436,  ...,  0.0017, -0.0889,  0.0838]],\n",
      "\n",
      "        [[-0.0827,  0.0298, -0.1436,  ...,  0.0017, -0.0889,  0.0838]],\n",
      "\n",
      "        [[-0.0827,  0.0298, -0.1437,  ...,  0.0017, -0.0888,  0.0837]]]), tensor([[[ 0.0824, -0.0028,  0.0020,  ...,  0.0151, -0.1028,  0.0976]],\n",
      "\n",
      "        [[ 0.0824, -0.0029,  0.0020,  ...,  0.0150, -0.1027,  0.0976]],\n",
      "\n",
      "        [[ 0.0823, -0.0030,  0.0021,  ...,  0.0150, -0.1025,  0.0976]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0824, -0.0030,  0.0021,  ...,  0.0150, -0.1025,  0.0976]],\n",
      "\n",
      "        [[ 0.0824, -0.0030,  0.0020,  ...,  0.0150, -0.1026,  0.0975]],\n",
      "\n",
      "        [[ 0.0824, -0.0029,  0.0021,  ...,  0.0150, -0.1027,  0.0976]]]), tensor([[[ 0.0580,  0.0395,  0.0862,  ..., -0.0474, -0.0513,  0.0990]],\n",
      "\n",
      "        [[ 0.0582,  0.0396,  0.0862,  ..., -0.0473, -0.0514,  0.0989]],\n",
      "\n",
      "        [[ 0.0581,  0.0398,  0.0861,  ..., -0.0472, -0.0515,  0.0989]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0581,  0.0398,  0.0861,  ..., -0.0472, -0.0515,  0.0989]],\n",
      "\n",
      "        [[ 0.0582,  0.0397,  0.0862,  ..., -0.0473, -0.0515,  0.0989]],\n",
      "\n",
      "        [[ 0.0582,  0.0397,  0.0862,  ..., -0.0472, -0.0515,  0.0989]]]), tensor([[[ 0.0306, -0.0196,  0.0006,  ..., -0.0463,  0.0320,  0.0978]],\n",
      "\n",
      "        [[ 0.0307, -0.0197,  0.0005,  ..., -0.0462,  0.0323,  0.0978]],\n",
      "\n",
      "        [[ 0.0308, -0.0197,  0.0006,  ..., -0.0463,  0.0322,  0.0978]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0307, -0.0197,  0.0005,  ..., -0.0463,  0.0322,  0.0978]],\n",
      "\n",
      "        [[ 0.0307, -0.0197,  0.0005,  ..., -0.0462,  0.0322,  0.0977]],\n",
      "\n",
      "        [[ 0.0307, -0.0197,  0.0005,  ..., -0.0461,  0.0322,  0.0978]]]), tensor([[[ 0.0137, -0.0156,  0.0005,  ..., -0.0568,  0.0253,  0.0302]],\n",
      "\n",
      "        [[ 0.0136, -0.0155,  0.0005,  ..., -0.0568,  0.0252,  0.0304]],\n",
      "\n",
      "        [[ 0.0135, -0.0155,  0.0005,  ..., -0.0567,  0.0251,  0.0304]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0135, -0.0155,  0.0005,  ..., -0.0567,  0.0251,  0.0305]],\n",
      "\n",
      "        [[ 0.0136, -0.0155,  0.0004,  ..., -0.0567,  0.0251,  0.0305]],\n",
      "\n",
      "        [[ 0.0136, -0.0155,  0.0005,  ..., -0.0567,  0.0252,  0.0304]]])]\n",
      "################################\n",
      "################################\n",
      "es_pr_MA_lstm_list: \n",
      "[tensor([[[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0449, -0.0023]]]), tensor([[[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0352]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0352]],\n",
      "\n",
      "        [[-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353]]]), tensor([[[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0245, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096]]]), tensor([[[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0046]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]]]), tensor([[[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0306,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0306,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]]]), tensor([[[-0.0188, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]],\n",
      "\n",
      "        [[-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]],\n",
      "\n",
      "        [[-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]],\n",
      "\n",
      "        [[-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]],\n",
      "\n",
      "        [[-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]]]), tensor([[[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        [[-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        [[-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        [[-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        [[-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]]]), tensor([[[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]]]), tensor([[[-2.5551e-05,  2.4317e-02, -2.4115e-02,  ..., -4.1141e-02,\n",
      "          -9.0992e-03, -1.9512e-03]],\n",
      "\n",
      "        [[-3.6614e-05,  2.4321e-02, -2.4085e-02,  ..., -4.1149e-02,\n",
      "          -9.0946e-03, -1.9324e-03]],\n",
      "\n",
      "        [[-5.7449e-05,  2.4337e-02, -2.4080e-02,  ..., -4.1152e-02,\n",
      "          -9.1168e-03, -1.9322e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.7153e-05,  2.4342e-02, -2.4079e-02,  ..., -4.1148e-02,\n",
      "          -9.1161e-03, -1.9266e-03]],\n",
      "\n",
      "        [[-4.9827e-05,  2.4336e-02, -2.4083e-02,  ..., -4.1145e-02,\n",
      "          -9.1162e-03, -1.9280e-03]],\n",
      "\n",
      "        [[-4.6090e-05,  2.4324e-02, -2.4090e-02,  ..., -4.1141e-02,\n",
      "          -9.1204e-03, -1.9308e-03]]])]\n",
      "################################\n",
      "################################\n",
      "es_pr_avg_lstm_list: \n",
      "[tensor([[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        ...,\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0449, -0.0023]]), tensor([[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0353],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0352],\n",
      "        ...,\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0352],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0411,  0.0025,  0.0353]]), tensor([[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0245, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096],\n",
      "        ...,\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0273, 0.0245, 0.0096]]), tensor([[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0046],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045],\n",
      "        ...,\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0120,  0.0045]]), tensor([[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        [ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        [ 0.0118,  0.0370,  0.0306,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        ...,\n",
      "        [ 0.0118,  0.0370,  0.0306,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        [ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        [ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]]), tensor([[-0.0188, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210],\n",
      "        [-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210],\n",
      "        [-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210],\n",
      "        ...,\n",
      "        [-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210],\n",
      "        [-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210],\n",
      "        [-0.0189, -0.0072,  0.0001,  ..., -0.0029,  0.0012, -0.0210]]), tensor([[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        [-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        [-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        ...,\n",
      "        [-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        [-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        [-0.0023, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]]), tensor([[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        ...,\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]]), tensor([[-2.5551e-05,  2.4317e-02, -2.4115e-02,  ..., -4.1141e-02,\n",
      "         -9.0992e-03, -1.9512e-03],\n",
      "        [-3.6614e-05,  2.4321e-02, -2.4085e-02,  ..., -4.1149e-02,\n",
      "         -9.0946e-03, -1.9324e-03],\n",
      "        [-5.7449e-05,  2.4337e-02, -2.4080e-02,  ..., -4.1152e-02,\n",
      "         -9.1168e-03, -1.9322e-03],\n",
      "        ...,\n",
      "        [-5.7153e-05,  2.4342e-02, -2.4079e-02,  ..., -4.1148e-02,\n",
      "         -9.1161e-03, -1.9266e-03],\n",
      "        [-4.9827e-05,  2.4336e-02, -2.4083e-02,  ..., -4.1145e-02,\n",
      "         -9.1162e-03, -1.9280e-03],\n",
      "        [-4.6090e-05,  2.4324e-02, -2.4090e-02,  ..., -4.1141e-02,\n",
      "         -9.1204e-03, -1.9308e-03]])]\n",
      "################################\n",
      "linguistic_input:  torch.Size([1680, 51])\n",
      "readability_input:  torch.Size([1680, 35])\n",
      "es_pr_feat_concat:  9 ,  torch.Size([1680, 186])\n",
      "################################\n",
      "es_pr_feat_concat: \n",
      "[tensor([[-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.0000,  0.1429],\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.2000,  0.0000,  0.0000],\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.2500,  0.1250],\n",
      "        ...,\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.4286,  0.1000]],\n",
      "       dtype=torch.float64), tensor([[-0.0530, -0.0280,  0.0106,  ...,  0.0000,  0.0000,  0.1429],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.2000,  0.0000,  0.0000],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0000,  0.2500,  0.1250],\n",
      "        ...,\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0530, -0.0279,  0.0106,  ...,  0.0000,  0.4286,  0.1000]],\n",
      "       dtype=torch.float64), tensor([[0.0194, 0.0025, 0.0190,  ..., 0.0000, 0.0000, 0.1429],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.2000, 0.0000, 0.0000],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0000, 0.2500, 0.1250],\n",
      "        ...,\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0194, 0.0025, 0.0189,  ..., 0.0000, 0.4286, 0.1000]],\n",
      "       dtype=torch.float64), tensor([[-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.0000,  0.1429],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.2000,  0.0000,  0.0000],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.2500,  0.1250],\n",
      "        ...,\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.4286,  0.1000]],\n",
      "       dtype=torch.float64), tensor([[0.0118, 0.0370, 0.0305,  ..., 0.0000, 0.0000, 0.1429],\n",
      "        [0.0118, 0.0370, 0.0305,  ..., 0.2000, 0.0000, 0.0000],\n",
      "        [0.0118, 0.0370, 0.0306,  ..., 0.0000, 0.2500, 0.1250],\n",
      "        ...,\n",
      "        [0.0118, 0.0370, 0.0306,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0118, 0.0370, 0.0305,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0118, 0.0370, 0.0305,  ..., 0.0000, 0.4286, 0.1000]],\n",
      "       dtype=torch.float64), tensor([[-1.8836e-02, -7.1972e-03,  1.0825e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  1.4286e-01],\n",
      "        [-1.8862e-02, -7.1927e-03,  1.0408e-04,  ...,  2.0000e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.8874e-02, -7.1981e-03,  1.1883e-04,  ...,  0.0000e+00,\n",
      "          2.5000e-01,  1.2500e-01],\n",
      "        ...,\n",
      "        [-1.8884e-02, -7.2002e-03,  1.2048e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.8873e-02, -7.2005e-03,  1.1698e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-1.8869e-02, -7.1953e-03,  1.1358e-04,  ...,  0.0000e+00,\n",
      "          4.2857e-01,  1.0000e-01]], dtype=torch.float64), tensor([[-0.0024, -0.0317,  0.0109,  ...,  0.0000,  0.0000,  0.1429],\n",
      "        [-0.0023, -0.0317,  0.0109,  ...,  0.2000,  0.0000,  0.0000],\n",
      "        [-0.0023, -0.0317,  0.0109,  ...,  0.0000,  0.2500,  0.1250],\n",
      "        ...,\n",
      "        [-0.0023, -0.0317,  0.0109,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0023, -0.0317,  0.0109,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0023, -0.0317,  0.0109,  ...,  0.0000,  0.4286,  0.1000]],\n",
      "       dtype=torch.float64), tensor([[-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.0000,  0.1429],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.2000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.2500,  0.1250],\n",
      "        ...,\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.4286,  0.1000]],\n",
      "       dtype=torch.float64), tensor([[-2.5551e-05,  2.4317e-02, -2.4115e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  1.4286e-01],\n",
      "        [-3.6614e-05,  2.4321e-02, -2.4085e-02,  ...,  2.0000e-01,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.7449e-05,  2.4337e-02, -2.4080e-02,  ...,  0.0000e+00,\n",
      "          2.5000e-01,  1.2500e-01],\n",
      "        ...,\n",
      "        [-5.7153e-05,  2.4342e-02, -2.4079e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.9827e-05,  2.4336e-02, -2.4083e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-4.6090e-05,  2.4324e-02, -2.4090e-02,  ...,  0.0000e+00,\n",
      "          4.2857e-01,  1.0000e-01]], dtype=torch.float64)]\n",
      "################################\n",
      "tensor([-1.8580e-02, -9.6275e-03, -3.6134e-02, -2.6484e-02,  1.3739e-02,\n",
      "         8.4013e-03, -2.5208e-02, -7.7523e-03, -1.1383e-03,  1.1364e-02,\n",
      "         1.3784e-02, -4.1173e-02, -2.4714e-02, -7.3800e-03,  2.3388e-03,\n",
      "        -3.8794e-02, -2.2252e-02, -1.3628e-02, -7.2090e-03,  3.8204e-02,\n",
      "        -3.5534e-02,  2.8406e-02, -6.5386e-03, -8.5657e-03, -3.0296e-02,\n",
      "         7.4947e-03,  2.0455e-03,  1.4989e-02, -5.5927e-02, -1.1349e-02,\n",
      "        -2.6279e-02, -3.2083e-02, -4.2369e-03, -1.7947e-03, -2.7516e-02,\n",
      "        -1.0496e-02,  1.6065e-02,  9.6401e-03, -1.2408e-03,  2.1493e-02,\n",
      "        -7.2576e-03, -6.9201e-03, -1.5556e-03,  8.7461e-03,  2.5817e-02,\n",
      "         4.1858e-02, -5.4983e-03,  1.9108e-02,  4.1112e-02,  1.9013e-02,\n",
      "        -3.4679e-02, -2.3215e-02, -1.5815e-02, -1.7389e-02, -1.1467e-02,\n",
      "         2.3632e-02,  1.1261e-02, -1.3495e-02, -4.6989e-02, -1.2050e-02,\n",
      "         3.9184e-02, -2.7777e-02, -3.5620e-03, -4.5077e-02,  3.4601e-02,\n",
      "        -2.0140e-02, -1.7798e-02,  2.9707e-02,  5.6499e-03,  1.1336e-02,\n",
      "        -2.5504e-02, -3.9255e-04, -1.5857e-02,  1.8387e-02,  1.2312e-02,\n",
      "        -9.3855e-03, -5.8877e-03, -1.4007e-02, -5.9951e-03,  1.9446e-02,\n",
      "         8.8898e-04,  2.6096e-03,  4.1130e-02,  2.3693e-02, -4.5953e-02,\n",
      "         1.6926e-02, -2.8261e-02, -7.2889e-03, -1.8849e-02, -3.2186e-03,\n",
      "         2.0643e-03,  7.0286e-03, -4.4176e-02, -1.2914e-02,  6.2636e-03,\n",
      "        -1.6195e-02,  3.5688e-02, -1.8008e-02,  4.4963e-02, -2.3330e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01], dtype=torch.float64)\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-1.8580e-02, -9.6275e-03, -3.6134e-02, -2.6484e-02,  1.3739e-02,\n",
      "         8.4013e-03, -2.5208e-02, -7.7523e-03, -1.1383e-03,  1.1364e-02,\n",
      "         1.3784e-02, -4.1173e-02, -2.4714e-02, -7.3800e-03,  2.3388e-03,\n",
      "        -3.8794e-02, -2.2252e-02, -1.3628e-02, -7.2090e-03,  3.8204e-02,\n",
      "        -3.5534e-02,  2.8406e-02, -6.5386e-03, -8.5657e-03, -3.0296e-02,\n",
      "         7.4947e-03,  2.0455e-03,  1.4989e-02, -5.5927e-02, -1.1349e-02,\n",
      "        -2.6279e-02, -3.2083e-02, -4.2369e-03, -1.7947e-03, -2.7516e-02,\n",
      "        -1.0496e-02,  1.6065e-02,  9.6401e-03, -1.2408e-03,  2.1493e-02,\n",
      "        -7.2576e-03, -6.9201e-03, -1.5556e-03,  8.7461e-03,  2.5817e-02,\n",
      "         4.1858e-02, -5.4983e-03,  1.9108e-02,  4.1112e-02,  1.9013e-02,\n",
      "        -3.4679e-02, -2.3215e-02, -1.5815e-02, -1.7389e-02, -1.1467e-02,\n",
      "         2.3632e-02,  1.1261e-02, -1.3495e-02, -4.6989e-02, -1.2050e-02,\n",
      "         3.9184e-02, -2.7777e-02, -3.5620e-03, -4.5077e-02,  3.4601e-02,\n",
      "        -2.0140e-02, -1.7798e-02,  2.9707e-02,  5.6499e-03,  1.1336e-02,\n",
      "        -2.5504e-02, -3.9255e-04, -1.5857e-02,  1.8387e-02,  1.2312e-02,\n",
      "        -9.3855e-03, -5.8877e-03, -1.4007e-02, -5.9951e-03,  1.9446e-02,\n",
      "         8.8898e-04,  2.6096e-03,  4.1130e-02,  2.3693e-02, -4.5953e-02,\n",
      "         1.6926e-02, -2.8261e-02, -7.2889e-03, -1.8849e-02, -3.2186e-03,\n",
      "         2.0643e-03,  7.0286e-03, -4.4176e-02, -1.2914e-02,  6.2636e-03,\n",
      "        -1.6195e-02,  3.5688e-02, -1.8008e-02,  4.4963e-02, -2.3330e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01,  4.6226e-02, -4.5062e-02,  8.3668e-02,  8.6256e-02,\n",
      "         6.1262e-02, -1.0606e-02,  1.4418e-02,  1.2168e-02, -4.1069e-02,\n",
      "         9.2980e-02, -2.2301e-01, -1.3080e-01, -2.6429e-01, -1.0233e-01,\n",
      "         1.1448e-01, -9.9456e-02,  1.4387e-02,  3.8941e-02, -2.0175e-01,\n",
      "        -4.6573e-02,  4.8684e-02, -7.8882e-02,  1.4083e-01,  1.2855e-01,\n",
      "        -9.4190e-02, -1.1140e-01,  1.0909e-01, -5.7283e-02, -1.4046e-02,\n",
      "        -6.5924e-02,  2.1573e-02,  7.3961e-02,  1.1794e-01,  1.3194e-01,\n",
      "        -4.9108e-02,  1.0753e-01,  2.3377e-01, -1.2112e-01,  3.0991e-02,\n",
      "        -6.9302e-02, -4.1745e-02, -4.7840e-02,  4.5298e-02, -1.0446e-01,\n",
      "        -4.7127e-02,  1.2611e-01,  1.8322e-01, -4.9127e-02, -4.9517e-02,\n",
      "         1.2129e-02, -1.9792e-02, -4.9198e-02,  2.2666e-01, -2.4132e-02,\n",
      "         1.4138e-01, -6.9885e-02, -5.3321e-02, -2.4924e-01, -1.1638e-01,\n",
      "         4.0776e-02,  1.0980e-02, -2.4547e-02, -1.2925e-02,  8.3960e-03,\n",
      "         2.9030e-02,  1.8935e-01,  1.6520e-01, -1.1152e-02, -2.0492e-01,\n",
      "         9.9305e-02, -1.6311e-01,  9.2808e-02,  5.2108e-02,  1.1706e-01,\n",
      "        -9.5666e-02,  1.1147e-01, -1.2632e-02, -4.2203e-02,  8.1831e-02,\n",
      "        -1.2826e-01,  3.8269e-02,  2.7490e-01,  2.0375e-02, -3.8414e-02,\n",
      "         2.1004e-02,  2.5723e-02,  9.6472e-02, -1.5534e-01, -1.3944e-01,\n",
      "        -6.3696e-02, -1.7872e-01,  1.3227e-01, -7.7554e-02, -4.6727e-03,\n",
      "         7.2329e-02,  6.5398e-02, -1.9125e-01,  6.4405e-03, -1.9923e-01,\n",
      "         1.0871e-01, -7.4405e-02,  2.8190e-02, -3.2543e-02, -7.4307e-02,\n",
      "        -1.8234e-01,  1.0277e-01,  2.0803e-03,  1.3893e-01,  9.9614e-02,\n",
      "         1.4803e-01,  2.3366e-01, -2.6262e-01,  1.9179e-02, -1.5900e-01,\n",
      "        -4.5925e-02,  9.8921e-02, -5.0292e-03, -1.0249e-02,  6.2622e-02,\n",
      "         1.2299e-01,  4.8087e-02,  9.4816e-02,  3.6391e-02, -5.1837e-02,\n",
      "        -1.9198e-02,  5.7203e-02, -9.6074e-02,  5.0968e-02, -1.6589e-02,\n",
      "         5.4928e-02, -3.9026e-02,  1.7419e-02,  7.7613e-02, -2.5611e-02,\n",
      "         1.3608e-01,  1.4928e-02, -2.6735e-02,  9.0034e-02,  3.6643e-02,\n",
      "        -1.0141e-01, -1.7485e-01,  3.2522e-02, -7.2376e-02, -6.3724e-02,\n",
      "        -2.3935e-02, -1.4725e-02,  4.5469e-02, -4.6729e-02, -1.3776e-01,\n",
      "         1.6914e-02, -9.4241e-02, -1.0484e-01, -1.5519e-01, -1.5580e-01,\n",
      "         8.7088e-02, -2.1818e-02,  1.3011e-01, -1.1458e-01,  7.2377e-02,\n",
      "         1.3306e-01,  2.0486e-01,  6.1046e-02,  5.8431e-02,  5.8372e-02,\n",
      "        -4.4216e-02, -1.0146e-01, -9.8990e-02,  3.2498e-02,  9.8596e-02,\n",
      "         1.6617e-01,  2.0214e-02, -4.3024e-03, -1.9996e-01,  7.6925e-02,\n",
      "         1.9737e-02,  5.5784e-02,  1.9367e-01,  2.0464e-02,  8.2749e-02,\n",
      "        -5.5263e-02, -1.1524e-01, -5.2538e-02, -1.2873e-01,  7.5041e-02,\n",
      "         2.1836e-03,  4.9216e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5328],\n",
      "        [0.5343],\n",
      "        [0.5240],\n",
      "        ...,\n",
      "        [0.5064],\n",
      "        [0.5041],\n",
      "        [0.5220]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-5.3013e-02, -2.7968e-02,  1.0612e-02, -3.6581e-02, -2.4812e-02,\n",
      "        -1.5011e-02,  2.4492e-02,  3.2319e-02, -3.3204e-03, -2.9864e-02,\n",
      "         4.5283e-03,  1.6374e-03, -5.3006e-03, -9.3562e-03, -3.5543e-02,\n",
      "        -1.1187e-02, -1.0989e-02,  3.2449e-02,  1.0834e-03,  4.5050e-02,\n",
      "        -1.6556e-02, -2.4395e-02, -2.2399e-02,  4.3002e-03, -3.6305e-02,\n",
      "        -3.9572e-02, -1.5302e-02, -9.2143e-03,  3.4849e-02,  1.2564e-02,\n",
      "        -6.5775e-03,  1.6029e-02,  2.9143e-02, -1.8890e-02,  3.0716e-02,\n",
      "        -1.7785e-02, -4.1849e-03,  2.0221e-02,  2.8547e-03, -1.7764e-02,\n",
      "         3.5067e-02, -4.8481e-03,  1.2375e-02,  9.1035e-03, -5.3270e-03,\n",
      "        -2.5500e-02,  3.2953e-02, -1.2543e-04, -7.8687e-03,  5.3659e-03,\n",
      "         8.1174e-03, -2.6671e-03, -8.2028e-03,  5.8145e-03, -1.7919e-02,\n",
      "         4.3638e-02,  1.7684e-02, -1.4178e-03,  2.4096e-02, -1.7346e-02,\n",
      "        -1.9534e-02, -1.9003e-03,  1.5237e-02, -7.5827e-03,  4.9351e-03,\n",
      "        -2.4291e-02, -1.6175e-02,  1.1944e-02,  9.2069e-03,  3.2599e-02,\n",
      "        -2.4218e-03, -1.5887e-02,  4.7490e-02, -5.5577e-03, -5.8048e-03,\n",
      "        -2.2533e-02, -2.7134e-02,  1.8467e-02,  4.4144e-02, -4.2039e-03,\n",
      "         2.7680e-02, -7.3408e-03,  5.4226e-03,  6.5734e-03, -1.2763e-02,\n",
      "        -1.3330e-02,  5.1775e-03,  2.0838e-02, -1.8599e-03,  1.9754e-02,\n",
      "         2.2285e-02,  3.7263e-02,  4.3040e-03, -2.2509e-02, -1.0078e-02,\n",
      "         2.2629e-02,  4.2846e-02,  4.1121e-02,  2.4544e-03,  3.5291e-02,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01, -8.1151e-02,  1.4063e-02, -7.5474e-02, -7.6335e-02,\n",
      "        -1.8093e-02, -1.4590e-02,  1.1222e-01,  8.7301e-02,  7.8136e-02,\n",
      "         5.4948e-02, -5.8252e-02,  2.7166e-02,  1.5668e-01,  1.1551e-01,\n",
      "         7.0109e-02, -9.8415e-02, -9.0201e-03, -6.9171e-02,  1.8392e-03,\n",
      "         9.3120e-02,  1.5432e-01,  1.5519e-01,  4.6730e-02, -5.4329e-02,\n",
      "        -2.3031e-02,  6.7815e-02, -9.3642e-02, -1.2133e-01,  3.6822e-03,\n",
      "         2.5527e-02,  3.6141e-02,  9.6465e-02, -6.0953e-02, -1.1749e-01,\n",
      "        -4.4562e-02,  1.8762e-03,  6.1216e-02,  3.8204e-03,  1.9200e-02,\n",
      "         2.9780e-02, -4.6342e-02,  2.4475e-02, -5.8930e-02, -4.5350e-02,\n",
      "        -1.7993e-01, -4.3908e-02, -2.0234e-01,  9.9278e-02,  1.2907e-01,\n",
      "         1.5639e-01,  2.4286e-02,  1.0964e-01, -5.6115e-02, -5.8202e-02,\n",
      "        -7.8943e-02, -1.2594e-01,  4.6765e-02,  4.8261e-02, -7.8380e-02,\n",
      "         5.6848e-02, -1.6079e-01,  2.1749e-02, -8.6077e-02, -2.8934e-02,\n",
      "        -6.7497e-02, -1.0980e-01,  1.4194e-01, -1.3587e-01,  9.2055e-02,\n",
      "        -7.1933e-02,  1.0576e-01,  8.6472e-02, -5.5285e-02, -9.5506e-02,\n",
      "         4.8179e-02, -2.8681e-02,  1.2025e-01,  1.0337e-01,  1.3555e-01,\n",
      "        -3.7692e-02, -3.7896e-02, -7.3501e-02, -5.2829e-02, -1.2080e-01,\n",
      "         5.8233e-02, -1.9064e-02,  1.4717e-02, -6.0121e-02,  2.8738e-02,\n",
      "         2.7386e-02, -6.8904e-02, -6.5248e-02, -1.3571e-01, -3.0538e-02,\n",
      "         4.4858e-02,  1.1304e-02, -9.9242e-02,  3.9322e-02,  3.2793e-02,\n",
      "         2.8700e-02, -2.5448e-02, -8.9072e-02,  1.2135e-01,  2.2083e-02,\n",
      "         8.1930e-02,  3.3932e-02,  8.4240e-02, -3.3846e-02,  2.5125e-02,\n",
      "        -3.7759e-02, -7.1838e-02, -1.5386e-01, -7.1097e-02,  1.0416e-01,\n",
      "         5.1397e-03, -1.1636e-01,  3.9344e-02,  1.5243e-01, -1.5319e-01,\n",
      "        -1.2609e-01, -1.0584e-01,  1.8378e-02,  5.9484e-02, -1.7384e-01,\n",
      "        -6.2971e-02,  3.1976e-02,  1.9430e-02,  6.9718e-02, -8.6910e-02,\n",
      "         1.3145e-01, -3.4632e-02,  4.8076e-02, -3.3882e-02, -3.0323e-02,\n",
      "        -1.2132e-01,  3.5303e-03,  1.9190e-02,  2.9779e-02,  5.0058e-02,\n",
      "        -1.8093e-02, -1.2090e-02,  8.4503e-02, -3.5402e-02,  5.3554e-02,\n",
      "        -2.8969e-02,  7.2865e-02,  1.3845e-02,  3.5522e-04, -1.0585e-01,\n",
      "         2.3854e-02,  1.3977e-02,  1.2173e-02,  4.4562e-02, -1.4159e-01,\n",
      "         4.0985e-02, -7.4653e-02, -1.4807e-03,  4.5837e-02, -9.9159e-02,\n",
      "        -1.6434e-01,  4.3344e-02, -1.7318e-01, -4.0622e-02, -8.5505e-02,\n",
      "         6.1947e-02, -5.8340e-02, -1.0651e-03, -1.3691e-01,  4.5440e-02,\n",
      "        -1.2083e-01,  1.2189e-01,  3.9516e-02,  2.7744e-02,  3.0818e-02,\n",
      "        -2.1794e-02, -7.6489e-02,  4.3019e-02,  5.1754e-02, -4.9826e-02,\n",
      "         1.1764e-01, -7.6125e-02, -3.2683e-02,  1.2497e-03, -1.1672e-01,\n",
      "         4.8115e-02, -7.5968e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.4854],\n",
      "        [0.4966],\n",
      "        [0.5101],\n",
      "        ...,\n",
      "        [0.4953],\n",
      "        [0.5187],\n",
      "        [0.4926]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([ 1.9415e-02,  2.5485e-03,  1.8954e-02,  7.7630e-03,  3.0180e-03,\n",
      "         1.3279e-02, -3.6962e-02, -1.0655e-02, -3.0842e-02, -1.6855e-02,\n",
      "         4.6552e-02, -2.2102e-02,  1.1678e-02,  1.3432e-02,  1.9905e-03,\n",
      "        -3.1505e-03,  2.4887e-03, -2.3518e-03, -1.5732e-02, -1.7250e-02,\n",
      "         9.5100e-03, -1.9396e-02, -2.2934e-02,  2.3157e-02, -5.0444e-03,\n",
      "        -4.1190e-02, -7.5796e-03,  4.4722e-02, -2.1623e-02,  8.0821e-03,\n",
      "         7.5832e-03,  1.6414e-02, -6.4476e-03, -6.3491e-03, -1.0342e-02,\n",
      "        -3.2018e-02,  2.2789e-03, -2.4005e-02, -5.8712e-03, -6.1653e-04,\n",
      "        -2.2371e-02,  1.0240e-02, -5.2349e-03, -4.0970e-02,  2.7050e-02,\n",
      "         3.0400e-02,  1.7385e-02,  2.8885e-02,  6.3358e-03, -2.6302e-02,\n",
      "        -1.2969e-02, -1.5812e-03, -5.3590e-03, -3.5924e-03,  1.3451e-03,\n",
      "        -1.0969e-02,  2.9085e-02,  3.9657e-02, -3.3154e-02,  2.9463e-03,\n",
      "         4.4186e-02, -1.2299e-02, -1.0333e-02,  2.6583e-02, -2.7059e-02,\n",
      "        -4.1824e-02, -3.6912e-03, -7.0984e-03,  6.7441e-03, -2.3802e-02,\n",
      "        -5.4057e-03, -2.9531e-03, -1.0038e-02, -2.2665e-03, -4.0487e-03,\n",
      "         1.1356e-02,  1.2916e-04,  1.4653e-03, -9.3945e-03, -2.1236e-02,\n",
      "         2.4466e-02,  1.0771e-03, -3.7053e-03, -1.4589e-02, -2.2308e-02,\n",
      "         1.0048e-02,  9.4780e-03, -1.7634e-02, -3.4934e-02, -4.4748e-02,\n",
      "        -3.8015e-02, -1.4109e-02, -6.1312e-03,  1.4061e-02, -2.8052e-02,\n",
      "        -1.9259e-02,  9.4003e-03,  2.7322e-02,  2.4548e-02,  9.6165e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01,  1.9702e-01,  1.0273e-01, -5.3694e-02, -1.8973e-01,\n",
      "        -5.4334e-02, -4.0565e-02,  1.7517e-01,  6.6306e-02, -4.8081e-02,\n",
      "        -1.9605e-02, -5.7181e-02,  7.3166e-02, -3.4014e-02, -1.4051e-02,\n",
      "         4.0362e-02, -3.0091e-02,  1.4531e-01,  1.4839e-01, -1.4985e-01,\n",
      "         1.2909e-01, -8.7746e-02, -1.1727e-01, -1.2680e-01,  6.9751e-03,\n",
      "         1.7627e-01, -1.4466e-01, -9.7880e-03,  7.4309e-02, -9.9260e-02,\n",
      "         7.9808e-02,  3.6518e-02, -2.4625e-01, -3.1841e-02,  1.5634e-01,\n",
      "         6.0898e-02, -5.5743e-03,  1.5356e-01,  7.7283e-02, -6.8929e-02,\n",
      "         6.2895e-03, -1.1311e-01,  6.9369e-02, -1.0995e-01, -1.0378e-01,\n",
      "        -8.6237e-02,  5.3899e-02,  9.7504e-03,  2.9193e-02,  1.0087e-01,\n",
      "         2.3021e-02, -3.6793e-02,  1.5662e-01, -1.1932e-02,  3.9213e-02,\n",
      "         9.8127e-03, -1.5170e-01,  1.0368e-03, -3.4497e-02, -5.3271e-03,\n",
      "         1.4830e-01,  6.2168e-02,  6.9067e-02,  6.2591e-02,  4.8476e-02,\n",
      "         4.3757e-02,  4.0845e-03, -1.1648e-01,  9.8582e-02, -3.5069e-02,\n",
      "        -5.8139e-02,  4.3508e-04,  1.1629e-01, -7.9260e-02,  2.9970e-01,\n",
      "         2.3095e-01,  1.4338e-01,  1.8948e-02,  4.3172e-03, -3.0395e-02,\n",
      "        -2.0465e-01,  6.0748e-03,  8.4860e-02,  3.6936e-02, -2.9471e-02,\n",
      "         1.1750e-01, -3.3953e-02,  1.4651e-01, -7.3426e-02, -2.0193e-02,\n",
      "        -8.7595e-02,  9.0643e-02, -2.9824e-02, -3.8620e-02,  5.9025e-02,\n",
      "        -1.3061e-01, -8.4825e-02,  5.2753e-02, -5.7718e-02, -1.6875e-02,\n",
      "         1.5615e-01,  7.6020e-02,  5.3956e-02, -1.8287e-02, -1.1310e-01,\n",
      "         6.3785e-02,  9.7229e-02, -1.2480e-02,  3.9892e-02,  2.1650e-02,\n",
      "         2.2044e-03,  1.4883e-01, -1.2362e-01, -1.3974e-01, -6.9471e-03,\n",
      "        -2.5058e-02, -7.5217e-02,  3.8043e-02,  2.2229e-02, -1.4822e-01,\n",
      "         3.7435e-02,  2.5580e-02,  1.6996e-01,  8.5074e-02, -4.8250e-02,\n",
      "        -8.4487e-02,  4.1064e-02, -1.6816e-01, -1.1994e-01, -1.1666e-01,\n",
      "        -1.0043e-02,  3.0189e-02, -2.6316e-01, -1.0779e-01,  5.4139e-03,\n",
      "         1.4313e-01, -2.5708e-02,  1.6424e-02,  2.7416e-02, -3.9125e-02,\n",
      "        -7.6172e-02, -5.3804e-02, -8.2401e-02, -1.1501e-01, -1.0132e-01,\n",
      "        -3.5652e-02, -1.5499e-02,  2.0433e-02,  7.0769e-02, -1.1577e-02,\n",
      "        -4.1627e-02, -2.7286e-01, -9.8786e-03, -7.6527e-02, -6.0674e-02,\n",
      "         2.0135e-01, -9.6660e-02, -1.8503e-01,  2.9247e-04, -8.4866e-02,\n",
      "        -2.5290e-02,  4.2630e-02,  1.0418e-01, -2.5250e-02, -1.0994e-01,\n",
      "         1.5236e-01, -2.6102e-02,  6.7950e-02, -1.7943e-02,  1.2170e-01,\n",
      "        -8.2999e-03, -2.1001e-01, -5.3898e-02,  5.5685e-02,  7.8673e-02,\n",
      "        -1.3198e-01, -9.4683e-02,  2.1220e-01, -8.2201e-02,  3.2903e-03,\n",
      "         3.1283e-02, -2.3205e-02, -5.2691e-03, -3.4987e-02,  2.2491e-02,\n",
      "        -1.1821e-01, -1.0705e-01])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5003],\n",
      "        [0.5047],\n",
      "        [0.4872],\n",
      "        ...,\n",
      "        [0.4833],\n",
      "        [0.4711],\n",
      "        [0.4815]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-6.5283e-03, -1.7133e-02,  8.9059e-03, -3.1827e-03,  2.5271e-02,\n",
      "        -1.3208e-02,  1.8297e-02,  1.5781e-02, -4.1308e-03,  2.4149e-02,\n",
      "         2.8697e-02,  1.6898e-02,  3.2445e-02,  4.3414e-02,  5.5369e-02,\n",
      "         4.6930e-02,  1.5511e-02, -3.0716e-02, -3.7287e-02,  3.6258e-03,\n",
      "        -2.1361e-03, -1.7145e-02, -5.7968e-03, -3.7301e-02, -8.3276e-03,\n",
      "        -2.6069e-02, -3.3598e-04, -1.7795e-03, -2.8460e-02,  5.9056e-03,\n",
      "        -1.9997e-02, -2.4630e-02, -2.2891e-02, -2.3028e-02,  3.0887e-02,\n",
      "         8.0404e-03, -5.6688e-03,  8.4331e-03, -2.1928e-02, -2.7729e-02,\n",
      "         2.0270e-03, -3.0122e-02, -4.7666e-02, -2.1458e-03, -6.8927e-03,\n",
      "        -1.6950e-02, -3.2035e-02, -2.8570e-02,  1.1414e-02, -3.4156e-02,\n",
      "        -3.0530e-02, -2.0285e-02,  2.0910e-02, -2.2040e-02, -2.1277e-02,\n",
      "         5.0808e-02, -1.4231e-02,  2.3283e-02, -2.3423e-02, -4.2506e-02,\n",
      "        -1.6328e-05,  2.0095e-03,  1.5277e-02,  4.9131e-03,  6.9936e-03,\n",
      "        -3.7896e-02, -1.6450e-02, -2.3886e-02,  1.5284e-03,  4.7611e-03,\n",
      "         7.9950e-03, -1.3997e-02, -8.5612e-03, -3.2479e-03, -2.9051e-02,\n",
      "         1.3308e-02, -6.2813e-03,  8.7531e-03,  4.1394e-02,  3.3396e-02,\n",
      "         1.4837e-02, -3.2068e-02,  1.1217e-02,  2.2932e-02, -1.9656e-02,\n",
      "        -1.1480e-02, -4.0293e-02, -1.3056e-02,  1.3915e-03, -1.8542e-03,\n",
      "         2.5853e-03,  1.1675e-02,  8.2842e-03,  5.3499e-02,  1.4084e-02,\n",
      "        -5.2523e-04, -1.4435e-02, -1.1012e-02, -1.1957e-02,  4.5488e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01, -5.0611e-03, -1.9878e-01, -4.5751e-02, -5.5914e-02,\n",
      "        -1.2700e-01,  6.8076e-03, -8.6753e-02, -9.9861e-02, -1.8934e-01,\n",
      "        -9.8778e-03,  8.9483e-02, -9.8249e-02, -1.3364e-01, -5.7323e-02,\n",
      "        -1.0625e-01,  1.4270e-02,  1.0282e-01,  1.0627e-01,  5.3077e-02,\n",
      "        -8.7922e-02,  1.0112e-02,  9.9892e-03, -3.0280e-02, -4.7433e-02,\n",
      "        -2.5781e-02, -7.7191e-02, -3.7069e-02,  1.3998e-01, -5.0367e-02,\n",
      "         8.4484e-02,  2.4754e-02, -3.0628e-02, -1.8732e-02,  5.2444e-02,\n",
      "        -8.4405e-02,  8.7702e-02,  1.3026e-01,  7.2759e-02, -6.3521e-02,\n",
      "         4.3868e-03,  2.0207e-02,  7.9722e-02,  1.9510e-02, -6.6707e-02,\n",
      "         1.9529e-01, -9.5157e-02,  4.3410e-02, -1.8571e-01,  5.5557e-02,\n",
      "        -3.2018e-02,  1.3573e-01,  1.3202e-01, -8.7102e-02, -1.0513e-01,\n",
      "        -2.7712e-02, -2.2118e-02,  8.3508e-02, -8.7402e-02,  8.2012e-03,\n",
      "        -1.0301e-02, -4.9594e-02, -6.5496e-02,  3.3057e-02, -6.2494e-02,\n",
      "         3.4290e-02, -3.6536e-02, -1.0531e-02, -2.0122e-02,  7.5899e-02,\n",
      "         6.6991e-02, -6.8429e-02,  5.3785e-02,  6.7810e-02,  7.1414e-02,\n",
      "        -1.7847e-02,  5.6376e-02,  2.9591e-01,  6.9702e-02, -9.3077e-02,\n",
      "         2.8909e-02, -2.6832e-01, -2.7580e-02, -1.0516e-01, -1.5376e-03,\n",
      "         6.8740e-02,  1.6290e-02,  2.1003e-01, -5.1309e-02,  2.2166e-03,\n",
      "        -5.8085e-02,  1.1002e-01, -9.3043e-02,  1.6100e-02,  3.2328e-02,\n",
      "        -4.9213e-02, -2.5719e-02,  1.1063e-01,  1.5216e-02, -1.6942e-01,\n",
      "        -1.3368e-01, -4.3938e-02,  8.7666e-02, -4.3411e-02, -4.3867e-02,\n",
      "         7.0452e-02,  1.5908e-01, -6.2414e-02,  1.9933e-02,  5.8436e-02,\n",
      "        -9.1027e-02,  8.3513e-02, -1.3507e-01, -1.3979e-01,  9.5301e-02,\n",
      "         1.0012e-01,  5.7544e-02,  1.5061e-01,  7.7530e-02, -8.7945e-02,\n",
      "         3.4384e-02, -1.5814e-01, -9.4941e-02,  7.2848e-02, -1.2292e-02,\n",
      "        -1.4191e-01, -6.6335e-02, -1.0782e-01,  1.5550e-01,  8.7770e-03,\n",
      "        -2.9334e-02,  9.8987e-04,  2.6520e-02, -7.1951e-04,  7.1772e-02,\n",
      "         3.9691e-03,  5.6695e-03,  1.0388e-01, -5.5886e-02, -5.7733e-02,\n",
      "        -7.9684e-02, -8.8678e-02,  8.3504e-03,  8.5907e-02, -5.5615e-02,\n",
      "        -4.5347e-02, -4.5222e-02, -9.2290e-02, -2.0514e-02,  2.1300e-01,\n",
      "         5.6457e-02,  7.5923e-02, -1.6335e-01,  1.3578e-01, -4.8068e-02,\n",
      "         9.8002e-02,  4.4011e-02, -1.3319e-01,  1.5313e-01, -2.7080e-02,\n",
      "         1.4706e-01,  1.5676e-01, -1.6239e-02,  5.1673e-02,  1.1912e-01,\n",
      "         9.2421e-02,  5.8957e-02,  3.5697e-02, -9.3552e-02,  1.8525e-01,\n",
      "        -9.8530e-02,  1.6141e-01,  1.2983e-01, -1.1521e-01,  1.0152e-02,\n",
      "        -6.9838e-02,  1.6544e-02, -6.3233e-02, -6.9107e-02,  2.7730e-02,\n",
      "        -2.2096e-02, -4.0989e-02,  6.5758e-03, -3.2915e-02, -3.2057e-02,\n",
      "         9.9635e-02, -9.7542e-03])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5012],\n",
      "        [0.4905],\n",
      "        [0.4999],\n",
      "        ...,\n",
      "        [0.5122],\n",
      "        [0.4932],\n",
      "        [0.4936]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([ 1.1849e-02,  3.6974e-02,  3.0523e-02,  9.6236e-03, -3.4336e-02,\n",
      "         1.8046e-02,  3.3743e-03, -8.1600e-03, -1.0559e-02,  4.8060e-03,\n",
      "        -1.0853e-02, -5.3391e-03, -1.3398e-02, -3.7102e-02,  2.6809e-02,\n",
      "        -1.4682e-02,  1.3610e-02,  1.1284e-02,  6.8024e-03,  1.1924e-02,\n",
      "         3.6682e-02, -8.0556e-03,  7.5089e-03, -7.9265e-04,  9.4888e-04,\n",
      "        -3.4862e-04,  3.8463e-02,  1.0441e-02,  1.2181e-03,  4.0839e-02,\n",
      "        -4.3530e-02,  8.0974e-03, -2.4311e-02, -5.0240e-02,  3.2869e-02,\n",
      "         4.6581e-02,  7.0486e-03, -4.2347e-02, -2.1849e-02, -8.5194e-03,\n",
      "        -4.9161e-03, -4.3568e-04, -1.5568e-02,  1.5636e-02, -1.4755e-02,\n",
      "         3.2295e-02, -3.5442e-02,  1.9728e-02, -3.0255e-02, -1.4138e-02,\n",
      "        -3.5564e-02,  3.7701e-02,  3.5110e-02,  9.4050e-03,  2.6784e-02,\n",
      "        -2.0204e-02, -3.1220e-02,  1.1151e-02,  8.2635e-03,  4.0154e-02,\n",
      "         6.9275e-03, -5.3168e-05,  2.6160e-02, -4.1693e-03,  2.2876e-02,\n",
      "        -4.5651e-02,  4.7579e-03,  3.5784e-03,  3.6416e-03, -3.2857e-02,\n",
      "        -2.5302e-02, -2.3053e-03, -6.0430e-03,  2.5136e-02, -6.0685e-03,\n",
      "        -4.1481e-02,  4.0376e-03,  5.7679e-02,  1.5205e-02, -7.2454e-03,\n",
      "         1.2988e-02,  4.2376e-04, -3.8742e-02, -1.0195e-02, -1.4313e-02,\n",
      "         3.8172e-02,  3.0392e-02,  2.5863e-02,  7.3924e-03,  8.3759e-03,\n",
      "        -1.3028e-02, -1.6951e-02, -1.4635e-02,  1.1608e-02, -8.6634e-03,\n",
      "        -4.7303e-02,  1.2277e-02,  2.5443e-02, -4.6390e-02, -3.9775e-02,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01,  1.1864e-01,  9.4096e-02,  1.6913e-01, -7.3317e-02,\n",
      "        -1.2582e-01,  1.7851e-01,  3.2729e-02, -7.2011e-02, -6.7449e-02,\n",
      "         6.0649e-02,  6.9389e-02,  4.2670e-02,  9.2635e-02,  6.8150e-02,\n",
      "         6.0848e-02, -1.8032e-02, -2.4555e-02, -4.9974e-02,  9.4586e-02,\n",
      "         4.1442e-02,  8.2468e-02, -8.2148e-02, -9.0737e-02,  1.6294e-01,\n",
      "         1.7003e-01, -4.6837e-02,  1.4761e-01, -6.8569e-02, -3.7269e-02,\n",
      "         1.4313e-01, -7.5411e-02,  1.1672e-01, -2.1835e-02,  4.1288e-02,\n",
      "         1.3186e-02,  1.6459e-01, -7.3384e-02,  1.3681e-01, -3.0413e-02,\n",
      "        -3.3663e-03, -2.5469e-02, -8.7707e-03, -4.5466e-02, -3.8952e-02,\n",
      "        -1.4572e-02, -1.3782e-01, -1.0538e-01,  1.1287e-01, -1.3148e-02,\n",
      "         4.1461e-02, -4.8989e-02, -7.6214e-02, -5.9928e-02,  4.8012e-02,\n",
      "        -1.5560e-02,  3.0253e-03,  1.5510e-01, -5.3381e-02, -1.6836e-01,\n",
      "         3.8060e-02, -2.5102e-01,  5.7738e-03, -1.4668e-01,  1.7611e-01,\n",
      "         1.6828e-01, -2.4304e-02,  1.4140e-01, -2.2369e-02,  3.4438e-02,\n",
      "        -6.6804e-02, -1.2411e-01,  1.1929e-01,  3.9154e-02, -1.2100e-01,\n",
      "        -1.0891e-01, -7.8803e-02,  9.8305e-02,  2.4314e-01,  7.5629e-02,\n",
      "         4.3394e-02, -6.8291e-02,  1.5513e-01, -3.5734e-02, -2.9805e-02,\n",
      "        -4.4777e-02, -1.4093e-04,  7.1768e-02,  2.6267e-03,  6.8964e-02,\n",
      "         2.3002e-02,  5.3750e-03,  9.3999e-02, -1.1781e-01, -8.4733e-02,\n",
      "        -4.8841e-02, -2.5035e-02,  4.9670e-03, -8.8441e-02, -6.7175e-02,\n",
      "        -7.1477e-03,  1.2066e-02, -1.2356e-01,  9.3097e-02,  6.9052e-02,\n",
      "        -6.3208e-02,  5.2211e-02,  1.1420e-01,  1.7035e-01,  1.0379e-01,\n",
      "        -1.1137e-01, -8.8253e-02,  2.3506e-02,  8.0937e-02, -9.9964e-02,\n",
      "         1.5211e-02, -4.9809e-03, -3.2424e-02,  2.9962e-02, -1.3097e-01,\n",
      "         6.9101e-02, -6.8605e-02, -2.6001e-01, -1.2077e-01, -1.3967e-02,\n",
      "        -9.7246e-02, -7.6551e-02, -9.6022e-02, -3.1129e-02,  2.8154e-02,\n",
      "         5.5383e-02,  1.0243e-01,  5.1915e-02, -3.6888e-02, -1.4943e-01,\n",
      "        -8.7239e-02,  4.9075e-02, -2.0265e-02,  9.4719e-02,  5.5995e-02,\n",
      "         1.7869e-01,  2.7599e-01,  8.3025e-02, -7.9000e-04, -8.6888e-02,\n",
      "         6.8656e-02, -1.6182e-01, -3.3424e-02, -5.4173e-02,  1.5058e-01,\n",
      "        -4.6320e-04, -7.6341e-02,  6.9482e-03,  2.3047e-02, -6.7506e-02,\n",
      "         7.8477e-02,  2.7260e-02, -1.6897e-01,  2.3607e-02,  1.0809e-01,\n",
      "         2.5133e-02,  5.1339e-02, -1.0005e-02, -4.6682e-02,  2.1741e-02,\n",
      "         9.1686e-02, -4.5773e-02, -7.3479e-02,  9.8366e-02, -1.0190e-01,\n",
      "         4.9121e-02, -1.1972e-01, -2.0710e-01,  2.1268e-02,  4.2378e-02,\n",
      "        -1.9311e-01, -5.1155e-02, -2.1584e-02, -6.3883e-02, -9.0442e-02,\n",
      "         8.9499e-02,  2.1845e-02,  1.7540e-02, -2.4530e-02,  6.5971e-02,\n",
      "        -5.8029e-02,  1.1983e-01])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5464],\n",
      "        [0.5416],\n",
      "        [0.5208],\n",
      "        ...,\n",
      "        [0.5440],\n",
      "        [0.5504],\n",
      "        [0.5289]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-1.8836e-02, -7.1972e-03,  1.0825e-04, -5.4474e-03,  3.7897e-03,\n",
      "        -4.0488e-02, -2.3401e-02, -2.5333e-02, -2.6999e-02, -4.2913e-03,\n",
      "        -1.0909e-02,  2.1828e-03,  2.9504e-02, -1.6529e-03, -6.4342e-03,\n",
      "         7.7472e-03, -1.8791e-02, -2.3464e-02, -1.6346e-02,  8.6021e-03,\n",
      "        -6.3106e-03, -2.4100e-03,  3.4562e-02,  1.0701e-02,  1.2906e-02,\n",
      "         1.7102e-02,  2.3661e-03,  1.3348e-02, -5.6371e-03,  7.5990e-03,\n",
      "        -1.0034e-02,  1.7085e-02,  5.3278e-02, -1.0580e-02,  1.6893e-03,\n",
      "         1.9967e-02, -1.9046e-02,  1.7482e-02,  2.6914e-04,  7.4219e-03,\n",
      "         2.2204e-02, -5.3262e-03,  4.5663e-03, -4.4916e-02, -3.9041e-02,\n",
      "        -2.1919e-02, -3.2239e-02, -2.3645e-02,  3.1871e-02,  3.4782e-02,\n",
      "        -3.1820e-02,  5.6933e-03,  1.4822e-02,  2.5069e-02,  1.1422e-02,\n",
      "        -1.0198e-02,  9.5208e-03,  3.9430e-03, -3.3106e-03, -2.7863e-03,\n",
      "        -4.5633e-03, -3.0815e-02, -2.0452e-02,  2.5429e-02, -3.2546e-02,\n",
      "        -6.2576e-03, -1.6314e-02,  1.8307e-02,  2.0025e-03, -1.3028e-02,\n",
      "        -8.0254e-03,  1.4895e-02,  2.0210e-02, -4.2289e-02, -3.3122e-02,\n",
      "        -3.4568e-03,  1.6039e-02,  3.1952e-02,  3.4290e-03, -1.9661e-02,\n",
      "         1.1618e-02, -1.4588e-02,  1.2839e-02,  2.2490e-02,  4.4325e-03,\n",
      "         7.5740e-03, -8.2381e-03, -3.4496e-02, -2.7648e-02, -2.0458e-02,\n",
      "        -2.9749e-02,  1.5179e-02,  1.0521e-02, -3.7844e-03,  2.8504e-02,\n",
      "         8.7575e-03, -1.4562e-02, -2.9148e-03,  1.1988e-03, -2.0979e-02,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01,  1.1344e-01, -7.9871e-02, -8.5407e-02,  1.4582e-01,\n",
      "        -3.2757e-02,  3.7623e-02, -2.8934e-02, -1.8251e-02, -8.1590e-03,\n",
      "        -7.3814e-02,  1.1431e-02,  3.6217e-02,  6.5776e-02, -2.1148e-01,\n",
      "        -1.2258e-02, -4.1534e-02,  4.1368e-02,  1.7809e-01, -4.5077e-02,\n",
      "        -6.0738e-02, -9.7060e-02,  1.2766e-02,  5.5046e-02, -1.0113e-01,\n",
      "        -4.8381e-03,  1.6891e-01, -9.6126e-02, -1.3801e-01, -2.0413e-01,\n",
      "         9.3697e-02, -7.0819e-03,  3.6064e-02,  5.1647e-02,  1.7802e-01,\n",
      "         8.9538e-03,  1.3335e-01,  4.6051e-02,  1.2344e-02,  5.5820e-02,\n",
      "         3.8323e-02, -1.3920e-01,  1.3061e-01,  4.1663e-03,  2.9641e-02,\n",
      "         3.7695e-02,  7.7314e-02, -9.0097e-02,  6.1157e-02, -1.0279e-02,\n",
      "         1.8184e-02, -1.6054e-01,  4.4496e-02,  3.8079e-02,  4.4643e-02,\n",
      "         2.0220e-01, -9.7872e-02,  4.8527e-03,  1.2935e-01,  2.0613e-02,\n",
      "        -5.4465e-02,  6.3328e-02, -5.5510e-02, -5.3476e-02,  2.3698e-02,\n",
      "        -2.1935e-01, -5.5630e-02,  1.4949e-01, -7.3818e-02, -1.4375e-01,\n",
      "         1.4255e-01,  1.2602e-01,  4.4512e-02,  5.4293e-02, -5.0192e-02,\n",
      "         1.0984e-01,  1.0898e-01, -1.7269e-02, -2.3007e-01, -6.9893e-02,\n",
      "        -1.2321e-01, -4.4078e-02,  6.9153e-02, -8.0653e-02, -4.0304e-02,\n",
      "        -8.9478e-02, -8.1018e-02,  5.6842e-02, -8.0915e-02, -3.3954e-02,\n",
      "        -7.2943e-02, -2.3627e-02, -6.7473e-02, -1.0300e-02,  1.9349e-01,\n",
      "        -7.6058e-02,  1.4447e-01,  1.3049e-01, -1.2972e-01,  2.5522e-02,\n",
      "        -6.1516e-02,  1.1612e-01,  6.6625e-02,  1.2577e-01,  8.4707e-03,\n",
      "         4.7015e-02, -4.2762e-03,  1.0279e-01,  1.3109e-01,  6.3574e-02,\n",
      "        -7.1245e-02, -1.6600e-01,  6.1988e-02, -2.8271e-02,  2.7161e-02,\n",
      "         1.7744e-01, -3.4689e-02,  6.5919e-02, -6.0873e-02,  3.0123e-02,\n",
      "        -2.1547e-01, -8.7928e-03,  1.2777e-01,  4.6412e-02, -1.8962e-01,\n",
      "        -9.3307e-02, -5.8541e-02, -1.8170e-01, -2.9500e-02, -9.6319e-02,\n",
      "        -4.1297e-02, -3.9536e-02,  3.6606e-02,  5.9094e-02, -1.3765e-01,\n",
      "         9.3465e-02, -1.3053e-01, -1.1166e-01,  4.7812e-02, -7.9122e-02,\n",
      "        -1.4468e-01,  6.4589e-02,  1.5440e-01, -1.3994e-01,  4.3705e-02,\n",
      "        -4.7201e-02, -2.0883e-01,  2.0489e-01, -1.5204e-01,  4.3600e-02,\n",
      "         7.3083e-02, -1.0179e-01, -7.2215e-02, -2.1350e-01, -7.0129e-02,\n",
      "        -1.0280e-01, -7.1476e-03, -1.4497e-02,  2.1344e-01,  5.3437e-02,\n",
      "         4.4503e-02,  7.7946e-02,  1.6829e-02, -9.7433e-02, -1.1417e-01,\n",
      "        -1.4357e-01,  2.6581e-02,  6.2384e-02,  3.6172e-02, -1.1437e-01,\n",
      "         8.2808e-02, -1.3009e-01, -6.5729e-02, -6.0876e-02, -6.1068e-03,\n",
      "        -1.3929e-01,  6.5485e-02,  8.7413e-02, -6.8982e-02,  5.1834e-02,\n",
      "         1.0623e-01,  1.5751e-01, -9.3843e-02, -1.7104e-01, -1.3079e-01,\n",
      "         1.0938e-01, -1.1570e-01])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5145],\n",
      "        [0.5306],\n",
      "        [0.4981],\n",
      "        ...,\n",
      "        [0.5115],\n",
      "        [0.5293],\n",
      "        [0.4939]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-2.3578e-03, -3.1656e-02,  1.0924e-02, -2.0817e-02, -2.1490e-02,\n",
      "        -2.1875e-02, -2.0298e-02, -2.5494e-02,  1.0573e-02,  3.9053e-02,\n",
      "         2.6909e-02,  4.2231e-02, -9.1552e-04,  2.3825e-02, -1.8355e-02,\n",
      "        -5.3992e-03, -3.1493e-02,  1.7370e-02, -4.6173e-03, -6.7737e-04,\n",
      "         4.3214e-02, -4.5224e-04,  1.2211e-02,  1.2671e-02,  4.6550e-02,\n",
      "        -1.6780e-02,  5.8651e-03, -2.2058e-02,  3.0949e-02, -4.2556e-02,\n",
      "         3.0013e-02,  9.3954e-03, -1.7067e-02,  1.3528e-02, -2.6677e-02,\n",
      "         1.0172e-02,  3.5300e-03,  1.1015e-02,  2.0657e-02,  1.5038e-02,\n",
      "         2.1676e-02,  3.3337e-02, -3.5001e-02, -7.2917e-04, -2.6556e-02,\n",
      "        -1.0047e-02, -4.1304e-02, -1.5635e-02, -2.4679e-02, -1.0062e-02,\n",
      "         2.0890e-03,  4.0801e-03, -2.3877e-02, -4.6371e-02,  6.9499e-03,\n",
      "         2.9266e-04,  1.3892e-02,  1.4686e-02, -9.0744e-03,  2.3168e-02,\n",
      "         2.4852e-02,  1.2318e-02, -1.4879e-02, -9.7093e-03,  6.6927e-02,\n",
      "        -1.5984e-02,  9.8243e-03,  2.5260e-03, -1.2366e-02,  4.4039e-04,\n",
      "         3.4540e-02, -1.7074e-02,  5.6774e-04,  3.9257e-03,  2.2842e-02,\n",
      "        -1.7955e-02,  1.1198e-02, -2.3861e-02,  4.1407e-03,  1.7302e-05,\n",
      "         2.0878e-03,  1.1027e-03, -3.2335e-02, -8.0490e-03, -2.2740e-02,\n",
      "         4.6093e-02, -1.5623e-02, -1.4273e-02,  2.8761e-02, -2.2386e-02,\n",
      "         2.2854e-02, -2.2987e-02,  2.9183e-02, -1.9037e-02, -1.1585e-02,\n",
      "         1.2865e-02, -3.2256e-02, -3.6692e-02,  4.1657e-02,  1.9328e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01, -5.3037e-02, -6.4612e-02, -1.2656e-02,  2.0369e-02,\n",
      "         1.0650e-01, -2.8253e-02, -1.2364e-02,  1.3117e-01,  5.5816e-02,\n",
      "         6.5289e-02,  1.1693e-01,  5.9546e-02, -1.1097e-01, -1.4709e-01,\n",
      "        -1.4113e-01, -1.0254e-01, -1.1042e-01,  1.8450e-02, -9.7147e-02,\n",
      "         7.7355e-02,  7.1406e-02,  9.5215e-03,  1.6358e-01, -1.2366e-01,\n",
      "        -7.5923e-02, -1.3098e-01,  1.0096e-02, -1.3476e-01,  8.3616e-03,\n",
      "        -6.3713e-02,  1.0021e-01, -3.9008e-03,  6.9448e-02, -4.6316e-02,\n",
      "         6.6813e-03,  1.2090e-01,  5.4112e-03,  1.0461e-01,  1.0696e-01,\n",
      "        -1.0120e-01,  4.0838e-02,  4.1994e-02, -4.4730e-02, -2.2424e-02,\n",
      "         6.2740e-03,  9.3350e-04, -9.9099e-02,  5.8126e-02, -4.6484e-02,\n",
      "         7.7517e-02, -1.2988e-01,  1.1709e-01, -1.0365e-01,  1.1917e-01,\n",
      "        -1.7960e-01,  2.7326e-03,  5.7018e-02,  5.4102e-02, -9.2017e-02,\n",
      "        -1.3080e-01, -6.0714e-02,  5.8363e-02,  6.9458e-02, -7.6227e-02,\n",
      "         1.0410e-01, -1.1527e-02,  6.5028e-02,  1.9958e-03, -5.5967e-02,\n",
      "        -7.4387e-02,  9.5896e-03, -1.5670e-01, -2.1510e-02,  6.1767e-02,\n",
      "        -1.4921e-02, -7.5328e-03, -2.4218e-02, -2.7321e-02, -2.3298e-02,\n",
      "        -1.7167e-01, -1.9725e-02, -1.3492e-01, -1.3450e-01,  2.0228e-02,\n",
      "        -4.8198e-02,  3.9747e-02,  1.4878e-01, -2.2605e-02,  3.2692e-03,\n",
      "        -6.1999e-02, -1.4930e-01, -1.2189e-01,  6.0749e-02, -2.4982e-02,\n",
      "        -3.5763e-02,  1.3649e-01,  1.6691e-02,  1.4982e-01, -3.1789e-03,\n",
      "         1.9232e-01,  3.0070e-02, -9.9976e-02,  4.9816e-02, -2.6531e-02,\n",
      "        -1.2343e-01,  7.0561e-02, -1.7811e-01, -1.5852e-01, -1.2330e-01,\n",
      "        -1.2351e-01,  6.5087e-02, -2.3782e-01, -1.4500e-01, -4.6101e-03,\n",
      "         1.2527e-01, -7.4758e-02, -6.4967e-02,  1.0718e-01, -2.9983e-01,\n",
      "        -1.7245e-01, -1.5951e-01, -5.2220e-02, -4.9484e-02, -2.3880e-02,\n",
      "         4.8107e-02,  1.3031e-01,  1.4974e-02,  1.3449e-01, -1.8837e-01,\n",
      "         5.4635e-02, -1.3670e-01, -1.4717e-01, -8.7103e-02,  7.1799e-02,\n",
      "         4.0745e-02, -5.4708e-02, -2.4831e-01, -1.4673e-01,  1.6356e-02,\n",
      "        -1.4066e-01, -3.4818e-02, -1.0813e-01, -5.9019e-02,  1.3665e-01,\n",
      "        -9.5839e-02, -2.9822e-02, -6.0841e-02,  5.9189e-02,  8.6612e-02,\n",
      "         2.6573e-02, -1.6147e-02, -1.3013e-01, -2.7771e-01,  1.2751e-02,\n",
      "        -9.1248e-02, -1.8397e-01,  1.0587e-01,  3.1946e-02, -9.9014e-02,\n",
      "         3.7051e-02,  9.9692e-02,  7.4196e-02,  9.6182e-02, -9.4577e-03,\n",
      "         9.0514e-02, -2.1595e-02, -6.9314e-02,  6.5422e-03,  3.4964e-02,\n",
      "         4.9661e-02, -8.4541e-02,  5.2338e-03,  9.1570e-02,  3.1611e-02,\n",
      "         7.6643e-02, -8.1455e-02, -1.0253e-01,  1.4892e-01, -3.7145e-02,\n",
      "         1.2253e-01, -5.7567e-02, -9.0934e-02,  3.9093e-02, -1.2146e-02,\n",
      "        -2.0146e-01, -5.9345e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5614],\n",
      "        [0.5500],\n",
      "        [0.5455],\n",
      "        ...,\n",
      "        [0.5431],\n",
      "        [0.5400],\n",
      "        [0.5595]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-6.5074e-04, -7.7318e-03, -3.1878e-02, -2.1887e-02,  3.9074e-02,\n",
      "        -3.0353e-02, -3.6731e-03,  1.3989e-02, -1.5789e-02,  3.4520e-02,\n",
      "        -4.7985e-02,  2.8485e-02,  3.4783e-02, -2.0549e-02, -2.7424e-02,\n",
      "         1.1552e-02, -9.4143e-03,  2.5547e-02, -9.8826e-03,  1.5711e-02,\n",
      "        -1.7968e-03, -7.7998e-03, -1.6814e-02, -2.1617e-03, -2.8707e-02,\n",
      "        -4.4378e-02, -1.7166e-02,  2.9603e-02, -1.5448e-03,  4.8931e-02,\n",
      "         4.2158e-02,  2.9310e-02, -4.7978e-02, -2.7227e-03,  1.5657e-02,\n",
      "         7.7461e-03,  2.7378e-02, -1.4316e-02,  1.5035e-03,  1.1344e-02,\n",
      "        -2.1987e-02,  6.3919e-03,  2.3890e-02, -1.6841e-02, -2.2688e-02,\n",
      "        -1.0133e-02,  4.2765e-03, -2.9547e-02,  2.9494e-03,  1.8619e-02,\n",
      "         1.5514e-02,  2.1961e-02, -2.3104e-02, -1.5184e-02,  1.1630e-02,\n",
      "         1.4618e-02,  3.8955e-02, -2.9242e-02, -8.4223e-03,  1.7737e-02,\n",
      "         4.9085e-03,  9.7815e-03,  3.1035e-03,  6.9047e-03,  4.1109e-03,\n",
      "        -8.5439e-03,  1.1434e-03, -2.7331e-02, -1.1360e-02,  4.5456e-02,\n",
      "        -9.3775e-03, -2.5920e-03, -2.2652e-02, -2.0100e-02,  1.0569e-02,\n",
      "        -3.8518e-02, -7.6172e-03, -2.7328e-02, -1.8145e-02, -2.1748e-02,\n",
      "        -4.7325e-02,  3.0307e-02,  1.9706e-02,  1.1551e-02, -6.9541e-03,\n",
      "         2.0734e-02,  1.1468e-02,  4.7605e-02, -3.6598e-03,  1.8416e-02,\n",
      "         4.0534e-03, -2.1683e-02, -9.7999e-03,  1.4482e-02, -5.9296e-03,\n",
      "        -4.9896e-02,  1.0408e-02, -8.4046e-03, -1.5070e-02,  1.7387e-02,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01,  1.6284e-02,  3.7299e-02,  2.9921e-02,  2.1538e-02,\n",
      "         9.2246e-02, -8.3238e-02, -3.0195e-02,  4.1280e-02,  1.9273e-02,\n",
      "        -2.8026e-02,  5.0986e-02, -1.2818e-01, -1.0167e-01,  4.1195e-02,\n",
      "        -1.9773e-01, -7.8265e-02,  1.3307e-01,  3.2040e-02, -1.2283e-01,\n",
      "         1.1074e-01, -7.7716e-02,  1.0047e-01, -2.2180e-01,  7.5971e-02,\n",
      "        -1.5413e-02, -2.5177e-02,  5.6307e-02, -6.4463e-02,  1.4084e-01,\n",
      "         1.2294e-01, -6.4920e-02, -1.1836e-02, -5.9466e-03,  1.1079e-01,\n",
      "        -6.6297e-03, -4.4494e-02,  9.2095e-02,  2.0412e-02,  8.3495e-02,\n",
      "        -3.5468e-02, -2.7704e-02, -2.1226e-02,  8.4190e-02,  2.1554e-02,\n",
      "         1.6328e-02, -1.2226e-01, -9.0105e-02,  1.8910e-01,  6.0023e-02,\n",
      "        -8.1886e-02, -5.7320e-02,  1.2403e-01, -1.7525e-02, -1.2487e-01,\n",
      "        -1.6979e-02, -4.8632e-02, -1.9689e-01,  1.0074e-01, -1.3514e-01,\n",
      "        -1.0375e-01, -1.2734e-01,  7.5887e-03, -9.5318e-02, -3.0876e-02,\n",
      "        -1.6283e-02, -4.0749e-02, -1.4193e-02, -1.2853e-01, -5.1123e-02,\n",
      "        -2.2568e-01, -1.0010e-01,  3.6668e-02, -6.9163e-02,  1.0510e-01,\n",
      "        -2.1154e-02, -1.1144e-01, -4.0573e-02, -5.1493e-02,  2.0740e-01,\n",
      "        -1.4071e-01, -6.4076e-02, -1.5124e-01,  9.3160e-02, -3.0520e-02,\n",
      "        -6.7146e-02, -7.3329e-02, -9.8857e-03,  1.4808e-01, -3.6878e-03,\n",
      "         4.7766e-02, -3.6878e-03,  6.5683e-02, -6.6207e-02, -1.0381e-01,\n",
      "        -3.7097e-02, -4.8174e-02, -5.0945e-02,  3.6532e-02,  1.2216e-01,\n",
      "        -9.9027e-03, -5.5321e-03,  2.1383e-01, -7.6576e-02, -9.5312e-02,\n",
      "        -2.0190e-01,  7.4394e-02, -4.1494e-02,  2.2993e-01,  1.0027e-01,\n",
      "         9.9860e-03, -1.9258e-01, -2.4079e-02, -7.2371e-02, -3.6297e-02,\n",
      "         1.1653e-01,  1.3103e-01, -2.4879e-01,  1.5283e-01,  2.7308e-02,\n",
      "        -8.7098e-04, -1.4459e-01, -1.2722e-01, -2.1782e-01,  6.1843e-02,\n",
      "        -1.1557e-01,  5.8377e-02,  6.6721e-02,  3.0055e-02, -1.1874e-01,\n",
      "         1.4861e-02, -8.0662e-02,  3.2461e-02, -1.3589e-01,  5.1165e-03,\n",
      "        -3.5502e-03, -9.7414e-02, -9.5949e-02,  1.2432e-01, -8.5278e-02,\n",
      "         1.2970e-02, -2.1506e-01, -1.7590e-01, -5.9283e-02, -1.0396e-01,\n",
      "         1.2191e-02, -1.3854e-01,  3.5100e-02, -1.4543e-01, -1.9751e-02,\n",
      "        -8.7387e-02,  5.4023e-02, -6.9929e-02,  1.4977e-01,  4.9735e-02,\n",
      "        -4.4503e-02,  3.7359e-02,  5.9219e-02,  1.1511e-01, -1.3071e-01,\n",
      "         1.7240e-01,  4.4603e-02,  8.2957e-02, -7.6631e-02,  1.0967e-01,\n",
      "        -1.3824e-01,  2.3001e-02, -1.5314e-01, -9.6560e-02, -9.2021e-03,\n",
      "         2.3235e-02, -1.1952e-01,  3.4808e-02,  1.7258e-01,  2.7443e-02,\n",
      "        -1.5951e-01,  5.7151e-02,  3.8458e-02, -3.8964e-02, -2.5905e-02,\n",
      "        -1.2905e-01, -4.9132e-02, -6.8246e-02, -2.1413e-01,  6.6957e-02,\n",
      "        -6.7665e-02,  5.5196e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5007],\n",
      "        [0.5224],\n",
      "        [0.5169],\n",
      "        ...,\n",
      "        [0.5069],\n",
      "        [0.5253],\n",
      "        [0.5073]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1680, 8, 186])\n",
      "target_rep: torch.Size([1680, 1, 186])\n",
      "att_output: torch.Size([1680, 1, 186])\n",
      "attention_concat: torch.Size([1680, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1680, 372])\n",
      "tensor([-2.5551e-05,  2.4317e-02, -2.4115e-02,  6.4319e-03, -3.8154e-03,\n",
      "        -4.1208e-03,  8.4805e-03,  3.9551e-03, -3.3746e-02, -3.5890e-02,\n",
      "        -1.2824e-02,  1.8187e-02,  2.5578e-02, -4.0395e-02, -6.5163e-03,\n",
      "        -9.9705e-03, -1.6029e-02,  2.6771e-02,  1.0975e-03, -2.6086e-02,\n",
      "         2.2436e-02, -8.2844e-03, -1.0661e-02, -2.1019e-02, -6.4550e-03,\n",
      "         7.5076e-03, -4.2029e-04, -2.2165e-02, -5.6082e-03,  3.2347e-02,\n",
      "         2.1895e-02, -9.1574e-03, -3.3035e-02,  1.9750e-02, -5.4369e-02,\n",
      "         3.9702e-03, -2.6404e-02, -1.0620e-02,  9.2820e-03, -1.0267e-02,\n",
      "         1.3898e-02,  1.6512e-03, -2.5879e-03,  1.5341e-03,  3.4170e-03,\n",
      "        -1.0653e-03,  6.0817e-03,  1.5731e-02, -2.1589e-03, -5.0271e-03,\n",
      "        -2.1539e-03, -1.0040e-02, -2.9213e-02,  2.2235e-02,  3.0880e-02,\n",
      "         1.3379e-02,  1.5010e-02, -1.5149e-02, -6.1278e-03, -7.6560e-03,\n",
      "        -3.5399e-02,  2.2779e-02,  2.1371e-03,  3.5760e-04, -2.1777e-02,\n",
      "         3.9261e-02, -3.2894e-02,  2.7974e-02,  2.0428e-02,  1.9076e-02,\n",
      "        -1.8771e-03, -2.1669e-03,  5.7683e-03, -6.2250e-03, -9.2713e-03,\n",
      "        -2.5438e-02, -5.0627e-03, -5.2080e-02,  1.2292e-02, -2.8447e-02,\n",
      "         1.4818e-02,  3.2077e-02,  1.5390e-02, -1.0990e-02,  1.3473e-02,\n",
      "        -3.5248e-02, -3.1481e-02,  2.7382e-02,  2.2126e-02, -1.6843e-03,\n",
      "        -4.9287e-02, -2.5015e-02,  6.9021e-03, -3.4364e-04, -8.0982e-03,\n",
      "        -6.5559e-03, -1.1495e-02, -4.1141e-02, -9.0992e-03, -1.9512e-03,\n",
      "         3.7865e-01,  1.3037e-01,  2.0262e-01,  2.8841e-02,  3.3866e-01,\n",
      "         3.4812e-01,  2.7273e-01,  5.0000e-01,  2.1429e-01,  8.3333e-02,\n",
      "         2.5000e-01,  3.2258e-01,  4.1558e-01,  4.3822e-01,  1.4100e-01,\n",
      "         1.7667e-01,  4.5564e-01,  5.7143e-01,  4.2857e-01,  0.0000e+00,\n",
      "         9.7376e-01,  0.0000e+00,  6.6298e-02,  2.7072e-01,  3.2320e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7403e-01,  6.6298e-02,\n",
      "         5.3186e-01,  3.8674e-02,  1.9521e-01,  6.1563e-02,  2.8177e-01,\n",
      "         2.9006e-01,  3.8453e-01,  1.9890e-01,  0.0000e+00,  2.3941e-01,\n",
      "         7.2130e-02,  0.0000e+00,  0.0000e+00,  5.8056e-01,  2.3941e-01,\n",
      "         6.3325e-01,  3.0387e-01,  1.5470e-01,  1.7403e-01,  9.1160e-02,\n",
      "         2.9171e-01,  5.9565e-01,  4.1568e-01,  5.2881e-01,  3.1777e-01,\n",
      "         4.6624e-01,  4.7861e-01,  4.8412e-01,  3.2609e-01,  3.5443e-01,\n",
      "         4.8915e-01,  7.1380e-01,  3.9205e-01,  3.5000e-01,  4.0467e-01,\n",
      "         3.3897e-01,  3.3544e-01,  3.4565e-01,  4.6575e-01,  3.5000e-01,\n",
      "         0.0000e+00,  2.8205e-01,  2.4194e-01,  2.9577e-01,  2.2727e-01,\n",
      "         2.6667e-01,  7.6923e-02,  2.1429e-01,  3.2308e-01,  2.2222e-01,\n",
      "         1.6667e-01,  0.0000e+00,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         1.4286e-01, -1.8550e-02,  2.4062e-01,  1.3385e-01,  4.9542e-02,\n",
      "        -1.4419e-01,  1.8764e-02, -5.5591e-03,  7.1005e-03, -2.0121e-02,\n",
      "         2.1332e-02,  3.9888e-02, -1.8245e-02, -2.0249e-02,  4.3584e-02,\n",
      "        -2.7074e-02,  4.5962e-02,  2.1281e-02, -4.7641e-02, -1.1111e-02,\n",
      "         6.5477e-02,  2.8033e-02, -5.9599e-02, -1.5305e-01,  1.3580e-02,\n",
      "        -1.9565e-01, -4.7556e-02,  5.4863e-02, -4.9045e-02, -1.3411e-01,\n",
      "         2.1338e-01,  7.4220e-02, -1.1745e-01,  8.4070e-02, -9.9676e-02,\n",
      "        -9.8905e-02, -1.9143e-01,  8.6286e-02, -1.5703e-01, -1.9222e-01,\n",
      "        -8.4659e-02, -1.4615e-01, -6.9953e-03,  1.7010e-01, -6.7151e-02,\n",
      "        -1.3845e-01,  1.4398e-01, -8.1919e-03,  5.2460e-02, -1.3720e-01,\n",
      "         3.0253e-01, -3.2627e-02,  8.0126e-02,  5.2473e-02, -1.0720e-02,\n",
      "         2.0583e-02, -9.6097e-02,  1.2522e-01, -8.9012e-02,  9.0091e-02,\n",
      "         2.2299e-02, -5.0461e-02, -9.1319e-02,  1.4135e-01,  1.0856e-01,\n",
      "        -3.3722e-02, -1.0112e-03, -8.8639e-02, -2.1418e-02, -3.0203e-02,\n",
      "         2.0425e-02, -1.0370e-01,  9.2745e-02, -1.1240e-01,  8.1041e-02,\n",
      "         1.2989e-01, -1.2387e-01, -3.1582e-03, -1.2254e-01,  7.8413e-02,\n",
      "         8.7138e-02,  3.5574e-02, -6.7608e-02, -1.2773e-02,  6.3893e-02,\n",
      "         9.1336e-03,  5.3376e-02, -1.1752e-02,  2.3307e-02,  9.1468e-02,\n",
      "        -1.4877e-01, -5.8699e-02, -5.4115e-02, -9.8447e-02, -1.5104e-02,\n",
      "        -1.8873e-01, -7.0981e-02,  1.8434e-01,  4.9128e-02, -2.2328e-02,\n",
      "         8.2722e-03, -7.1194e-03, -2.0640e-01,  1.7634e-02,  6.6206e-02,\n",
      "         6.3509e-02,  1.3119e-01, -1.7337e-02, -3.4243e-02, -2.0046e-02,\n",
      "         5.2821e-02, -1.0081e-01, -1.9893e-02, -1.9911e-01,  1.1884e-01,\n",
      "         2.1779e-02, -3.2637e-02, -1.4638e-01, -4.5704e-02,  8.2116e-02,\n",
      "         6.0396e-02,  4.3955e-02,  1.9561e-01,  1.5368e-02, -1.6921e-03,\n",
      "        -5.7611e-02,  2.3571e-02, -1.3759e-01, -5.3216e-02, -5.8908e-02,\n",
      "        -1.1116e-01,  6.7700e-02, -5.8753e-02, -1.4189e-01, -1.1878e-01,\n",
      "         9.2653e-02,  8.0656e-02, -1.4523e-01, -5.3753e-02, -2.8371e-02,\n",
      "        -7.5797e-02, -1.5949e-01,  1.1834e-01,  1.8339e-03, -6.9625e-02,\n",
      "         9.1126e-03,  4.1839e-02,  5.4944e-02, -4.8561e-02, -5.4285e-02,\n",
      "        -5.1217e-02, -8.8725e-03,  8.1448e-02, -5.8088e-02,  4.0100e-03,\n",
      "        -1.3912e-01,  8.5160e-02, -1.7368e-01, -2.0664e-01,  1.0310e-01,\n",
      "        -8.0267e-02, -1.1989e-01, -2.7694e-01,  8.5779e-02,  1.7703e-01,\n",
      "         1.0075e-01,  5.6356e-02, -1.0475e-01,  5.2388e-02,  1.0679e-01,\n",
      "        -3.8927e-02,  3.8072e-02, -9.3381e-02, -1.6603e-01,  1.4172e-01,\n",
      "         3.1635e-02, -1.0116e-01, -1.3712e-01,  5.0953e-02, -2.0401e-01,\n",
      "         3.6478e-02,  1.7257e-01, -3.1936e-02,  1.1254e-01, -8.4820e-02,\n",
      "        -1.2565e-01,  6.6780e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.4939],\n",
      "        [0.4981],\n",
      "        [0.5082],\n",
      "        ...,\n",
      "        [0.5080],\n",
      "        [0.5077],\n",
      "        [0.5139]])\n",
      "################################\n",
      "y: torch.Size([1680, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_drop_x:  torch.Size([1783, 4850, 50])\n",
      "pos_resh_W torch.Size([1783, 97, 50, 50])\n",
      "pos_zcnn torch.Size([1783, 97, 46, 100])\n",
      "pos_avg_zcnn torch.Size([1783, 97, 100])\n",
      "prompt_zcnn torch.Size([1783, 97, 46, 100])\n",
      "prompt_avg_zcnn torch.Size([1783, 97, 100])\n",
      "prompt_MA_lstm shape:  torch.Size([1783, 97, 100])\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0232, -0.0094,  0.0136,  ..., -0.1460, -0.0373, -0.0200],\n",
      "          [-0.0451, -0.0851, -0.0589,  ..., -0.0301,  0.0033,  0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0234, -0.0093,  0.0135,  ..., -0.1459, -0.0374, -0.0198],\n",
      "          [-0.0450, -0.0852, -0.0589,  ..., -0.0301,  0.0030,  0.0017]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0231, -0.0095,  0.0139,  ..., -0.1457, -0.0380, -0.0202],\n",
      "          [-0.0451, -0.0844, -0.0590,  ..., -0.0304,  0.0031,  0.0018]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0235, -0.0094,  0.0142,  ..., -0.1459, -0.0381, -0.0200],\n",
      "          [-0.0454, -0.0845, -0.0591,  ..., -0.0306,  0.0030,  0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0236, -0.0092,  0.0143,  ..., -0.1459, -0.0377, -0.0200],\n",
      "          [-0.0450, -0.0849, -0.0589,  ..., -0.0301,  0.0033,  0.0013]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0233, -0.0094,  0.0140,  ..., -0.1457, -0.0377, -0.0200],\n",
      "          [-0.0451, -0.0848, -0.0588,  ..., -0.0298,  0.0032,  0.0018]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0872, -0.0714, -0.0726,  ...,  0.0147, -0.0315,  0.0204],\n",
      "          [ 0.0431, -0.0762, -0.0031,  ..., -0.0158, -0.0196,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[-0.0876, -0.0713, -0.0728,  ...,  0.0151, -0.0313,  0.0205],\n",
      "          [ 0.0431, -0.0767, -0.0030,  ..., -0.0158, -0.0197,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[-0.0874, -0.0714, -0.0726,  ...,  0.0147, -0.0315,  0.0203],\n",
      "          [ 0.0432, -0.0761, -0.0032,  ..., -0.0156, -0.0198,  0.0817]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0879, -0.0710, -0.0732,  ...,  0.0152, -0.0311,  0.0204],\n",
      "          [ 0.0431, -0.0767, -0.0032,  ..., -0.0156, -0.0198,  0.0816]]],\n",
      "\n",
      "\n",
      "        [[[-0.0878, -0.0714, -0.0727,  ...,  0.0154, -0.0312,  0.0203],\n",
      "          [ 0.0435, -0.0767, -0.0033,  ..., -0.0158, -0.0200,  0.0815]]],\n",
      "\n",
      "\n",
      "        [[[-0.0876, -0.0718, -0.0724,  ...,  0.0151, -0.0312,  0.0202],\n",
      "          [ 0.0434, -0.0767, -0.0032,  ..., -0.0155, -0.0201,  0.0816]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0425,  0.0960, -0.0449,  ..., -0.0635, -0.0925,  0.0796],\n",
      "          [-0.0988,  0.0870, -0.0794,  ...,  0.0343, -0.0361, -0.0285]]],\n",
      "\n",
      "\n",
      "        [[[-0.0427,  0.0964, -0.0451,  ..., -0.0634, -0.0925,  0.0792],\n",
      "          [-0.0990,  0.0872, -0.0800,  ...,  0.0341, -0.0363, -0.0284]]],\n",
      "\n",
      "\n",
      "        [[[-0.0422,  0.0962, -0.0448,  ..., -0.0633, -0.0926,  0.0796],\n",
      "          [-0.0990,  0.0869, -0.0790,  ...,  0.0340, -0.0363, -0.0288]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0430,  0.0962, -0.0450,  ..., -0.0634, -0.0931,  0.0795],\n",
      "          [-0.0992,  0.0870, -0.0797,  ...,  0.0342, -0.0359, -0.0287]]],\n",
      "\n",
      "\n",
      "        [[[-0.0425,  0.0962, -0.0450,  ..., -0.0634, -0.0930,  0.0798],\n",
      "          [-0.0992,  0.0869, -0.0793,  ...,  0.0341, -0.0363, -0.0288]]],\n",
      "\n",
      "\n",
      "        [[[-0.0423,  0.0965, -0.0452,  ..., -0.0634, -0.0924,  0.0797],\n",
      "          [-0.0989,  0.0865, -0.0791,  ...,  0.0340, -0.0364, -0.0287]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0592, -0.0543, -0.0323,  ..., -0.1099,  0.0353,  0.1009],\n",
      "          [-0.0437, -0.0770, -0.0451,  ...,  0.1007,  0.0997,  0.0380]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0589, -0.0544, -0.0321,  ..., -0.1101,  0.0351,  0.1009],\n",
      "          [-0.0433, -0.0768, -0.0454,  ...,  0.1009,  0.0996,  0.0380]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0591, -0.0545, -0.0325,  ..., -0.1101,  0.0354,  0.1014],\n",
      "          [-0.0438, -0.0768, -0.0450,  ...,  0.1000,  0.0993,  0.0381]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0590, -0.0544, -0.0321,  ..., -0.1107,  0.0349,  0.1010],\n",
      "          [-0.0435, -0.0766, -0.0453,  ...,  0.1009,  0.0992,  0.0384]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0591, -0.0547, -0.0322,  ..., -0.1104,  0.0351,  0.1009],\n",
      "          [-0.0437, -0.0770, -0.0454,  ...,  0.1003,  0.0996,  0.0384]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0590, -0.0545, -0.0321,  ..., -0.1100,  0.0354,  0.1011],\n",
      "          [-0.0437, -0.0769, -0.0456,  ...,  0.1003,  0.0994,  0.0379]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0860, -0.0391,  0.0155,  ..., -0.0991,  0.0460, -0.0548],\n",
      "          [ 0.0001, -0.0902, -0.0085,  ..., -0.1169, -0.0732, -0.0096]]],\n",
      "\n",
      "\n",
      "        [[[-0.0862, -0.0394,  0.0159,  ..., -0.0990,  0.0459, -0.0548],\n",
      "          [ 0.0002, -0.0900, -0.0082,  ..., -0.1169, -0.0733, -0.0095]]],\n",
      "\n",
      "\n",
      "        [[[-0.0858, -0.0391,  0.0155,  ..., -0.0990,  0.0457, -0.0548],\n",
      "          [ 0.0005, -0.0899, -0.0083,  ..., -0.1170, -0.0734, -0.0096]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0864, -0.0395,  0.0162,  ..., -0.0989,  0.0457, -0.0551],\n",
      "          [ 0.0002, -0.0898, -0.0078,  ..., -0.1171, -0.0732, -0.0096]]],\n",
      "\n",
      "\n",
      "        [[[-0.0863, -0.0391,  0.0159,  ..., -0.0987,  0.0457, -0.0550],\n",
      "          [ 0.0008, -0.0899, -0.0084,  ..., -0.1169, -0.0733, -0.0095]]],\n",
      "\n",
      "\n",
      "        [[[-0.0859, -0.0392,  0.0157,  ..., -0.0990,  0.0459, -0.0548],\n",
      "          [ 0.0007, -0.0903, -0.0086,  ..., -0.1169, -0.0732, -0.0097]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[-0.0144, -0.0624,  0.0689,  ..., -0.0668,  0.0021, -0.0415],\n",
      "          [ 0.0857, -0.0304,  0.0306,  ...,  0.0068,  0.0398, -0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.0142, -0.0626,  0.0690,  ..., -0.0671,  0.0023, -0.0413],\n",
      "          [ 0.0855, -0.0305,  0.0309,  ...,  0.0067,  0.0396, -0.0022]]],\n",
      "\n",
      "\n",
      "        [[[-0.0143, -0.0623,  0.0687,  ..., -0.0671,  0.0022, -0.0409],\n",
      "          [ 0.0858, -0.0303,  0.0306,  ...,  0.0063,  0.0395, -0.0017]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0139, -0.0626,  0.0691,  ..., -0.0670,  0.0030, -0.0411],\n",
      "          [ 0.0853, -0.0302,  0.0315,  ...,  0.0067,  0.0393, -0.0021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0140, -0.0624,  0.0686,  ..., -0.0669,  0.0024, -0.0409],\n",
      "          [ 0.0854, -0.0303,  0.0309,  ...,  0.0062,  0.0395, -0.0016]]],\n",
      "\n",
      "\n",
      "        [[[-0.0143, -0.0622,  0.0688,  ..., -0.0671,  0.0023, -0.0411],\n",
      "          [ 0.0857, -0.0306,  0.0308,  ...,  0.0064,  0.0395, -0.0016]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0188, -0.0432, -0.1317,  ...,  0.0322,  0.0690,  0.0868],\n",
      "          [ 0.1398,  0.0463, -0.0419,  ...,  0.1288,  0.1165,  0.0777]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0194, -0.0432, -0.1316,  ...,  0.0322,  0.0690,  0.0868],\n",
      "          [ 0.1400,  0.0463, -0.0417,  ...,  0.1289,  0.1164,  0.0780]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0184, -0.0432, -0.1317,  ...,  0.0325,  0.0693,  0.0867],\n",
      "          [ 0.1398,  0.0462, -0.0423,  ...,  0.1289,  0.1164,  0.0779]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0196, -0.0432, -0.1317,  ...,  0.0322,  0.0693,  0.0862],\n",
      "          [ 0.1398,  0.0462, -0.0417,  ...,  0.1288,  0.1162,  0.0782]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0188, -0.0433, -0.1318,  ...,  0.0327,  0.0691,  0.0866],\n",
      "          [ 0.1399,  0.0465, -0.0421,  ...,  0.1290,  0.1161,  0.0780]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0184, -0.0431, -0.1314,  ...,  0.0326,  0.0691,  0.0868],\n",
      "          [ 0.1398,  0.0465, -0.0423,  ...,  0.1293,  0.1167,  0.0780]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0157,  0.0928, -0.1314,  ..., -0.0086,  0.0331,  0.0902],\n",
      "          [-0.1252,  0.0620,  0.0610,  ...,  0.0340,  0.1031,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0155,  0.0930, -0.1319,  ..., -0.0088,  0.0328,  0.0902],\n",
      "          [-0.1253,  0.0619,  0.0610,  ...,  0.0344,  0.1031,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0160,  0.0928, -0.1314,  ..., -0.0089,  0.0329,  0.0903],\n",
      "          [-0.1255,  0.0619,  0.0603,  ...,  0.0345,  0.1031,  0.0038]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0153,  0.0931, -0.1319,  ..., -0.0087,  0.0325,  0.0905],\n",
      "          [-0.1252,  0.0617,  0.0608,  ...,  0.0347,  0.1031,  0.0037]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0157,  0.0926, -0.1317,  ..., -0.0091,  0.0330,  0.0905],\n",
      "          [-0.1255,  0.0618,  0.0606,  ...,  0.0344,  0.1029,  0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0161,  0.0925, -0.1314,  ..., -0.0090,  0.0329,  0.0904],\n",
      "          [-0.1255,  0.0621,  0.0606,  ...,  0.0341,  0.1030,  0.0037]]]])\n",
      "########################\n",
      "########################\n",
      "scaled_attention:  tensor([[[[ 0.0210, -0.0474,  0.1119,  ...,  0.0321,  0.0980, -0.0181],\n",
      "          [ 0.0241,  0.0186,  0.0205,  ..., -0.0210,  0.0526,  0.0158]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0210, -0.0473,  0.1118,  ...,  0.0319,  0.0979, -0.0181],\n",
      "          [ 0.0244,  0.0190,  0.0205,  ..., -0.0212,  0.0524,  0.0160]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0208, -0.0474,  0.1120,  ...,  0.0324,  0.0980, -0.0180],\n",
      "          [ 0.0245,  0.0190,  0.0203,  ..., -0.0214,  0.0523,  0.0161]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0208, -0.0473,  0.1121,  ...,  0.0319,  0.0977, -0.0177],\n",
      "          [ 0.0243,  0.0193,  0.0206,  ..., -0.0213,  0.0522,  0.0163]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0209, -0.0474,  0.1120,  ...,  0.0321,  0.0981, -0.0178],\n",
      "          [ 0.0249,  0.0192,  0.0202,  ..., -0.0216,  0.0526,  0.0162]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0209, -0.0475,  0.1118,  ...,  0.0325,  0.0978, -0.0179],\n",
      "          [ 0.0250,  0.0187,  0.0201,  ..., -0.0211,  0.0523,  0.0159]]]])\n",
      "########################\n",
      "################################\n",
      "es_pr_MA_list: \n",
      "[tensor([[[ 0.1122,  0.0497,  0.0943,  ..., -0.1089,  0.0703, -0.0187]],\n",
      "\n",
      "        [[ 0.1124,  0.0497,  0.0943,  ..., -0.1089,  0.0703, -0.0188]],\n",
      "\n",
      "        [[ 0.1125,  0.0496,  0.0943,  ..., -0.1092,  0.0705, -0.0188]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1126,  0.0495,  0.0945,  ..., -0.1091,  0.0705, -0.0187]],\n",
      "\n",
      "        [[ 0.1122,  0.0497,  0.0944,  ..., -0.1089,  0.0704, -0.0187]],\n",
      "\n",
      "        [[ 0.1123,  0.0497,  0.0943,  ..., -0.1090,  0.0703, -0.0187]]]), tensor([[[ 0.0171,  0.0666, -0.0631,  ..., -0.0950,  0.0374, -0.0086]],\n",
      "\n",
      "        [[ 0.0171,  0.0665, -0.0630,  ..., -0.0949,  0.0375, -0.0083]],\n",
      "\n",
      "        [[ 0.0169,  0.0665, -0.0630,  ..., -0.0949,  0.0375, -0.0084]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0171,  0.0664, -0.0631,  ..., -0.0950,  0.0376, -0.0084]],\n",
      "\n",
      "        [[ 0.0170,  0.0664, -0.0630,  ..., -0.0949,  0.0376, -0.0082]],\n",
      "\n",
      "        [[ 0.0170,  0.0665, -0.0628,  ..., -0.0949,  0.0376, -0.0083]]]), tensor([[[ 0.0784, -0.0223, -0.0115,  ...,  0.0975, -0.0559, -0.0167]],\n",
      "\n",
      "        [[ 0.0785, -0.0222, -0.0116,  ...,  0.0979, -0.0559, -0.0165]],\n",
      "\n",
      "        [[ 0.0786, -0.0222, -0.0116,  ...,  0.0978, -0.0560, -0.0166]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0786, -0.0223, -0.0116,  ...,  0.0979, -0.0561, -0.0168]],\n",
      "\n",
      "        [[ 0.0787, -0.0222, -0.0116,  ...,  0.0980, -0.0561, -0.0167]],\n",
      "\n",
      "        [[ 0.0784, -0.0223, -0.0115,  ...,  0.0978, -0.0563, -0.0166]]]), tensor([[[-0.0324,  0.0208, -0.0669,  ...,  0.0558,  0.0325,  0.0416]],\n",
      "\n",
      "        [[-0.0327,  0.0208, -0.0670,  ...,  0.0557,  0.0325,  0.0417]],\n",
      "\n",
      "        [[-0.0324,  0.0211, -0.0668,  ...,  0.0559,  0.0322,  0.0417]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0328,  0.0210, -0.0669,  ...,  0.0559,  0.0323,  0.0418]],\n",
      "\n",
      "        [[-0.0327,  0.0211, -0.0670,  ...,  0.0559,  0.0324,  0.0417]],\n",
      "\n",
      "        [[-0.0328,  0.0211, -0.0670,  ...,  0.0560,  0.0325,  0.0416]]]), tensor([[[-0.0826,  0.0299, -0.1436,  ...,  0.0016, -0.0888,  0.0835]],\n",
      "\n",
      "        [[-0.0825,  0.0300, -0.1437,  ...,  0.0017, -0.0888,  0.0835]],\n",
      "\n",
      "        [[-0.0825,  0.0302, -0.1438,  ...,  0.0017, -0.0887,  0.0836]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0827,  0.0299, -0.1438,  ...,  0.0016, -0.0887,  0.0833]],\n",
      "\n",
      "        [[-0.0824,  0.0302, -0.1438,  ...,  0.0016, -0.0886,  0.0834]],\n",
      "\n",
      "        [[-0.0824,  0.0304, -0.1437,  ...,  0.0016, -0.0886,  0.0836]]]), tensor([[[ 0.0823, -0.0029,  0.0020,  ...,  0.0152, -0.1028,  0.0977]],\n",
      "\n",
      "        [[ 0.0824, -0.0027,  0.0020,  ...,  0.0152, -0.1029,  0.0977]],\n",
      "\n",
      "        [[ 0.0822, -0.0028,  0.0021,  ...,  0.0150, -0.1029,  0.0976]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0821, -0.0026,  0.0019,  ...,  0.0152, -0.1029,  0.0974]],\n",
      "\n",
      "        [[ 0.0823, -0.0029,  0.0019,  ...,  0.0150, -0.1031,  0.0977]],\n",
      "\n",
      "        [[ 0.0824, -0.0028,  0.0021,  ...,  0.0148, -0.1031,  0.0977]]]), tensor([[[ 0.0579,  0.0395,  0.0861,  ..., -0.0473, -0.0513,  0.0991]],\n",
      "\n",
      "        [[ 0.0579,  0.0393,  0.0862,  ..., -0.0475, -0.0512,  0.0992]],\n",
      "\n",
      "        [[ 0.0580,  0.0393,  0.0861,  ..., -0.0472, -0.0514,  0.0992]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0579,  0.0390,  0.0862,  ..., -0.0476, -0.0511,  0.0991]],\n",
      "\n",
      "        [[ 0.0579,  0.0390,  0.0860,  ..., -0.0472, -0.0513,  0.0992]],\n",
      "\n",
      "        [[ 0.0579,  0.0392,  0.0862,  ..., -0.0473, -0.0513,  0.0990]]]), tensor([[[ 0.0305, -0.0196,  0.0005,  ..., -0.0463,  0.0321,  0.0979]],\n",
      "\n",
      "        [[ 0.0303, -0.0194,  0.0006,  ..., -0.0463,  0.0321,  0.0977]],\n",
      "\n",
      "        [[ 0.0304, -0.0196,  0.0006,  ..., -0.0461,  0.0317,  0.0978]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0302, -0.0194,  0.0008,  ..., -0.0462,  0.0320,  0.0978]],\n",
      "\n",
      "        [[ 0.0301, -0.0196,  0.0007,  ..., -0.0461,  0.0321,  0.0980]],\n",
      "\n",
      "        [[ 0.0301, -0.0196,  0.0006,  ..., -0.0463,  0.0320,  0.0978]]]), tensor([[[ 0.0137, -0.0156,  0.0006,  ..., -0.0567,  0.0254,  0.0302]],\n",
      "\n",
      "        [[ 0.0138, -0.0156,  0.0005,  ..., -0.0568,  0.0255,  0.0303]],\n",
      "\n",
      "        [[ 0.0138, -0.0157,  0.0004,  ..., -0.0566,  0.0255,  0.0302]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0139, -0.0158,  0.0005,  ..., -0.0567,  0.0257,  0.0300]],\n",
      "\n",
      "        [[ 0.0139, -0.0157,  0.0004,  ..., -0.0564,  0.0255,  0.0302]],\n",
      "\n",
      "        [[ 0.0137, -0.0156,  0.0004,  ..., -0.0566,  0.0253,  0.0304]]])]\n",
      "################################\n",
      "################################\n",
      "es_pr_MA_lstm_list: \n",
      "[tensor([[[-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0096, -0.0361,  ..., -0.0181,  0.0449, -0.0023]],\n",
      "\n",
      "        [[-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023]]]), tensor([[[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0024,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0024,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0353]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0024,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0024,  0.0353]],\n",
      "\n",
      "        [[-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0025,  0.0353]]]), tensor([[[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0026, 0.0190,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0245, 0.0096]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0195, 0.0026, 0.0190,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0246, 0.0096]],\n",
      "\n",
      "        [[0.0194, 0.0026, 0.0189,  ..., 0.0273, 0.0245, 0.0096]]]), tensor([[[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046]],\n",
      "\n",
      "        [[-0.0065, -0.0172,  0.0089,  ..., -0.0110, -0.0120,  0.0045]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046]],\n",
      "\n",
      "        [[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0045]],\n",
      "\n",
      "        [[-0.0066, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0045]]]), tensor([[[ 0.0119,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        [[ 0.0119,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0397]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0119,  0.0369,  0.0305,  ...,  0.0255, -0.0464, -0.0397]],\n",
      "\n",
      "        [[ 0.0119,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0397]],\n",
      "\n",
      "        [[ 0.0118,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0398]]]), tensor([[[-1.8803e-02, -7.1988e-03,  9.5597e-05,  ..., -2.9384e-03,\n",
      "           1.1988e-03, -2.1004e-02]],\n",
      "\n",
      "        [[-1.8776e-02, -7.2012e-03,  1.0130e-04,  ..., -2.9351e-03,\n",
      "           1.1810e-03, -2.1004e-02]],\n",
      "\n",
      "        [[-1.8779e-02, -7.2134e-03,  1.2273e-04,  ..., -2.8788e-03,\n",
      "           1.2195e-03, -2.0987e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.8749e-02, -7.2016e-03,  1.2205e-04,  ..., -2.9244e-03,\n",
      "           1.1667e-03, -2.1020e-02]],\n",
      "\n",
      "        [[-1.8743e-02, -7.2096e-03,  1.1171e-04,  ..., -2.9098e-03,\n",
      "           1.1995e-03, -2.1016e-02]],\n",
      "\n",
      "        [[-1.8783e-02, -7.1970e-03,  1.0709e-04,  ..., -2.9113e-03,\n",
      "           1.1947e-03, -2.0987e-02]]]), tensor([[[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0416,  0.0019]],\n",
      "\n",
      "        [[-0.0024, -0.0317,  0.0110,  ..., -0.0367,  0.0416,  0.0019]],\n",
      "\n",
      "        [[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0024, -0.0316,  0.0109,  ..., -0.0367,  0.0416,  0.0020]],\n",
      "\n",
      "        [[-0.0024, -0.0317,  0.0109,  ..., -0.0368,  0.0416,  0.0019]],\n",
      "\n",
      "        [[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0416,  0.0019]]]), tensor([[[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0150,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0006, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174]],\n",
      "\n",
      "        [[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0150,  0.0174]]]), tensor([[[-4.1293e-06,  2.4297e-02, -2.4103e-02,  ..., -4.1140e-02,\n",
      "          -9.1146e-03, -1.9521e-03]],\n",
      "\n",
      "        [[ 1.8038e-05,  2.4286e-02, -2.4126e-02,  ..., -4.1116e-02,\n",
      "          -9.1219e-03, -1.9641e-03]],\n",
      "\n",
      "        [[-9.3332e-06,  2.4307e-02, -2.4147e-02,  ..., -4.1105e-02,\n",
      "          -9.1396e-03, -1.9538e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8.9600e-06,  2.4297e-02, -2.4131e-02,  ..., -4.1121e-02,\n",
      "          -9.1108e-03, -1.9684e-03]],\n",
      "\n",
      "        [[ 1.9461e-05,  2.4292e-02, -2.4173e-02,  ..., -4.1099e-02,\n",
      "          -9.1440e-03, -1.9660e-03]],\n",
      "\n",
      "        [[ 3.4713e-06,  2.4295e-02, -2.4148e-02,  ..., -4.1092e-02,\n",
      "          -9.1233e-03, -1.9542e-03]]])]\n",
      "################################\n",
      "################################\n",
      "es_pr_avg_lstm_list: \n",
      "[tensor([[-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023],\n",
      "        [-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023],\n",
      "        [-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023],\n",
      "        ...,\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0180,  0.0450, -0.0023],\n",
      "        [-0.0186, -0.0096, -0.0361,  ..., -0.0181,  0.0449, -0.0023],\n",
      "        [-0.0186, -0.0097, -0.0361,  ..., -0.0180,  0.0449, -0.0023]]), tensor([[-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0024,  0.0353],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0024,  0.0353],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0025,  0.0353],\n",
      "        ...,\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0411,  0.0024,  0.0353],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0024,  0.0353],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0412,  0.0025,  0.0353]]), tensor([[0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0026, 0.0190,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0245, 0.0096],\n",
      "        ...,\n",
      "        [0.0195, 0.0026, 0.0190,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0025, 0.0190,  ..., 0.0273, 0.0246, 0.0096],\n",
      "        [0.0194, 0.0026, 0.0189,  ..., 0.0273, 0.0245, 0.0096]]), tensor([[-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046],\n",
      "        [-0.0065, -0.0172,  0.0089,  ..., -0.0110, -0.0120,  0.0045],\n",
      "        ...,\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0046],\n",
      "        [-0.0065, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0045],\n",
      "        [-0.0066, -0.0171,  0.0089,  ..., -0.0110, -0.0119,  0.0045]]), tensor([[ 0.0119,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        [ 0.0119,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0397],\n",
      "        [ 0.0118,  0.0370,  0.0305,  ...,  0.0254, -0.0464, -0.0398],\n",
      "        ...,\n",
      "        [ 0.0119,  0.0369,  0.0305,  ...,  0.0255, -0.0464, -0.0397],\n",
      "        [ 0.0119,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0397],\n",
      "        [ 0.0118,  0.0370,  0.0305,  ...,  0.0255, -0.0464, -0.0398]]), tensor([[-1.8803e-02, -7.1988e-03,  9.5597e-05,  ..., -2.9384e-03,\n",
      "          1.1988e-03, -2.1004e-02],\n",
      "        [-1.8776e-02, -7.2012e-03,  1.0130e-04,  ..., -2.9351e-03,\n",
      "          1.1810e-03, -2.1004e-02],\n",
      "        [-1.8779e-02, -7.2134e-03,  1.2273e-04,  ..., -2.8788e-03,\n",
      "          1.2195e-03, -2.0987e-02],\n",
      "        ...,\n",
      "        [-1.8749e-02, -7.2016e-03,  1.2205e-04,  ..., -2.9244e-03,\n",
      "          1.1667e-03, -2.1020e-02],\n",
      "        [-1.8743e-02, -7.2096e-03,  1.1171e-04,  ..., -2.9098e-03,\n",
      "          1.1995e-03, -2.1016e-02],\n",
      "        [-1.8783e-02, -7.1970e-03,  1.0709e-04,  ..., -2.9113e-03,\n",
      "          1.1947e-03, -2.0987e-02]]), tensor([[-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0416,  0.0019],\n",
      "        [-0.0024, -0.0317,  0.0110,  ..., -0.0367,  0.0416,  0.0019],\n",
      "        [-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0417,  0.0019],\n",
      "        ...,\n",
      "        [-0.0024, -0.0316,  0.0109,  ..., -0.0367,  0.0416,  0.0020],\n",
      "        [-0.0024, -0.0317,  0.0109,  ..., -0.0368,  0.0416,  0.0019],\n",
      "        [-0.0024, -0.0317,  0.0109,  ..., -0.0367,  0.0416,  0.0019]]), tensor([[-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0151,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0150,  0.0174],\n",
      "        [-0.0007, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174],\n",
      "        ...,\n",
      "        [-0.0006, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174],\n",
      "        [-0.0007, -0.0078, -0.0319,  ..., -0.0084, -0.0150,  0.0174],\n",
      "        [-0.0007, -0.0077, -0.0319,  ..., -0.0084, -0.0150,  0.0174]]), tensor([[-4.1293e-06,  2.4297e-02, -2.4103e-02,  ..., -4.1140e-02,\n",
      "         -9.1146e-03, -1.9521e-03],\n",
      "        [ 1.8038e-05,  2.4286e-02, -2.4126e-02,  ..., -4.1116e-02,\n",
      "         -9.1219e-03, -1.9641e-03],\n",
      "        [-9.3332e-06,  2.4307e-02, -2.4147e-02,  ..., -4.1105e-02,\n",
      "         -9.1396e-03, -1.9538e-03],\n",
      "        ...,\n",
      "        [ 8.9600e-06,  2.4297e-02, -2.4131e-02,  ..., -4.1121e-02,\n",
      "         -9.1108e-03, -1.9684e-03],\n",
      "        [ 1.9461e-05,  2.4292e-02, -2.4173e-02,  ..., -4.1099e-02,\n",
      "         -9.1440e-03, -1.9660e-03],\n",
      "        [ 3.4713e-06,  2.4295e-02, -2.4148e-02,  ..., -4.1092e-02,\n",
      "         -9.1233e-03, -1.9542e-03]])]\n",
      "################################\n",
      "linguistic_input:  torch.Size([1783, 51])\n",
      "readability_input:  torch.Size([1783, 35])\n",
      "es_pr_feat_concat:  9 ,  torch.Size([1783, 186])\n",
      "################################\n",
      "es_pr_feat_concat: \n",
      "[tensor([[-0.0186, -0.0097, -0.0361,  ...,  0.0000,  0.2000,  0.2500],\n",
      "        [-0.0186, -0.0097, -0.0361,  ...,  0.2222,  0.0000,  0.0833],\n",
      "        [-0.0186, -0.0097, -0.0361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.0000,  0.1000,  0.0833],\n",
      "        [-0.0186, -0.0096, -0.0361,  ...,  0.2222,  0.2000,  0.2500],\n",
      "        [-0.0186, -0.0097, -0.0361,  ...,  0.2222,  0.0000,  0.2500]],\n",
      "       dtype=torch.float64), tensor([[-0.0530, -0.0280,  0.0106,  ...,  0.0000,  0.2000,  0.2500],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.2222,  0.0000,  0.0833],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.0000,  0.1000,  0.0833],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.2222,  0.2000,  0.2500],\n",
      "        [-0.0530, -0.0280,  0.0106,  ...,  0.2222,  0.0000,  0.2500]],\n",
      "       dtype=torch.float64), tensor([[0.0194, 0.0025, 0.0190,  ..., 0.0000, 0.2000, 0.2500],\n",
      "        [0.0194, 0.0026, 0.0190,  ..., 0.2222, 0.0000, 0.0833],\n",
      "        [0.0194, 0.0025, 0.0190,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0195, 0.0026, 0.0190,  ..., 0.0000, 0.1000, 0.0833],\n",
      "        [0.0194, 0.0025, 0.0190,  ..., 0.2222, 0.2000, 0.2500],\n",
      "        [0.0194, 0.0026, 0.0189,  ..., 0.2222, 0.0000, 0.2500]],\n",
      "       dtype=torch.float64), tensor([[-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.2000,  0.2500],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.2222,  0.0000,  0.0833],\n",
      "        [-0.0065, -0.0172,  0.0089,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.0000,  0.1000,  0.0833],\n",
      "        [-0.0065, -0.0171,  0.0089,  ...,  0.2222,  0.2000,  0.2500],\n",
      "        [-0.0066, -0.0171,  0.0089,  ...,  0.2222,  0.0000,  0.2500]],\n",
      "       dtype=torch.float64), tensor([[0.0119, 0.0370, 0.0305,  ..., 0.0000, 0.2000, 0.2500],\n",
      "        [0.0119, 0.0370, 0.0305,  ..., 0.2222, 0.0000, 0.0833],\n",
      "        [0.0118, 0.0370, 0.0305,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0119, 0.0369, 0.0305,  ..., 0.0000, 0.1000, 0.0833],\n",
      "        [0.0119, 0.0370, 0.0305,  ..., 0.2222, 0.2000, 0.2500],\n",
      "        [0.0118, 0.0370, 0.0305,  ..., 0.2222, 0.0000, 0.2500]],\n",
      "       dtype=torch.float64), tensor([[-1.8803e-02, -7.1988e-03,  9.5597e-05,  ...,  0.0000e+00,\n",
      "          2.0000e-01,  2.5000e-01],\n",
      "        [-1.8776e-02, -7.2012e-03,  1.0130e-04,  ...,  2.2222e-01,\n",
      "          0.0000e+00,  8.3333e-02],\n",
      "        [-1.8779e-02, -7.2134e-03,  1.2273e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [-1.8749e-02, -7.2016e-03,  1.2205e-04,  ...,  0.0000e+00,\n",
      "          1.0000e-01,  8.3333e-02],\n",
      "        [-1.8743e-02, -7.2096e-03,  1.1171e-04,  ...,  2.2222e-01,\n",
      "          2.0000e-01,  2.5000e-01],\n",
      "        [-1.8783e-02, -7.1970e-03,  1.0709e-04,  ...,  2.2222e-01,\n",
      "          0.0000e+00,  2.5000e-01]], dtype=torch.float64), tensor([[-0.0024, -0.0317,  0.0109,  ...,  0.0000,  0.2000,  0.2500],\n",
      "        [-0.0024, -0.0317,  0.0110,  ...,  0.2222,  0.0000,  0.0833],\n",
      "        [-0.0024, -0.0317,  0.0109,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0024, -0.0316,  0.0109,  ...,  0.0000,  0.1000,  0.0833],\n",
      "        [-0.0024, -0.0317,  0.0109,  ...,  0.2222,  0.2000,  0.2500],\n",
      "        [-0.0024, -0.0317,  0.0109,  ...,  0.2222,  0.0000,  0.2500]],\n",
      "       dtype=torch.float64), tensor([[-0.0007, -0.0077, -0.0319,  ...,  0.0000,  0.2000,  0.2500],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.2222,  0.0000,  0.0833],\n",
      "        [-0.0007, -0.0078, -0.0319,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0006, -0.0078, -0.0319,  ...,  0.0000,  0.1000,  0.0833],\n",
      "        [-0.0007, -0.0078, -0.0319,  ...,  0.2222,  0.2000,  0.2500],\n",
      "        [-0.0007, -0.0077, -0.0319,  ...,  0.2222,  0.0000,  0.2500]],\n",
      "       dtype=torch.float64), tensor([[-4.1293e-06,  2.4297e-02, -2.4103e-02,  ...,  0.0000e+00,\n",
      "          2.0000e-01,  2.5000e-01],\n",
      "        [ 1.8038e-05,  2.4286e-02, -2.4126e-02,  ...,  2.2222e-01,\n",
      "          0.0000e+00,  8.3333e-02],\n",
      "        [-9.3332e-06,  2.4307e-02, -2.4147e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 8.9600e-06,  2.4297e-02, -2.4131e-02,  ...,  0.0000e+00,\n",
      "          1.0000e-01,  8.3333e-02],\n",
      "        [ 1.9461e-05,  2.4292e-02, -2.4173e-02,  ...,  2.2222e-01,\n",
      "          2.0000e-01,  2.5000e-01],\n",
      "        [ 3.4713e-06,  2.4295e-02, -2.4148e-02,  ...,  2.2222e-01,\n",
      "          0.0000e+00,  2.5000e-01]], dtype=torch.float64)]\n",
      "################################\n",
      "tensor([-1.8613e-02, -9.6514e-03, -3.6145e-02, -2.6474e-02,  1.3753e-02,\n",
      "         8.4233e-03, -2.5251e-02, -7.7713e-03, -1.1089e-03,  1.1337e-02,\n",
      "         1.3776e-02, -4.1181e-02, -2.4699e-02, -7.3833e-03,  2.3245e-03,\n",
      "        -3.8780e-02, -2.2232e-02, -1.3658e-02, -7.2222e-03,  3.8211e-02,\n",
      "        -3.5544e-02,  2.8403e-02, -6.5192e-03, -8.5522e-03, -3.0311e-02,\n",
      "         7.5115e-03,  2.0534e-03,  1.4996e-02, -5.5925e-02, -1.1371e-02,\n",
      "        -2.6279e-02, -3.2104e-02, -4.2642e-03, -1.7818e-03, -2.7530e-02,\n",
      "        -1.0503e-02,  1.6077e-02,  9.6159e-03, -1.2552e-03,  2.1478e-02,\n",
      "        -7.2183e-03, -6.9122e-03, -1.5449e-03,  8.7390e-03,  2.5788e-02,\n",
      "         4.1868e-02, -5.5063e-03,  1.9138e-02,  4.1089e-02,  1.9032e-02,\n",
      "        -3.4695e-02, -2.3200e-02, -1.5816e-02, -1.7381e-02, -1.1470e-02,\n",
      "         2.3647e-02,  1.1241e-02, -1.3488e-02, -4.7001e-02, -1.2038e-02,\n",
      "         3.9166e-02, -2.7770e-02, -3.5505e-03, -4.5114e-02,  3.4612e-02,\n",
      "        -2.0160e-02, -1.7748e-02,  2.9714e-02,  5.6455e-03,  1.1340e-02,\n",
      "        -2.5515e-02, -4.2115e-04, -1.5890e-02,  1.8393e-02,  1.2318e-02,\n",
      "        -9.3874e-03, -5.9032e-03, -1.4018e-02, -5.9784e-03,  1.9468e-02,\n",
      "         8.6237e-04,  2.5889e-03,  4.1126e-02,  2.3706e-02, -4.5967e-02,\n",
      "         1.6893e-02, -2.8246e-02, -7.2934e-03, -1.8841e-02, -3.2093e-03,\n",
      "         2.0581e-03,  7.0672e-03, -4.4162e-02, -1.2926e-02,  6.2668e-03,\n",
      "        -1.6179e-02,  3.5676e-02, -1.8011e-02,  4.4949e-02, -2.3137e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01], dtype=torch.float64)\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-1.8613e-02, -9.6514e-03, -3.6145e-02, -2.6474e-02,  1.3753e-02,\n",
      "         8.4233e-03, -2.5251e-02, -7.7713e-03, -1.1089e-03,  1.1337e-02,\n",
      "         1.3776e-02, -4.1181e-02, -2.4699e-02, -7.3833e-03,  2.3245e-03,\n",
      "        -3.8780e-02, -2.2232e-02, -1.3658e-02, -7.2222e-03,  3.8211e-02,\n",
      "        -3.5544e-02,  2.8403e-02, -6.5192e-03, -8.5522e-03, -3.0311e-02,\n",
      "         7.5115e-03,  2.0534e-03,  1.4996e-02, -5.5925e-02, -1.1371e-02,\n",
      "        -2.6279e-02, -3.2104e-02, -4.2642e-03, -1.7818e-03, -2.7530e-02,\n",
      "        -1.0503e-02,  1.6077e-02,  9.6159e-03, -1.2552e-03,  2.1478e-02,\n",
      "        -7.2183e-03, -6.9122e-03, -1.5449e-03,  8.7390e-03,  2.5788e-02,\n",
      "         4.1868e-02, -5.5063e-03,  1.9138e-02,  4.1089e-02,  1.9032e-02,\n",
      "        -3.4695e-02, -2.3200e-02, -1.5816e-02, -1.7381e-02, -1.1470e-02,\n",
      "         2.3647e-02,  1.1241e-02, -1.3488e-02, -4.7001e-02, -1.2038e-02,\n",
      "         3.9166e-02, -2.7770e-02, -3.5505e-03, -4.5114e-02,  3.4612e-02,\n",
      "        -2.0160e-02, -1.7748e-02,  2.9714e-02,  5.6455e-03,  1.1340e-02,\n",
      "        -2.5515e-02, -4.2115e-04, -1.5890e-02,  1.8393e-02,  1.2318e-02,\n",
      "        -9.3874e-03, -5.9032e-03, -1.4018e-02, -5.9784e-03,  1.9468e-02,\n",
      "         8.6237e-04,  2.5889e-03,  4.1126e-02,  2.3706e-02, -4.5967e-02,\n",
      "         1.6893e-02, -2.8246e-02, -7.2934e-03, -1.8841e-02, -3.2093e-03,\n",
      "         2.0581e-03,  7.0672e-03, -4.4162e-02, -1.2926e-02,  6.2668e-03,\n",
      "        -1.6179e-02,  3.5676e-02, -1.8011e-02,  4.4949e-02, -2.3137e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  5.8474e-02,  9.5405e-03,  1.2305e-01,  7.8303e-02,\n",
      "         5.5241e-02, -7.6714e-02,  1.4748e-01,  4.9019e-02, -6.2779e-02,\n",
      "         2.0048e-01, -9.3509e-02, -1.3709e-01, -1.0033e-01, -6.5431e-02,\n",
      "         1.0196e-01, -7.3298e-02, -8.9935e-02, -7.2320e-02, -1.6241e-01,\n",
      "        -4.6188e-02,  6.9494e-02, -5.5360e-02,  4.4770e-02,  8.3820e-02,\n",
      "        -6.9774e-02, -3.1860e-02,  4.9375e-02,  1.0511e-02,  3.7234e-02,\n",
      "        -6.0323e-02, -3.4979e-02,  5.3176e-02,  9.3916e-02,  9.7922e-02,\n",
      "        -5.9940e-03,  2.5659e-02,  1.2571e-01, -5.7771e-02,  1.9421e-02,\n",
      "         6.2286e-02, -3.2363e-02, -3.3175e-02,  1.9347e-02, -1.0022e-01,\n",
      "        -4.1877e-02,  9.1659e-02,  7.1375e-02, -9.0501e-02, -2.3723e-02,\n",
      "         4.9408e-02, -9.5193e-02,  1.5717e-02,  1.0819e-01,  1.1397e-01,\n",
      "         1.3744e-01, -7.9717e-02,  3.0771e-02, -1.2227e-01, -1.9450e-02,\n",
      "         4.3892e-02, -6.5618e-02,  3.9765e-02, -5.3863e-03, -3.4367e-02,\n",
      "         3.4076e-02,  1.1365e-01,  1.1408e-01, -7.9869e-02, -1.1883e-01,\n",
      "         9.3885e-02, -6.9809e-02,  1.3030e-02,  2.1021e-02,  1.4794e-02,\n",
      "        -5.1264e-02,  1.1674e-01, -6.3074e-02, -1.3770e-01,  1.0939e-01,\n",
      "        -3.2992e-02, -7.4906e-03,  2.2007e-01,  2.6854e-02, -4.0263e-02,\n",
      "         1.8109e-02,  1.0657e-01,  1.0805e-01, -5.1015e-02, -6.7932e-02,\n",
      "        -4.6023e-02, -1.0468e-01,  1.3558e-01, -8.7994e-02, -5.0373e-02,\n",
      "         8.9056e-02,  9.2214e-03, -1.7131e-01,  2.8702e-03, -1.6955e-01,\n",
      "         1.2027e-01, -4.0107e-03,  3.1833e-02,  3.2931e-02, -8.6541e-03,\n",
      "        -1.2545e-01, -7.0754e-04, -3.7591e-02,  1.3981e-01,  1.8591e-02,\n",
      "         1.0651e-01,  1.1220e-01, -1.1104e-01, -4.7993e-02, -1.6602e-01,\n",
      "        -4.0133e-02,  7.2148e-02,  4.4652e-02,  2.9254e-02, -1.0064e-02,\n",
      "         1.6527e-02, -1.4263e-02,  9.8881e-02,  4.7514e-03, -7.7161e-02,\n",
      "        -2.1979e-03,  2.0417e-02, -1.2415e-01,  1.2339e-02,  5.9537e-02,\n",
      "         1.0496e-01, -3.5116e-02,  1.0686e-02,  1.6488e-02, -1.7925e-03,\n",
      "         1.1860e-01, -2.7798e-03, -7.7223e-02,  1.0620e-01,  6.7863e-02,\n",
      "         1.6128e-02, -6.1047e-02,  5.4279e-02, -9.8070e-02, -1.4566e-02,\n",
      "        -7.9786e-02, -3.2546e-03, -2.7306e-02,  9.1540e-02,  4.9334e-02,\n",
      "         3.3980e-02, -3.0016e-02, -1.4273e-02, -8.5166e-02, -3.0729e-02,\n",
      "        -1.7063e-02, -8.5751e-03,  2.5850e-02, -9.3205e-02, -1.1819e-02,\n",
      "         1.2743e-01,  6.8202e-02, -4.2253e-02, -5.7441e-02,  3.1431e-02,\n",
      "         3.2877e-02, -7.8535e-02,  2.0366e-02,  3.9205e-02,  2.4748e-02,\n",
      "         1.2316e-01,  4.8102e-02,  2.1053e-02, -1.8818e-01,  8.1589e-02,\n",
      "         7.6801e-02,  1.0956e-01,  1.0834e-01, -8.0535e-03, -5.7312e-03,\n",
      "        -6.2170e-03, -1.6320e-01,  1.5090e-02, -8.4024e-03, -1.8154e-02,\n",
      "        -5.3234e-02,  3.3065e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5283],\n",
      "        [0.5280],\n",
      "        [0.5230],\n",
      "        ...,\n",
      "        [0.5249],\n",
      "        [0.5260],\n",
      "        [0.5180]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-5.3040e-02, -2.8002e-02,  1.0613e-02, -3.6622e-02, -2.4825e-02,\n",
      "        -1.5021e-02,  2.4525e-02,  3.2305e-02, -3.3356e-03, -2.9856e-02,\n",
      "         4.5518e-03,  1.6353e-03, -5.3029e-03, -9.3707e-03, -3.5519e-02,\n",
      "        -1.1195e-02, -1.0980e-02,  3.2455e-02,  1.1090e-03,  4.5053e-02,\n",
      "        -1.6554e-02, -2.4400e-02, -2.2402e-02,  4.3223e-03, -3.6297e-02,\n",
      "        -3.9560e-02, -1.5298e-02, -9.1761e-03,  3.4850e-02,  1.2571e-02,\n",
      "        -6.5841e-03,  1.6043e-02,  2.9128e-02, -1.8905e-02,  3.0733e-02,\n",
      "        -1.7807e-02, -4.1695e-03,  2.0231e-02,  2.8569e-03, -1.7772e-02,\n",
      "         3.5049e-02, -4.8282e-03,  1.2365e-02,  9.0754e-03, -5.3079e-03,\n",
      "        -2.5480e-02,  3.2941e-02, -1.6533e-04, -7.8753e-03,  5.3471e-03,\n",
      "         8.1069e-03, -2.6503e-03, -8.1786e-03,  5.8221e-03, -1.7920e-02,\n",
      "         4.3642e-02,  1.7693e-02, -1.3826e-03,  2.4085e-02, -1.7344e-02,\n",
      "        -1.9526e-02, -1.8794e-03,  1.5231e-02, -7.5582e-03,  4.9271e-03,\n",
      "        -2.4290e-02, -1.6189e-02,  1.1949e-02,  9.2170e-03,  3.2583e-02,\n",
      "        -2.4211e-03, -1.5898e-02,  4.7502e-02, -5.5542e-03, -5.8383e-03,\n",
      "        -2.2531e-02, -2.7122e-02,  1.8467e-02,  4.4127e-02, -4.1982e-03,\n",
      "         2.7662e-02, -7.3388e-03,  5.4290e-03,  6.5573e-03, -1.2758e-02,\n",
      "        -1.3309e-02,  5.1599e-03,  2.0837e-02, -1.8597e-03,  1.9759e-02,\n",
      "         2.2265e-02,  3.7255e-02,  4.3094e-03, -2.2509e-02, -1.0073e-02,\n",
      "         2.2606e-02,  4.2834e-02,  4.1146e-02,  2.4491e-03,  3.5309e-02,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01, -1.2599e-01,  5.0705e-02,  1.5666e-02, -4.9096e-02,\n",
      "        -6.0714e-02, -5.8649e-02,  1.5166e-01, -1.7845e-02,  8.7057e-02,\n",
      "         5.8199e-02, -1.6100e-02,  5.1786e-02,  1.0547e-01,  7.4986e-02,\n",
      "         9.6428e-03, -4.7332e-02, -1.9169e-02,  3.6733e-02,  1.7975e-02,\n",
      "         6.4113e-02,  1.4336e-01,  9.0799e-02,  3.5082e-02, -2.3673e-02,\n",
      "         9.3334e-02,  1.3351e-01,  1.2729e-02, -8.2192e-02,  9.1805e-02,\n",
      "         8.2186e-03,  3.6258e-02,  7.4148e-02, -7.7649e-02, -9.0359e-02,\n",
      "        -8.1545e-02,  1.0311e-01, -4.5977e-02,  7.5359e-02, -3.2763e-02,\n",
      "         4.4952e-02,  1.0570e-03,  4.4273e-02, -9.1760e-02, -4.2475e-02,\n",
      "        -1.3682e-01, -2.8582e-02, -1.5927e-01,  6.6066e-02,  1.4774e-01,\n",
      "         2.0216e-02,  1.3255e-02,  3.0620e-02,  2.8466e-02, -5.4246e-02,\n",
      "        -1.1260e-01, -1.2779e-01, -1.0127e-01,  5.1655e-02, -4.2768e-02,\n",
      "         1.9695e-02, -1.4287e-01,  1.4235e-02, -6.2761e-02, -3.0158e-02,\n",
      "        -7.7934e-02, -1.4663e-01,  1.1395e-01, -4.1963e-02,  2.3025e-02,\n",
      "        -6.8638e-02,  1.1671e-01,  8.9045e-02,  1.1915e-03, -9.2746e-03,\n",
      "        -3.3125e-02, -8.5676e-03,  3.8411e-03,  2.8299e-02,  1.1344e-01,\n",
      "        -1.0606e-02,  2.1562e-02, -6.7565e-02, -9.2602e-02, -1.0843e-01,\n",
      "         1.9477e-02, -9.1009e-02,  4.4457e-02, -8.1444e-02, -3.9122e-03,\n",
      "         1.3493e-01, -7.1087e-02, -1.2154e-01, -5.3578e-02, -1.1334e-01,\n",
      "         5.4867e-02,  6.8983e-03, -8.6860e-02,  5.9883e-02,  6.6222e-02,\n",
      "         4.1784e-03, -5.6043e-02, -5.0308e-02,  3.6467e-02, -4.0802e-02,\n",
      "        -6.1516e-03,  3.4261e-02,  1.0503e-02, -1.0207e-02, -4.7165e-02,\n",
      "        -2.4550e-02, -1.9848e-02, -1.0256e-01,  4.6232e-03,  9.5784e-02,\n",
      "        -2.4607e-02, -1.9257e-02,  1.3248e-02,  1.6440e-01, -5.5993e-02,\n",
      "        -7.7833e-02, -4.7814e-02,  2.0071e-02,  7.5741e-02, -9.0454e-02,\n",
      "        -8.3490e-02,  4.7792e-02, -2.0957e-02,  9.4293e-02, -5.5332e-02,\n",
      "         1.8145e-04, -2.2994e-03,  2.2310e-02, -4.0828e-02, -4.0510e-02,\n",
      "        -1.0949e-01, -2.7833e-02, -9.5012e-02, -6.3876e-02,  1.0398e-01,\n",
      "        -3.0720e-02, -7.8343e-03,  9.5754e-02, -1.7389e-02, -3.8495e-02,\n",
      "        -3.4525e-02, -7.8175e-03,  4.1913e-03,  1.6224e-03, -5.1531e-02,\n",
      "         3.5565e-02, -2.6480e-02,  7.6051e-03, -1.0527e-02, -1.6660e-01,\n",
      "         5.5316e-02, -2.4117e-02, -1.3778e-02, -6.8236e-02, -3.4082e-02,\n",
      "        -1.3544e-01,  2.8125e-02, -1.8776e-01,  3.6122e-02, -7.7760e-02,\n",
      "         4.1001e-02, -2.4881e-02,  5.2710e-02, -3.4962e-02,  1.4924e-02,\n",
      "        -1.1849e-01,  6.5794e-02,  8.7387e-02, -6.9806e-02,  1.0994e-01,\n",
      "        -1.3326e-01, -5.3605e-02, -2.4228e-02,  7.4287e-02, -5.2700e-02,\n",
      "         4.0522e-02, -4.5447e-02, -2.4756e-02,  3.3513e-02, -1.0339e-01,\n",
      "         8.4118e-03, -4.1316e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5167],\n",
      "        [0.4995],\n",
      "        [0.5178],\n",
      "        ...,\n",
      "        [0.4938],\n",
      "        [0.5060],\n",
      "        [0.5005]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([ 1.9411e-02,  2.5464e-03,  1.8959e-02,  7.7831e-03,  2.9958e-03,\n",
      "         1.3292e-02, -3.6954e-02, -1.0648e-02, -3.0839e-02, -1.6870e-02,\n",
      "         4.6559e-02, -2.2130e-02,  1.1675e-02,  1.3455e-02,  2.0252e-03,\n",
      "        -3.1464e-03,  2.5023e-03, -2.3686e-03, -1.5727e-02, -1.7237e-02,\n",
      "         9.5492e-03, -1.9391e-02, -2.2964e-02,  2.3192e-02, -5.0384e-03,\n",
      "        -4.1180e-02, -7.5831e-03,  4.4692e-02, -2.1656e-02,  8.0779e-03,\n",
      "         7.6051e-03,  1.6425e-02, -6.4527e-03, -6.3344e-03, -1.0373e-02,\n",
      "        -3.2024e-02,  2.2780e-03, -2.4019e-02, -5.8709e-03, -6.0602e-04,\n",
      "        -2.2394e-02,  1.0255e-02, -5.2587e-03, -4.0963e-02,  2.7056e-02,\n",
      "         3.0401e-02,  1.7399e-02,  2.8903e-02,  6.3466e-03, -2.6281e-02,\n",
      "        -1.2954e-02, -1.5819e-03, -5.3283e-03, -3.5944e-03,  1.3615e-03,\n",
      "        -1.0979e-02,  2.9080e-02,  3.9680e-02, -3.3155e-02,  2.9677e-03,\n",
      "         4.4174e-02, -1.2296e-02, -1.0345e-02,  2.6573e-02, -2.7078e-02,\n",
      "        -4.1822e-02, -3.6964e-03, -7.0948e-03,  6.7388e-03, -2.3797e-02,\n",
      "        -5.4185e-03, -2.9496e-03, -1.0023e-02, -2.2784e-03, -4.0398e-03,\n",
      "         1.1341e-02,  1.1982e-04,  1.4739e-03, -9.3912e-03, -2.1230e-02,\n",
      "         2.4450e-02,  1.0642e-03, -3.7143e-03, -1.4608e-02, -2.2300e-02,\n",
      "         1.0041e-02,  9.4983e-03, -1.7637e-02, -3.4960e-02, -4.4755e-02,\n",
      "        -3.8036e-02, -1.4098e-02, -6.1154e-03,  1.4055e-02, -2.8065e-02,\n",
      "        -1.9268e-02,  9.3931e-03,  2.7349e-02,  2.4554e-02,  9.6212e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  2.5860e-01, -2.0367e-02, -6.9288e-02, -2.1071e-01,\n",
      "        -1.2531e-01, -3.0950e-02,  1.4426e-01,  1.3895e-01, -6.2489e-02,\n",
      "        -1.9306e-02, -2.8437e-02,  7.2275e-02, -2.2379e-02,  9.3813e-03,\n",
      "         1.8224e-03, -8.9365e-02,  1.0252e-01,  9.4286e-02, -6.1406e-03,\n",
      "         8.7753e-02, -4.7135e-02, -8.1336e-03, -2.0572e-01, -1.8729e-02,\n",
      "         7.3789e-02, -1.7610e-01, -4.4550e-02,  3.9130e-02, -3.2908e-02,\n",
      "         2.5017e-02, -3.3072e-02, -1.1924e-01, -1.1559e-01,  1.8483e-01,\n",
      "        -2.6626e-02, -2.0760e-02,  1.6735e-01, -8.5917e-03, -2.0055e-02,\n",
      "         8.3111e-02, -8.4127e-02,  4.9452e-02, -9.7295e-02, -1.1193e-01,\n",
      "        -8.0472e-02,  6.5192e-02, -1.0011e-03,  2.1025e-02,  8.9481e-02,\n",
      "         8.3760e-02, -9.6377e-02,  1.0742e-01,  4.1001e-02,  5.6390e-02,\n",
      "        -6.4220e-03, -7.0158e-02,  1.0830e-01, -2.6317e-02, -7.6337e-02,\n",
      "         4.6053e-02,  5.3154e-02,  4.2110e-02,  5.8147e-02,  8.2938e-02,\n",
      "         1.2271e-01, -5.4945e-02, -6.7212e-02,  4.2654e-02, -5.0208e-02,\n",
      "        -3.9530e-02,  4.6800e-02,  8.7165e-02, -4.8456e-02,  1.1866e-01,\n",
      "         2.8014e-01,  1.0698e-01,  4.4508e-02, -7.1491e-03, -1.0957e-02,\n",
      "        -1.3932e-01,  1.0835e-02,  1.1191e-01, -6.6001e-03,  2.4925e-02,\n",
      "         1.8591e-01,  1.7753e-02,  1.0970e-01, -2.1743e-03,  3.4455e-02,\n",
      "        -3.0038e-02, -2.4489e-02, -1.4237e-02, -5.5593e-03,  3.9930e-02,\n",
      "        -1.1584e-01, -4.6352e-02,  2.2322e-02,  6.5533e-03, -1.1567e-01,\n",
      "         5.2885e-02,  1.7554e-02,  9.1668e-02, -5.7175e-02, -1.0876e-01,\n",
      "         6.5398e-02,  5.9587e-02, -4.8953e-02, -2.7580e-02,  2.1284e-02,\n",
      "         3.1785e-02,  3.0722e-02, -8.1348e-02, -1.5414e-01,  2.3003e-02,\n",
      "         1.8202e-02, -1.4320e-01,  3.2550e-02, -2.2653e-04, -4.1781e-02,\n",
      "         7.6777e-02,  4.4327e-02,  1.6200e-01,  1.0581e-01, -9.8402e-02,\n",
      "         4.6573e-02,  3.8476e-02, -1.1152e-01,  9.8117e-03, -1.1751e-02,\n",
      "         1.5588e-02, -3.0693e-02, -1.2810e-01, -1.1017e-01,  2.2828e-02,\n",
      "         1.3794e-01,  9.3268e-02, -3.0230e-02,  2.8446e-02, -4.0004e-02,\n",
      "         3.6699e-02, -7.8485e-02, -8.1676e-02, -5.5767e-02,  3.7605e-02,\n",
      "         4.6439e-02, -9.8194e-02, -9.0676e-02,  5.6799e-02, -9.9121e-03,\n",
      "        -4.3227e-02, -1.5697e-01, -7.9576e-02, -1.8709e-02, -5.0571e-02,\n",
      "         7.7317e-02, -2.5929e-02, -1.6094e-01, -2.8248e-02, -2.5399e-02,\n",
      "        -1.9414e-02, -5.5955e-02,  1.0868e-01, -3.0827e-02, -2.0960e-01,\n",
      "         9.6676e-02,  2.7367e-02,  3.0926e-02,  1.7099e-02,  1.4693e-01,\n",
      "        -1.0792e-01, -1.5053e-01,  2.2944e-02, -1.9030e-02,  4.8378e-03,\n",
      "        -8.1821e-02, -1.4625e-01,  1.5986e-01, -1.2840e-01, -7.4059e-02,\n",
      "         4.7691e-02, -7.3153e-02,  1.0747e-01,  5.0627e-02,  3.4302e-02,\n",
      "        -8.3938e-02, -4.7530e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.4934],\n",
      "        [0.5017],\n",
      "        [0.4823],\n",
      "        ...,\n",
      "        [0.4839],\n",
      "        [0.4757],\n",
      "        [0.4955]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-6.5144e-03, -1.7110e-02,  8.8975e-03, -3.2310e-03,  2.5271e-02,\n",
      "        -1.3225e-02,  1.8274e-02,  1.5788e-02, -4.1384e-03,  2.4144e-02,\n",
      "         2.8658e-02,  1.6889e-02,  3.2435e-02,  4.3411e-02,  5.5372e-02,\n",
      "         4.6962e-02,  1.5500e-02, -3.0712e-02, -3.7283e-02,  3.6176e-03,\n",
      "        -2.1247e-03, -1.7137e-02, -5.7818e-03, -3.7316e-02, -8.3397e-03,\n",
      "        -2.6060e-02, -3.7714e-04, -1.7729e-03, -2.8487e-02,  5.8892e-03,\n",
      "        -2.0016e-02, -2.4632e-02, -2.2892e-02, -2.3041e-02,  3.0900e-02,\n",
      "         8.0322e-03, -5.6378e-03,  8.4166e-03, -2.1972e-02, -2.7743e-02,\n",
      "         2.0480e-03, -3.0142e-02, -4.7662e-02, -2.1427e-03, -6.8896e-03,\n",
      "        -1.6990e-02, -3.2073e-02, -2.8548e-02,  1.1410e-02, -3.4136e-02,\n",
      "        -3.0550e-02, -2.0274e-02,  2.0918e-02, -2.2069e-02, -2.1234e-02,\n",
      "         5.0765e-02, -1.4201e-02,  2.3257e-02, -2.3446e-02, -4.2513e-02,\n",
      "        -5.2301e-05,  2.0010e-03,  1.5269e-02,  4.8858e-03,  7.0124e-03,\n",
      "        -3.7942e-02, -1.6476e-02, -2.3870e-02,  1.5298e-03,  4.7946e-03,\n",
      "         7.9415e-03, -1.3976e-02, -8.5755e-03, -3.2719e-03, -2.9055e-02,\n",
      "         1.3311e-02, -6.2703e-03,  8.7468e-03,  4.1404e-02,  3.3428e-02,\n",
      "         1.4844e-02, -3.2051e-02,  1.1221e-02,  2.2967e-02, -1.9694e-02,\n",
      "        -1.1483e-02, -4.0332e-02, -1.3060e-02,  1.4258e-03, -1.8652e-03,\n",
      "         2.5938e-03,  1.1671e-02,  8.3024e-03,  5.3471e-02,  1.4103e-02,\n",
      "        -5.2834e-04, -1.4448e-02, -1.1002e-02, -1.1927e-02,  4.5898e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  7.0248e-02, -1.2978e-01,  6.4335e-03, -5.8775e-02,\n",
      "        -1.1497e-01,  1.3174e-01, -6.8522e-02,  5.1668e-02, -1.9925e-01,\n",
      "         4.4888e-02,  5.9793e-02, -7.2049e-02, -9.9690e-02, -4.1436e-02,\n",
      "        -4.3913e-03,  1.5916e-02,  3.4950e-02,  1.0576e-01,  1.4378e-01,\n",
      "         1.0108e-02, -4.5070e-03,  4.1835e-02, -4.5090e-02, -3.5815e-02,\n",
      "        -9.3402e-03, -7.9609e-02, -5.4069e-02,  3.2974e-02, -3.8042e-02,\n",
      "         1.8468e-02,  2.6774e-02, -1.1647e-01,  2.6158e-02,  4.9935e-02,\n",
      "        -1.5706e-01,  9.0211e-02,  5.1049e-02,  2.1541e-02, -6.3262e-02,\n",
      "         4.5503e-02,  3.2519e-02,  1.7866e-02, -7.2869e-02, -3.8564e-02,\n",
      "         8.2335e-02, -5.3515e-02,  1.9771e-02, -9.1458e-02,  2.2386e-02,\n",
      "         6.3995e-02,  8.0455e-02,  1.4595e-01, -9.6938e-02, -8.4138e-02,\n",
      "        -7.1021e-03,  2.9518e-02,  4.7869e-02, -9.2252e-02, -5.1656e-02,\n",
      "         1.2287e-02, -9.1642e-02, -1.3409e-01,  1.2420e-02, -4.3656e-02,\n",
      "         1.3120e-01, -7.6047e-02,  3.5139e-03, -5.3484e-02,  1.1648e-01,\n",
      "         8.1813e-02, -1.4106e-01,  3.2753e-02,  1.6860e-02,  1.0105e-01,\n",
      "        -1.9071e-02,  2.4768e-02,  2.0376e-01,  5.3037e-02, -6.0202e-02,\n",
      "        -2.0571e-02, -2.3038e-01,  4.7708e-02, -1.4832e-01,  7.2215e-02,\n",
      "         1.4153e-01,  4.1161e-02,  1.4760e-01, -2.7853e-02,  4.1670e-02,\n",
      "        -8.2729e-02,  2.5491e-02,  2.2471e-02, -6.0184e-02,  9.3287e-02,\n",
      "        -5.2369e-02, -5.4219e-02,  6.0357e-02, -5.0866e-02, -8.2271e-02,\n",
      "        -6.0813e-02,  3.4472e-02, -2.9774e-02, -5.5459e-02, -1.9752e-02,\n",
      "         7.8302e-02,  2.2148e-01, -5.1303e-02,  2.4470e-02,  4.1626e-02,\n",
      "        -5.0614e-02,  5.5411e-02, -1.3781e-01, -1.3171e-01,  5.3263e-02,\n",
      "         4.9314e-02,  9.9591e-02,  1.1454e-01,  3.5029e-02, -1.1293e-01,\n",
      "         9.1436e-02, -9.5242e-02,  3.5585e-04,  2.8079e-02, -5.4678e-02,\n",
      "        -1.1567e-02,  1.3607e-02,  4.6612e-03, -1.3891e-02,  4.3639e-03,\n",
      "        -6.9782e-02,  1.9485e-02, -5.7200e-02,  1.1353e-01,  1.1935e-02,\n",
      "         6.0467e-02, -2.9552e-02,  1.2723e-02, -1.9798e-02,  3.3676e-02,\n",
      "        -4.4366e-02, -9.0947e-02,  4.3663e-03,  8.9647e-02,  6.7399e-02,\n",
      "         6.8210e-02, -3.2434e-02, -9.7112e-03, -6.2462e-02,  1.5094e-01,\n",
      "         6.0720e-02,  1.4383e-02, -1.3781e-01,  9.1362e-02,  1.7658e-02,\n",
      "         7.3914e-02,  7.1554e-02, -1.2457e-01,  1.8529e-01,  4.0253e-02,\n",
      "         1.0018e-01,  4.9266e-02, -4.6870e-02,  1.2322e-01, -9.4004e-03,\n",
      "        -1.4824e-02, -3.4219e-02,  7.0319e-02, -8.5588e-03,  1.6122e-01,\n",
      "        -7.4018e-02,  1.4826e-01,  1.2930e-01, -1.9220e-01,  2.1164e-02,\n",
      "        -1.0717e-01, -7.8336e-03, -4.6802e-02, -2.7794e-02,  5.5134e-02,\n",
      "         7.3778e-02,  1.2351e-01,  1.5300e-02, -8.8520e-02, -5.9761e-04,\n",
      "         1.4086e-01,  1.1532e-01])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5015],\n",
      "        [0.5009],\n",
      "        [0.5073],\n",
      "        ...,\n",
      "        [0.4966],\n",
      "        [0.5034],\n",
      "        [0.5052]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([ 1.1852e-02,  3.6967e-02,  3.0542e-02,  9.6271e-03, -3.4332e-02,\n",
      "         1.8059e-02,  3.3708e-03, -8.1820e-03, -1.0558e-02,  4.7900e-03,\n",
      "        -1.0875e-02, -5.3545e-03, -1.3389e-02, -3.7088e-02,  2.6812e-02,\n",
      "        -1.4697e-02,  1.3584e-02,  1.1289e-02,  6.8155e-03,  1.1916e-02,\n",
      "         3.6669e-02, -8.0885e-03,  7.4965e-03, -8.0424e-04,  9.3786e-04,\n",
      "        -3.3870e-04,  3.8467e-02,  1.0461e-02,  1.2153e-03,  4.0833e-02,\n",
      "        -4.3518e-02,  8.1067e-03, -2.4287e-02, -5.0229e-02,  3.2873e-02,\n",
      "         4.6570e-02,  7.0269e-03, -4.2333e-02, -2.1845e-02, -8.5670e-03,\n",
      "        -4.8986e-03, -4.4362e-04, -1.5573e-02,  1.5645e-02, -1.4747e-02,\n",
      "         3.2299e-02, -3.5458e-02,  1.9725e-02, -3.0225e-02, -1.4132e-02,\n",
      "        -3.5559e-02,  3.7710e-02,  3.5115e-02,  9.4245e-03,  2.6800e-02,\n",
      "        -2.0212e-02, -3.1201e-02,  1.1133e-02,  8.2875e-03,  4.0163e-02,\n",
      "         6.9492e-03, -3.3763e-05,  2.6165e-02, -4.1686e-03,  2.2859e-02,\n",
      "        -4.5654e-02,  4.7722e-03,  3.6034e-03,  3.6448e-03, -3.2881e-02,\n",
      "        -2.5298e-02, -2.2895e-03, -6.0842e-03,  2.5143e-02, -6.0577e-03,\n",
      "        -4.1489e-02,  4.0297e-03,  5.7703e-02,  1.5211e-02, -7.2179e-03,\n",
      "         1.3006e-02,  4.4084e-04, -3.8735e-02, -1.0206e-02, -1.4343e-02,\n",
      "         3.8189e-02,  3.0405e-02,  2.5914e-02,  7.4264e-03,  8.3502e-03,\n",
      "        -1.3013e-02, -1.6953e-02, -1.4607e-02,  1.1613e-02, -8.6741e-03,\n",
      "        -4.7307e-02,  1.2272e-02,  2.5445e-02, -4.6382e-02, -3.9764e-02,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  9.6602e-02,  5.3369e-02,  1.8159e-01, -6.6704e-02,\n",
      "        -1.1376e-01,  6.4541e-02,  9.6691e-02,  4.6548e-02, -8.8751e-02,\n",
      "        -2.4926e-02,  3.1265e-02,  1.0318e-01,  8.9955e-02,  3.6250e-02,\n",
      "        -8.6333e-03,  6.1472e-02, -2.2076e-03,  8.0518e-03,  4.6389e-02,\n",
      "         5.2431e-02,  5.5285e-02,  6.8273e-04, -1.1123e-01,  9.2545e-02,\n",
      "         7.4217e-02, -1.9606e-02,  9.3307e-02, -9.6795e-02, -6.8523e-02,\n",
      "         4.6001e-02,  5.1019e-03,  2.6752e-02,  4.9999e-02,  1.4168e-01,\n",
      "         2.1613e-03, -8.8572e-03,  3.9497e-02,  1.0709e-01,  7.9561e-02,\n",
      "        -5.3638e-03, -4.3123e-02,  4.7176e-02, -2.3379e-02, -1.1045e-02,\n",
      "        -1.2685e-02, -6.5145e-02, -1.4168e-01,  9.1599e-02,  5.0898e-03,\n",
      "        -2.4385e-03,  3.4641e-02,  8.9501e-03, -7.4836e-02,  2.2424e-02,\n",
      "        -4.7833e-02, -2.6258e-02,  1.2377e-01, -3.9914e-02, -9.2833e-02,\n",
      "         4.6860e-02, -1.9179e-01, -7.1933e-03, -8.3246e-02,  1.2027e-01,\n",
      "         5.6820e-02,  4.7227e-02,  6.4070e-02,  5.0871e-02,  3.1092e-02,\n",
      "        -1.1444e-01, -4.7160e-02,  7.8272e-02,  4.3817e-02, -9.7268e-02,\n",
      "        -7.8438e-02, -4.5625e-02,  4.1414e-02,  1.0038e-01, -8.5166e-05,\n",
      "        -1.8832e-02, -7.2940e-02,  1.3865e-01, -8.3014e-02,  4.0646e-02,\n",
      "        -9.3655e-02, -2.6070e-02,  1.0807e-01,  3.1196e-02, -1.5180e-02,\n",
      "        -8.6628e-02, -1.9005e-02,  8.1977e-02, -1.4453e-01, -5.9520e-02,\n",
      "        -1.7713e-02, -4.9119e-02, -1.9017e-02, -4.7361e-02, -5.1290e-02,\n",
      "         1.3209e-02,  7.7130e-02, -8.5571e-02, -7.2526e-03,  2.6163e-02,\n",
      "        -8.6393e-02,  3.4784e-02,  2.8158e-02,  4.3907e-02,  1.4208e-01,\n",
      "        -1.0797e-01,  2.6858e-02, -2.1886e-02,  3.4457e-02, -1.9514e-01,\n",
      "         6.4187e-02,  1.9977e-03, -9.6036e-02, -4.9107e-02, -1.1774e-01,\n",
      "         8.0631e-03, -5.7727e-02, -1.6658e-01, -1.2921e-01,  9.8396e-03,\n",
      "        -2.2408e-02, -1.4794e-02,  1.4130e-02,  4.8626e-03, -1.2059e-02,\n",
      "        -2.0037e-02,  4.9751e-02, -1.3206e-02, -4.4699e-02, -9.9649e-02,\n",
      "        -1.6549e-01, -3.5041e-02, -2.6943e-03,  1.5139e-01,  1.6238e-01,\n",
      "         1.3801e-01,  1.9891e-01,  1.0466e-02,  5.3149e-02, -3.5072e-02,\n",
      "         5.6383e-02, -3.0865e-02,  4.1190e-02, -4.5178e-02,  8.3959e-02,\n",
      "         4.3225e-02, -6.9204e-02,  2.8038e-02,  1.3062e-02,  4.8833e-02,\n",
      "         8.1677e-02,  2.5447e-02, -6.3119e-02,  1.5517e-01, -5.3819e-02,\n",
      "         9.5615e-03,  1.1670e-01, -5.6058e-02, -8.3011e-02, -2.0597e-03,\n",
      "         8.7099e-02,  3.5667e-02, -1.2848e-01,  2.8829e-02, -7.6750e-02,\n",
      "         4.2138e-02, -8.8183e-02, -1.4351e-01,  3.8078e-02, -4.0689e-02,\n",
      "        -1.8940e-01,  2.3841e-02, -5.8815e-02,  4.8734e-02, -8.2560e-02,\n",
      "         4.6635e-02, -3.4616e-03,  6.6973e-02, -5.0136e-03,  9.2302e-02,\n",
      "         3.3545e-02,  1.7861e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5325],\n",
      "        [0.5522],\n",
      "        [0.5431],\n",
      "        ...,\n",
      "        [0.5503],\n",
      "        [0.5428],\n",
      "        [0.5424]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-1.8803e-02, -7.1988e-03,  9.5597e-05, -5.4604e-03,  3.7986e-03,\n",
      "        -4.0481e-02, -2.3397e-02, -2.5342e-02, -2.6979e-02, -4.2698e-03,\n",
      "        -1.0922e-02,  2.1754e-03,  2.9503e-02, -1.6490e-03, -6.4427e-03,\n",
      "         7.7516e-03, -1.8782e-02, -2.3464e-02, -1.6355e-02,  8.6130e-03,\n",
      "        -6.2866e-03, -2.4479e-03,  3.4568e-02,  1.0700e-02,  1.2900e-02,\n",
      "         1.7101e-02,  2.3341e-03,  1.3363e-02, -5.6421e-03,  7.5864e-03,\n",
      "        -1.0012e-02,  1.7098e-02,  5.3263e-02, -1.0591e-02,  1.7146e-03,\n",
      "         1.9951e-02, -1.9031e-02,  1.7471e-02,  2.8824e-04,  7.4233e-03,\n",
      "         2.2190e-02, -5.3386e-03,  4.5387e-03, -4.4893e-02, -3.9042e-02,\n",
      "        -2.1884e-02, -3.2227e-02, -2.3650e-02,  3.1876e-02,  3.4792e-02,\n",
      "        -3.1830e-02,  5.7207e-03,  1.4846e-02,  2.5073e-02,  1.1435e-02,\n",
      "        -1.0193e-02,  9.5291e-03,  3.9353e-03, -3.3142e-03, -2.7473e-03,\n",
      "        -4.5691e-03, -3.0794e-02, -2.0467e-02,  2.5420e-02, -3.2529e-02,\n",
      "        -6.2746e-03, -1.6322e-02,  1.8305e-02,  1.9850e-03, -1.3029e-02,\n",
      "        -8.0255e-03,  1.4917e-02,  2.0203e-02, -4.2283e-02, -3.3138e-02,\n",
      "        -3.4618e-03,  1.6072e-02,  3.1940e-02,  3.4523e-03, -1.9688e-02,\n",
      "         1.1633e-02, -1.4569e-02,  1.2842e-02,  2.2477e-02,  4.4217e-03,\n",
      "         7.6001e-03, -8.2229e-03, -3.4487e-02, -2.7657e-02, -2.0438e-02,\n",
      "        -2.9734e-02,  1.5205e-02,  1.0541e-02, -3.7720e-03,  2.8507e-02,\n",
      "         8.7517e-03, -1.4598e-02, -2.9384e-03,  1.1988e-03, -2.1004e-02,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  7.9363e-02,  6.0022e-02,  5.3243e-04,  7.2535e-02,\n",
      "        -5.9543e-02, -1.7746e-02, -7.5156e-02, -7.5924e-02,  5.7555e-02,\n",
      "        -1.7765e-01, -2.4721e-02,  2.0971e-02,  1.0033e-01, -2.0470e-01,\n",
      "         3.7784e-02, -7.5226e-02,  1.0291e-01,  1.7478e-01, -1.2583e-01,\n",
      "        -2.5614e-02, -5.4676e-02,  5.9493e-02,  5.5084e-02, -7.4566e-02,\n",
      "        -5.9565e-02,  1.3969e-01,  4.7803e-02, -8.2652e-02, -1.2664e-01,\n",
      "         9.6729e-02, -4.8426e-02, -8.9908e-03,  9.3555e-02,  1.5059e-01,\n",
      "         1.8385e-02,  8.7484e-02, -2.0267e-02, -1.0055e-01,  4.7084e-02,\n",
      "         7.7250e-03, -1.5232e-01, -4.0334e-02,  7.0593e-02,  2.4185e-02,\n",
      "         4.4780e-02,  7.8335e-02, -5.6616e-02,  7.4238e-04, -5.2948e-02,\n",
      "        -5.1895e-02, -1.4981e-01, -1.0119e-02,  4.4661e-03,  7.3942e-02,\n",
      "         1.2857e-01, -1.1063e-01, -1.4690e-02,  6.6068e-02, -5.1666e-02,\n",
      "        -3.1609e-02,  1.6427e-02,  7.3762e-03,  1.3029e-02,  2.8014e-02,\n",
      "        -1.2916e-01, -6.6060e-02,  1.5682e-01, -6.1625e-03, -3.0522e-02,\n",
      "         8.5978e-02,  6.1374e-02,  4.8506e-02,  1.0979e-01, -1.7783e-02,\n",
      "         1.3252e-01,  1.4239e-01,  9.4731e-02, -1.1188e-01, -7.5875e-02,\n",
      "        -1.0388e-01, -3.0395e-03,  1.6208e-01, -1.4128e-01, -1.0900e-02,\n",
      "        -1.1102e-01, -1.6929e-01,  2.8814e-02,  3.7657e-02,  2.5321e-02,\n",
      "        -2.3450e-02, -7.8325e-02, -4.8992e-02, -5.2797e-02,  1.2663e-01,\n",
      "        -7.3111e-02,  1.3871e-01, -2.5051e-02, -5.8412e-02,  1.2375e-01,\n",
      "        -1.0458e-01,  2.2712e-02, -6.5641e-02,  1.9200e-01, -3.1669e-02,\n",
      "         1.6253e-01, -3.7657e-02,  1.7232e-02,  7.5769e-02,  1.2505e-01,\n",
      "         3.9248e-02, -8.4521e-02, -1.6341e-02, -1.0946e-01, -1.3108e-01,\n",
      "         1.9622e-01, -6.5008e-02,  9.4387e-02, -7.6500e-02, -1.8052e-02,\n",
      "        -1.5660e-01, -6.8725e-02,  1.1993e-01,  1.0090e-01, -1.6440e-02,\n",
      "        -4.8486e-02,  1.1337e-02, -1.3866e-01, -4.9222e-02, -7.4616e-02,\n",
      "        -3.1612e-02,  5.2876e-02, -3.4329e-02,  1.2092e-01, -1.7923e-01,\n",
      "         5.2858e-02, -1.1483e-01, -1.3113e-01,  8.0907e-02,  4.9141e-02,\n",
      "        -8.3521e-02,  8.7936e-02, -1.4830e-03, -5.1470e-02,  8.5545e-02,\n",
      "        -7.2995e-02, -1.6641e-01,  2.2253e-01, -1.2542e-01,  2.3989e-02,\n",
      "         3.0652e-02, -9.4552e-02, -4.2806e-02, -1.2614e-01, -5.5844e-02,\n",
      "        -1.6762e-01,  1.5177e-02,  3.1224e-02,  1.9273e-01,  1.3567e-01,\n",
      "         1.2956e-02,  9.7266e-02,  3.4970e-02, -9.4357e-02, -1.5050e-01,\n",
      "        -1.1868e-01,  6.1523e-02,  3.6725e-02,  1.1999e-01, -9.7411e-02,\n",
      "         9.2121e-02, -7.4681e-02, -2.5520e-02, -1.2445e-01, -5.9045e-02,\n",
      "        -2.3927e-01,  8.8270e-02,  3.9694e-03, -1.0986e-01,  2.2560e-02,\n",
      "         9.6081e-02,  2.6488e-01, -5.4379e-02, -1.3616e-01,  2.1194e-03,\n",
      "         2.6065e-02, -6.2295e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5204],\n",
      "        [0.5346],\n",
      "        [0.5517],\n",
      "        ...,\n",
      "        [0.5221],\n",
      "        [0.5221],\n",
      "        [0.5340]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-2.3511e-03, -3.1665e-02,  1.0946e-02, -2.0821e-02, -2.1519e-02,\n",
      "        -2.1857e-02, -2.0258e-02, -2.5532e-02,  1.0573e-02,  3.9076e-02,\n",
      "         2.6911e-02,  4.2243e-02, -9.1896e-04,  2.3838e-02, -1.8328e-02,\n",
      "        -5.3831e-03, -3.1494e-02,  1.7390e-02, -4.6118e-03, -6.6487e-04,\n",
      "         4.3199e-02, -4.4432e-04,  1.2217e-02,  1.2656e-02,  4.6553e-02,\n",
      "        -1.6783e-02,  5.8799e-03, -2.2051e-02,  3.0950e-02, -4.2539e-02,\n",
      "         2.9999e-02,  9.3923e-03, -1.7063e-02,  1.3526e-02, -2.6689e-02,\n",
      "         1.0147e-02,  3.5562e-03,  1.1007e-02,  2.0639e-02,  1.5062e-02,\n",
      "         2.1681e-02,  3.3367e-02, -3.5003e-02, -6.9640e-04, -2.6505e-02,\n",
      "        -1.0058e-02, -4.1327e-02, -1.5638e-02, -2.4690e-02, -1.0079e-02,\n",
      "         2.1133e-03,  4.1005e-03, -2.3841e-02, -4.6363e-02,  6.9653e-03,\n",
      "         2.9103e-04,  1.3886e-02,  1.4694e-02, -9.0748e-03,  2.3173e-02,\n",
      "         2.4854e-02,  1.2323e-02, -1.4892e-02, -9.7433e-03,  6.6919e-02,\n",
      "        -1.5990e-02,  9.8060e-03,  2.5132e-03, -1.2361e-02,  4.6650e-04,\n",
      "         3.4536e-02, -1.7044e-02,  5.5890e-04,  3.9369e-03,  2.2847e-02,\n",
      "        -1.7945e-02,  1.1188e-02, -2.3877e-02,  4.1990e-03,  7.5611e-06,\n",
      "         2.0903e-03,  1.0903e-03, -3.2329e-02, -8.0650e-03, -2.2738e-02,\n",
      "         4.6066e-02, -1.5573e-02, -1.4265e-02,  2.8772e-02, -2.2372e-02,\n",
      "         2.2835e-02, -2.2993e-02,  2.9205e-02, -1.9072e-02, -1.1579e-02,\n",
      "         1.2877e-02, -3.2251e-02, -3.6698e-02,  4.1636e-02,  1.9300e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01,  2.3739e-02,  1.3008e-02,  9.3042e-04,  6.6228e-02,\n",
      "         1.2585e-01, -1.3051e-01, -3.3693e-02,  9.6792e-02,  5.1650e-02,\n",
      "         1.9111e-01,  2.9309e-02,  3.9271e-02, -3.1232e-02, -3.7179e-02,\n",
      "        -1.0886e-01, -1.3521e-02, -4.4631e-02, -2.4913e-02, -3.3529e-02,\n",
      "         1.9627e-02,  8.0408e-02,  1.3027e-01,  1.2663e-01, -9.7628e-02,\n",
      "        -1.0965e-01, -8.2792e-02,  4.3861e-03, -1.4742e-01, -5.6507e-02,\n",
      "        -7.2131e-02,  5.5080e-02, -3.9700e-02,  4.4011e-02, -1.3877e-01,\n",
      "        -1.9465e-02,  4.9928e-02, -5.3993e-03,  4.7721e-02,  3.6351e-02,\n",
      "        -1.1286e-01, -1.6534e-02, -8.8511e-02, -1.1870e-01,  4.6714e-02,\n",
      "         8.2795e-03, -8.6451e-02,  2.1138e-02,  2.9731e-03, -4.8847e-02,\n",
      "         5.5542e-02, -6.5816e-02,  9.6722e-02, -6.2014e-02,  1.8011e-01,\n",
      "        -1.7515e-01, -4.0096e-03,  3.5737e-02, -1.5556e-02, -4.0511e-02,\n",
      "        -9.6901e-02,  8.1206e-02,  3.2524e-02,  1.5480e-01, -7.7614e-02,\n",
      "         2.9099e-02, -3.0561e-02,  6.4776e-02, -7.4519e-02, -3.7294e-02,\n",
      "        -8.6275e-02,  2.9621e-02, -1.5000e-01, -1.5918e-02,  4.3017e-02,\n",
      "         3.8542e-02, -6.2975e-03, -2.0344e-02, -6.4622e-02, -5.9619e-02,\n",
      "        -1.8383e-02, -2.1561e-02, -1.6772e-01, -1.2512e-01,  1.1964e-01,\n",
      "        -6.9309e-02,  5.9654e-02,  1.8118e-01,  1.6176e-02, -4.5966e-02,\n",
      "        -5.5881e-02, -1.2672e-01, -1.2527e-01,  4.1349e-03,  3.5020e-02,\n",
      "         1.3916e-02,  1.3725e-01,  7.0348e-02,  8.7965e-02,  4.7822e-02,\n",
      "         1.5359e-01, -1.0440e-01, -8.2368e-02,  2.6405e-02,  4.1758e-02,\n",
      "        -5.3201e-02, -2.2272e-02, -1.4107e-01, -1.5446e-01, -1.0138e-01,\n",
      "         2.4306e-04,  8.4974e-02, -1.9424e-01, -2.4256e-01,  5.8642e-02,\n",
      "         1.7791e-01,  4.5643e-02, -3.6164e-02,  1.3027e-01, -2.0813e-01,\n",
      "        -2.0294e-01, -1.2727e-01, -5.9301e-02, -1.2839e-01, -8.3264e-03,\n",
      "         5.7793e-02,  1.8051e-01, -1.9932e-02,  1.0749e-01, -2.4519e-01,\n",
      "         2.0901e-02, -2.7227e-03, -4.0804e-02, -1.2031e-01,  2.5547e-02,\n",
      "         4.3705e-02, -7.9634e-02, -7.7015e-02, -1.2573e-01,  1.7633e-02,\n",
      "        -6.5631e-02,  2.1515e-02, -6.7204e-02, -3.4966e-02,  1.3336e-01,\n",
      "        -5.5698e-02, -2.6763e-02,  3.1583e-03,  9.5816e-02,  8.6297e-02,\n",
      "         4.5966e-02, -2.1643e-03, -7.7317e-02, -1.4250e-01,  6.4946e-03,\n",
      "        -9.9870e-02, -9.8072e-02,  1.3240e-02,  1.3895e-02, -5.7235e-02,\n",
      "        -6.5093e-02,  9.0693e-02,  1.0360e-02,  1.1151e-01,  8.4154e-02,\n",
      "         8.4328e-02, -9.6511e-02, -1.1994e-01,  5.0932e-02, -3.6329e-02,\n",
      "         5.7885e-02, -2.9370e-03,  3.8909e-02,  3.7260e-02,  1.7147e-01,\n",
      "         4.4360e-02, -8.7106e-02, -4.8595e-02,  1.1138e-01, -4.3512e-02,\n",
      "         8.8505e-02, -4.9318e-02, -6.1474e-02, -4.8190e-02, -8.6977e-02,\n",
      "        -2.2105e-01, -8.1492e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5555],\n",
      "        [0.5591],\n",
      "        [0.5449],\n",
      "        ...,\n",
      "        [0.5470],\n",
      "        [0.5546],\n",
      "        [0.5465]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-6.6013e-04, -7.7136e-03, -3.1890e-02, -2.1872e-02,  3.9065e-02,\n",
      "        -3.0385e-02, -3.6812e-03,  1.3989e-02, -1.5782e-02,  3.4559e-02,\n",
      "        -4.8009e-02,  2.8478e-02,  3.4769e-02, -2.0524e-02, -2.7433e-02,\n",
      "         1.1537e-02, -9.4036e-03,  2.5542e-02, -9.8673e-03,  1.5713e-02,\n",
      "        -1.7729e-03, -7.8026e-03, -1.6812e-02, -2.1656e-03, -2.8716e-02,\n",
      "        -4.4369e-02, -1.7160e-02,  2.9601e-02, -1.5433e-03,  4.8930e-02,\n",
      "         4.2196e-02,  2.9356e-02, -4.7979e-02, -2.7316e-03,  1.5692e-02,\n",
      "         7.7608e-03,  2.7375e-02, -1.4327e-02,  1.5112e-03,  1.1361e-02,\n",
      "        -2.1976e-02,  6.4197e-03,  2.3879e-02, -1.6860e-02, -2.2693e-02,\n",
      "        -1.0139e-02,  4.2929e-03, -2.9537e-02,  2.9689e-03,  1.8617e-02,\n",
      "         1.5538e-02,  2.1931e-02, -2.3105e-02, -1.5195e-02,  1.1628e-02,\n",
      "         1.4627e-02,  3.8978e-02, -2.9226e-02, -8.4008e-03,  1.7706e-02,\n",
      "         4.9131e-03,  9.7747e-03,  3.1047e-03,  6.9094e-03,  4.0637e-03,\n",
      "        -8.5404e-03,  1.1414e-03, -2.7318e-02, -1.1375e-02,  4.5483e-02,\n",
      "        -9.3718e-03, -2.6056e-03, -2.2685e-02, -2.0112e-02,  1.0573e-02,\n",
      "        -3.8513e-02, -7.6359e-03, -2.7336e-02, -1.8148e-02, -2.1725e-02,\n",
      "        -4.7332e-02,  3.0295e-02,  1.9700e-02,  1.1562e-02, -6.9375e-03,\n",
      "         2.0736e-02,  1.1450e-02,  4.7582e-02, -3.6539e-03,  1.8437e-02,\n",
      "         4.0409e-03, -2.1679e-02, -9.7885e-03,  1.4478e-02, -5.9261e-03,\n",
      "        -4.9881e-02,  1.0425e-02, -8.4215e-03, -1.5057e-02,  1.7400e-02,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01, -8.5041e-04,  2.3715e-03, -2.1882e-02,  3.7213e-02,\n",
      "         1.4401e-01, -8.1441e-02, -4.2350e-02,  9.5544e-02,  1.0121e-01,\n",
      "        -9.1037e-02,  7.0475e-02, -1.1940e-01, -1.2098e-02,  3.2816e-03,\n",
      "        -1.7387e-01, -9.3211e-02,  3.0441e-02,  1.3795e-01, -1.3005e-01,\n",
      "         3.9114e-02,  2.2505e-03,  8.2772e-02, -1.6420e-01,  1.2287e-01,\n",
      "         1.3587e-02,  1.9327e-02,  7.0744e-02,  1.3826e-02,  5.2388e-02,\n",
      "         1.6507e-01, -6.6556e-02,  3.8878e-02, -5.2404e-02,  1.8919e-02,\n",
      "         4.8082e-02, -9.1176e-03,  8.0794e-02,  7.3045e-02,  6.4172e-02,\n",
      "        -9.4909e-02, -5.7283e-02,  1.6187e-02,  9.3364e-02, -5.9187e-02,\n",
      "         7.6970e-02, -1.3462e-01, -8.0496e-02,  1.6434e-01,  9.6332e-02,\n",
      "        -1.2178e-01, -3.3139e-02,  6.6351e-02,  9.8070e-03, -1.2164e-01,\n",
      "        -1.2594e-01, -2.6309e-02, -2.0729e-01,  7.6691e-02, -1.2164e-01,\n",
      "        -1.0796e-01, -4.4916e-02,  2.4912e-02, -1.9196e-01, -8.8158e-03,\n",
      "        -2.7294e-02, -1.1787e-01,  3.2301e-02, -1.3290e-01, -1.3958e-01,\n",
      "        -1.8725e-01, -4.1358e-02, -4.2427e-02, -2.3733e-02,  1.3939e-02,\n",
      "         2.4834e-03, -2.1693e-02, -8.2662e-02, -4.0472e-02,  1.9359e-01,\n",
      "        -1.5180e-01, -5.0286e-04, -4.6827e-02,  9.9610e-02, -1.0942e-01,\n",
      "        -7.8122e-02, -1.0408e-01, -3.1014e-02,  1.5682e-01, -5.5589e-03,\n",
      "         2.6005e-03, -7.1347e-02,  1.7243e-01, -3.7886e-02, -1.0548e-01,\n",
      "        -1.1196e-01,  6.9136e-03, -8.4832e-02,  2.2487e-02,  6.2707e-02,\n",
      "         1.4686e-02,  7.0328e-02,  1.6334e-01, -1.6931e-02, -5.0957e-02,\n",
      "        -2.0929e-01,  1.5219e-01,  3.1918e-03,  8.1691e-02,  1.7400e-01,\n",
      "         7.3832e-02, -2.3341e-01,  7.4197e-02,  4.7208e-02, -7.2586e-02,\n",
      "         4.0382e-02,  9.5378e-02, -1.5690e-01,  1.2825e-01, -5.7023e-03,\n",
      "        -8.8634e-02, -8.2326e-02, -1.2347e-01, -1.0933e-01,  2.4245e-02,\n",
      "        -9.2444e-02,  7.3753e-02,  1.1351e-01, -6.8341e-04, -2.8892e-02,\n",
      "        -1.4881e-02, -2.6535e-03, -5.7522e-02, -1.7255e-01,  9.0381e-02,\n",
      "         1.0059e-01, -2.8182e-02, -7.0579e-02,  8.6626e-02, -1.3553e-02,\n",
      "        -1.0970e-02, -1.7843e-01, -1.4420e-01, -2.3142e-02, -1.0233e-01,\n",
      "        -8.3164e-02, -1.5546e-01,  7.0385e-02,  1.9364e-02,  3.1194e-02,\n",
      "        -3.1433e-02,  1.5017e-01, -6.7825e-02,  1.0415e-01,  5.7375e-02,\n",
      "         3.1779e-02, -1.6406e-02,  6.2762e-02,  9.9800e-02, -1.3046e-01,\n",
      "         5.1235e-02, -6.4021e-02,  1.1836e-01,  1.6792e-02,  1.3798e-01,\n",
      "        -7.4252e-02,  1.8117e-02, -1.2309e-01, -1.8789e-01, -1.8066e-02,\n",
      "         8.2535e-02, -9.6570e-02,  8.3026e-02,  1.3897e-01, -5.2323e-02,\n",
      "        -1.1211e-01,  4.8136e-02, -8.6227e-02, -1.1005e-01,  9.2076e-02,\n",
      "        -5.2118e-02, -1.0347e-01,  5.0027e-03, -1.7652e-01,  3.8045e-02,\n",
      "         6.1074e-03,  3.7556e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.5011],\n",
      "        [0.4978],\n",
      "        [0.5064],\n",
      "        ...,\n",
      "        [0.4907],\n",
      "        [0.4912],\n",
      "        [0.4993]])\n",
      "################################\n",
      "non_target_rep: torch.Size([1783, 8, 186])\n",
      "target_rep: torch.Size([1783, 1, 186])\n",
      "att_output: torch.Size([1783, 1, 186])\n",
      "attention_concat: torch.Size([1783, 1, 372])\n",
      "################################\n",
      "attention_concat flatten: torch.Size([1783, 372])\n",
      "tensor([-4.1293e-06,  2.4297e-02, -2.4103e-02,  6.4270e-03, -3.8282e-03,\n",
      "        -4.1337e-03,  8.5023e-03,  3.9886e-03, -3.3736e-02, -3.5878e-02,\n",
      "        -1.2825e-02,  1.8182e-02,  2.5574e-02, -4.0427e-02, -6.5410e-03,\n",
      "        -9.9843e-03, -1.6058e-02,  2.6756e-02,  1.1150e-03, -2.6066e-02,\n",
      "         2.2432e-02, -8.2738e-03, -1.0659e-02, -2.0997e-02, -6.4552e-03,\n",
      "         7.5214e-03, -4.0756e-04, -2.2167e-02, -5.6136e-03,  3.2353e-02,\n",
      "         2.1918e-02, -9.1564e-03, -3.3053e-02,  1.9748e-02, -5.4351e-02,\n",
      "         3.9710e-03, -2.6419e-02, -1.0660e-02,  9.2822e-03, -1.0265e-02,\n",
      "         1.3900e-02,  1.6365e-03, -2.5901e-03,  1.5431e-03,  3.4005e-03,\n",
      "        -1.0397e-03,  6.0740e-03,  1.5731e-02, -2.1359e-03, -5.0160e-03,\n",
      "        -2.1567e-03, -1.0005e-02, -2.9217e-02,  2.2244e-02,  3.0877e-02,\n",
      "         1.3387e-02,  1.4991e-02, -1.5156e-02, -6.1194e-03, -7.6837e-03,\n",
      "        -3.5409e-02,  2.2788e-02,  2.1196e-03,  3.9109e-04, -2.1760e-02,\n",
      "         3.9269e-02, -3.2927e-02,  2.7963e-02,  2.0435e-02,  1.9075e-02,\n",
      "        -1.8756e-03, -2.1654e-03,  5.7787e-03, -6.2191e-03, -9.2456e-03,\n",
      "        -2.5420e-02, -5.0739e-03, -5.2072e-02,  1.2289e-02, -2.8437e-02,\n",
      "         1.4796e-02,  3.2064e-02,  1.5368e-02, -1.0974e-02,  1.3481e-02,\n",
      "        -3.5270e-02, -3.1471e-02,  2.7390e-02,  2.2112e-02, -1.6948e-03,\n",
      "        -4.9274e-02, -2.5008e-02,  6.9078e-03, -3.0743e-04, -8.0955e-03,\n",
      "        -6.5426e-03, -1.1469e-02, -4.1140e-02, -9.1146e-03, -1.9521e-03,\n",
      "         2.8764e-01,  1.3948e-01,  2.4867e-01,  2.9948e-01,  2.0432e-01,\n",
      "         2.4062e-01,  2.2059e-01,  2.7132e-01,  2.5000e-01,  1.9737e-01,\n",
      "         5.0000e-01,  1.7532e-01,  5.0000e-01,  3.4401e-01,  3.1513e-01,\n",
      "         2.4956e-01,  4.8775e-01,  2.5000e-01,  5.0000e-01,  2.9167e-01,\n",
      "         0.0000e+00,  1.2948e-01,  3.7708e-01,  1.2911e-01,  3.5220e-01,\n",
      "         1.8059e-01,  2.0253e-01,  2.1534e-01,  8.1073e-02,  4.3601e-02,\n",
      "         5.4937e-01,  3.8186e-01,  0.0000e+00,  3.3474e-01,  1.9793e-01,\n",
      "         0.0000e+00,  4.8167e-01,  0.0000e+00,  0.0000e+00,  1.5190e-01,\n",
      "         5.7731e-01,  0.0000e+00,  2.0000e-01,  5.5522e-01,  0.0000e+00,\n",
      "         3.3513e-01,  1.8987e-01,  2.0886e-01,  3.9964e-01,  0.0000e+00,\n",
      "         2.6329e-01,  3.9188e-01,  4.8062e-01,  2.3830e-01,  7.4803e-01,\n",
      "         3.9738e-01,  3.5456e-01,  4.8224e-01,  1.9745e-01,  3.5956e-01,\n",
      "         2.0792e-01,  1.5419e-01,  5.3108e-01,  5.0000e-02,  4.0172e-01,\n",
      "         2.0039e-01,  1.7493e-01,  2.3412e-01,  2.8249e-01,  1.1429e-01,\n",
      "         3.3333e-01,  9.4737e-02,  7.5758e-02,  1.9672e-01,  2.2449e-01,\n",
      "         1.4286e-01,  1.8182e-01,  2.1739e-01,  1.7094e-01,  2.4390e-02,\n",
      "         8.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e-01,\n",
      "         2.5000e-01, -1.3710e-01,  1.5142e-01,  4.9700e-02,  4.1820e-02,\n",
      "        -1.5645e-01,  3.3351e-02, -4.0188e-02, -1.9187e-02,  2.0588e-02,\n",
      "         1.5490e-04,  7.3040e-04, -4.7905e-02, -7.5688e-02,  1.6746e-01,\n",
      "        -8.3847e-02,  3.1839e-02, -4.9132e-02,  3.4512e-04, -5.4139e-02,\n",
      "         3.2459e-02,  3.5178e-03, -2.2515e-02, -3.3354e-02,  7.7731e-02,\n",
      "        -1.2673e-01, -2.6375e-02,  4.2892e-02, -1.6364e-02, -5.3737e-02,\n",
      "         8.4668e-02,  1.0492e-01, -9.8415e-02,  6.0502e-03, -1.1712e-01,\n",
      "        -9.9690e-02, -2.6869e-01,  1.5126e-01, -1.0977e-01, -1.4776e-01,\n",
      "        -3.0217e-02, -1.0156e-01,  3.5314e-03,  1.1231e-01,  2.7838e-02,\n",
      "        -1.5576e-01,  1.8089e-02, -1.9315e-02,  1.2190e-01,  1.1318e-02,\n",
      "         1.9272e-01, -1.1763e-01,  8.1202e-02,  3.8540e-02,  2.2235e-02,\n",
      "         7.3826e-02, -1.5314e-01,  7.1565e-02, -6.2389e-02,  3.9125e-02,\n",
      "        -1.9170e-02,  3.3208e-02,  4.6786e-02,  3.1486e-02,  5.2414e-02,\n",
      "         7.9974e-03, -3.4195e-02, -7.6037e-02,  7.1171e-03, -4.3545e-02,\n",
      "         2.4933e-02, -5.5309e-02,  1.0074e-01, -6.6755e-02, -2.9454e-02,\n",
      "         6.3006e-02, -1.2136e-01,  6.2618e-03, -1.6022e-01,  1.6351e-01,\n",
      "         6.9092e-02,  1.5962e-02, -8.8523e-02,  3.6054e-02,  1.6689e-02,\n",
      "        -1.0075e-02,  7.8908e-02, -6.4132e-02, -2.9588e-02,  5.1051e-02,\n",
      "        -1.3961e-01,  5.3772e-02, -7.3269e-02, -1.2791e-01, -3.3435e-02,\n",
      "        -5.2097e-02, -3.2400e-02,  1.5386e-01,  6.0232e-02,  2.9789e-02,\n",
      "        -7.3636e-02,  3.2614e-02, -1.5525e-01, -5.9190e-03,  5.4245e-02,\n",
      "         1.7457e-02,  4.0241e-02, -4.7879e-02, -2.4103e-02, -1.8044e-02,\n",
      "         8.7301e-02, -7.6849e-02, -3.2794e-02, -2.1425e-01,  1.0692e-01,\n",
      "        -6.5075e-02,  1.5849e-02, -5.6573e-02, -4.0668e-04, -8.2742e-03,\n",
      "         5.9785e-02, -7.7821e-03,  1.8400e-01,  1.7562e-01,  5.9253e-02,\n",
      "         2.5097e-02,  3.0204e-02, -4.8125e-02,  2.9944e-02, -3.2923e-02,\n",
      "         9.7359e-03,  7.2817e-02, -6.3834e-02, -1.3576e-01, -4.6954e-02,\n",
      "         7.2265e-02,  5.4662e-02, -7.6518e-02, -1.2056e-02,  8.8078e-02,\n",
      "        -9.6689e-02, -1.5945e-01,  1.0489e-01, -4.9413e-02, -3.4219e-02,\n",
      "        -2.8669e-02,  3.4897e-02,  1.0340e-02, -6.0010e-02, -1.5340e-02,\n",
      "        -2.1100e-02, -7.3318e-02, -1.9231e-03, -5.7499e-02,  2.4467e-02,\n",
      "        -1.0914e-01,  7.4114e-03, -1.6326e-01, -1.7396e-01,  7.2967e-02,\n",
      "         1.7487e-02, -5.3129e-02, -1.4077e-01,  5.8525e-02,  1.4473e-01,\n",
      "         7.7514e-03,  2.2099e-02, -7.3198e-02,  4.6581e-02,  5.4288e-02,\n",
      "        -3.4746e-02,  1.6277e-01, -5.7419e-02, -7.4024e-02,  8.8695e-03,\n",
      "         4.3278e-03, -7.8741e-02, -1.0807e-01, -2.8434e-02, -1.8800e-01,\n",
      "         3.9522e-02,  1.5552e-01, -4.6127e-02,  1.1114e-01, -5.6580e-02,\n",
      "        -1.1073e-01,  5.2833e-02])\n",
      "################################\n",
      "################################\n",
      "final_pred:\n",
      "tensor([[0.4996],\n",
      "        [0.5121],\n",
      "        [0.4981],\n",
      "        ...,\n",
      "        [0.4949],\n",
      "        [0.5056],\n",
      "        [0.5084]])\n",
      "################################\n",
      "y: torch.Size([1783, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joohwan/miniforge3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "/Users/joohwan/miniforge3/lib/python3.10/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save best model to checkpoint_best1_1.pth\n",
      "CURRENT EPOCH: -1\n",
      "[DEV] AVG QWK: -0.0\n",
      "[DEV] score QWK: 0.042\n",
      "[DEV] content QWK: -0.009\n",
      "[DEV] organization QWK: 0.0\n",
      "[DEV] word_choice QWK: -0.022\n",
      "[DEV] sentence_fluency QWK: 0.024\n",
      "[DEV] conventions QWK: -0.039\n",
      "[DEV] prompt_adherence QWK: 0.036\n",
      "[DEV] language QWK: -0.004\n",
      "[DEV] narrativity QWK: -0.032\n",
      "------------------------\n",
      "[TEST] AVG QWK: -0.024\n",
      "[TEST] score QWK: 0.001\n",
      "[TEST] content QWK: -0.094\n",
      "[TEST] organization QWK: 0.063\n",
      "[TEST] word_choice QWK: -0.125\n",
      "[TEST] sentence_fluency QWK: 0.0\n",
      "[TEST] conventions QWK: 0.012\n",
      "------------------------\n",
      "[BEST TEST] AVG QWK: -0.024, {epoch}: -1\n",
      "[BEST TEST] score QWK: 0.001\n",
      "[BEST TEST] content QWK: -0.094\n",
      "[BEST TEST] organization QWK: 0.063\n",
      "[BEST TEST] word_choice QWK: -0.125\n",
      "[BEST TEST] sentence_fluency QWK: 0.0\n",
      "[BEST TEST] conventions QWK: 0.012\n",
      "--------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluator = AllAttEvaluator(\n",
    "    test_prompt_id, dev_data['prompt_ids'], test_data['prompt_ids'],\n",
    "    [x.numpy() for x in dev_features_list],\n",
    "    [x.numpy() for x in test_features_list], Y_dev, Y_test, seed\n",
    ")\n",
    "\n",
    "evaluator.evaluate(model, -1, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomHistory:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.val_loss.append(val_loss)\n",
    "        \n",
    "        \n",
    "custom_hist = CustomHistory()\n",
    "\n",
    "for epoch in range(epochs): # 50\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = [x.to(device) for x in batch_data]\n",
    "        inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "        outputs = model(*inputs)\n",
    "        loss = criterion.loss_function(targets.float(),outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_data[0].size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in dev_loader:\n",
    "            batch_data = [x.to(device) for x in batch_data]\n",
    "            inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "            outputs = model(*inputs)\n",
    "            #loss = criterion(outputs, targets)\n",
    "            loss = criterion.loss_function(targets, outputs)\n",
    "            val_loss += loss.item() * batch_data[0].size(0)\n",
    "        val_loss /= len(dev_loader.dataset)\n",
    "\n",
    "    custom_hist.update(train_loss, val_loss)\n",
    "\n",
    "    # evaluate\n",
    "    tt_time = time.time() - start_time\n",
    "    print(f\"Training one epoch in {tt_time:.3f} s\")\n",
    "    evaluator.evaluate(model, epoch + 1)\n",
    "    print(f\"Train Loss: {train_loss:.4f} || Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "evaluator.print_final_info()\n",
    "\n",
    "'''# show the loss as the graph\n",
    "fig, loss_graph = plt.subplots()\n",
    "loss_graph.plot(custom_hist.train_loss,'y',label='train loss')\n",
    "loss_graph.plot(custom_hist.val_loss,'r',label='val loss')\n",
    "loss_graph.set_xlabel('epoch')\n",
    "loss_graph.set_ylabel('loss')\n",
    "plt.savefig(str('images/protact/test_prompt_'+ str(test_prompt_id) + '_seed_' + str(seed) + '_loss.png'))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
