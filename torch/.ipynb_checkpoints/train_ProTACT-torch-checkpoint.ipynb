{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload behavior\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models.ProTACT import ProTACT\n",
    "from models.Loss import LossFunctions\n",
    "from configs.configs import Configs\n",
    "from utils.read_data_pr import read_pos_vocab, read_word_vocab, read_prompts_we, read_essays_prompts, read_prompts_pos\n",
    "from utils.general_utils import get_scaled_down_scores, pad_hierarchical_text_sequences, get_attribute_masks, load_word_embedding_dict, build_embedd_table\n",
    "from evaluators.multitask_evaluator_all_attributes import Evaluator as AllAttEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hayoung\\anaconda3\\lib\\site-packages\\autoreload\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import autoreload\n",
    "print(autoreload.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHistory:\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_loss.append(train_loss)\n",
    "        self.val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.18.1\n",
      "ipykernel        : 6.15.2\n",
      "ipywidgets       : 7.6.5\n",
      "jupyter_client   : 7.3.4\n",
      "jupyter_core     : 4.11.1\n",
      "jupyter_server   : 1.18.1\n",
      "jupyterlab       : 3.4.4\n",
      "nbclient         : 0.5.13\n",
      "nbconvert        : 6.4.4\n",
      "nbformat         : 5.5.0\n",
      "notebook         : 6.4.12\n",
      "qtconsole        : 5.2.2\n",
      "traitlets        : 5.1.1\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"main started\")\n",
    "    parser = argparse.ArgumentParser(description=\"ProTACT model\")\n",
    "    parser.add_argument('--test_prompt_id', type=int,\n",
    "                        default=1, help='prompt id of test essay set')\n",
    "    parser.add_argument('--seed', type=int, default=12, help='set random seed')\n",
    "    parser.add_argument('--model_name', type=str,\n",
    "                        choices=['ProTACT'],\n",
    "                        help='name of model')\n",
    "    parser.add_argument('--num_heads', type=int, default=2,\n",
    "                        help='set the number of heads in Multihead Attention')\n",
    "    parser.add_argument('--features_path', type=str,\n",
    "                        default='data/hand_crafted_v3.csv')\n",
    "    args = parser.parse_args()\n",
    "    test_prompt_id = args.test_prompt_id\n",
    "    seed = args.seed\n",
    "    num_heads = args.num_heads\n",
    "    features_path = args.features_path + str(test_prompt_id) + '.csv'\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    print(torch.cuda.is_available())\n",
    "    print(\"Test prompt id is {} of type {}\".format(\n",
    "        test_prompt_id, type(test_prompt_id)))\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    configs = Configs()\n",
    "\n",
    "    data_path = configs.DATA_PATH\n",
    "    train_path = data_path + str(test_prompt_id) + '/train.pk'\n",
    "    dev_path = data_path + str(test_prompt_id) + '/dev.pk'\n",
    "    test_path = data_path + str(test_prompt_id) + '/test.pk'\n",
    "    pretrained_embedding = configs.PRETRAINED_EMBEDDING\n",
    "    embedding_path = configs.EMBEDDING_PATH\n",
    "    readability_path = configs.READABILITY_PATH\n",
    "    prompt_path = configs.PROMPT_PATH\n",
    "    vocab_size = configs.VOCAB_SIZE\n",
    "    epochs = configs.EPOCHS\n",
    "    batch_size = configs.BATCH_SIZE\n",
    "    print(\"Numhead : \", num_heads, \" | Features : \",\n",
    "          features_path, \" | Pos_emb : \", configs.EMBEDDING_DIM)\n",
    "\n",
    "    read_configs = {\n",
    "        'train_path': train_path,\n",
    "        'dev_path': dev_path,\n",
    "        'test_path': test_path,\n",
    "        'features_path': features_path,\n",
    "        'readability_path': readability_path,\n",
    "        'vocab_size': vocab_size\n",
    "    }\n",
    "    # read POS for prompts\n",
    "    pos_vocab = read_pos_vocab(read_configs)\n",
    "    prompt_pos_data = read_prompts_pos(\n",
    "        prompt_path, pos_vocab)  # for prompt POS embedding\n",
    "\n",
    "    # read words for prompts\n",
    "    word_vocab = read_word_vocab(read_configs)\n",
    "    # for prompt word embedding\n",
    "    prompt_data = read_prompts_we(prompt_path, word_vocab)\n",
    "\n",
    "    train_data, dev_data, test_data = read_essays_prompts(\n",
    "        read_configs, prompt_data, prompt_pos_data, pos_vocab)\n",
    "\n",
    "    if pretrained_embedding:\n",
    "        embedd_dict, embedd_dim, _ = load_word_embedding_dict(embedding_path)\n",
    "        embedd_matrix = build_embedd_table(\n",
    "            word_vocab, embedd_dict, embedd_dim, caseless=True)\n",
    "        embed_table = embedd_matrix\n",
    "    else:\n",
    "        embed_table = None\n",
    "\n",
    "    max_sentlen = max(train_data['max_sentlen'],\n",
    "                      dev_data['max_sentlen'], test_data['max_sentlen'])\n",
    "    max_sentnum = max(train_data['max_sentnum'],\n",
    "                      dev_data['max_sentnum'], test_data['max_sentnum'])\n",
    "    prompt_max_sentlen = prompt_data['max_sentlen']\n",
    "    prompt_max_sentnum = prompt_data['max_sentnum']\n",
    "\n",
    "    print('max sent length: {}'.format(max_sentlen))\n",
    "    print('max sent num: {}'.format(max_sentnum))\n",
    "    print('max prompt sent length: {}'.format(prompt_max_sentlen))\n",
    "    print('max prompt sent num: {}'.format(prompt_max_sentnum))\n",
    "\n",
    "    train_data['y_scaled'] = get_scaled_down_scores(\n",
    "        train_data['data_y'], train_data['prompt_ids'])\n",
    "    dev_data['y_scaled'] = get_scaled_down_scores(\n",
    "        dev_data['data_y'], dev_data['prompt_ids'])\n",
    "    test_data['y_scaled'] = get_scaled_down_scores(\n",
    "        test_data['data_y'], test_data['prompt_ids'])\n",
    "\n",
    "    X_train_pos = pad_hierarchical_text_sequences(\n",
    "        train_data['pos_x'], max_sentnum, max_sentlen)\n",
    "    X_dev_pos = pad_hierarchical_text_sequences(\n",
    "        dev_data['pos_x'], max_sentnum, max_sentlen)\n",
    "    X_test_pos = pad_hierarchical_text_sequences(\n",
    "        test_data['pos_x'], max_sentnum, max_sentlen)\n",
    "\n",
    "    X_train_pos = X_train_pos.reshape(\n",
    "        (X_train_pos.shape[0], X_train_pos.shape[1] * X_train_pos.shape[2]))\n",
    "    X_dev_pos = X_dev_pos.reshape(\n",
    "        (X_dev_pos.shape[0], X_dev_pos.shape[1] * X_dev_pos.shape[2]))\n",
    "    X_test_pos = X_test_pos.reshape(\n",
    "        (X_test_pos.shape[0], X_test_pos.shape[1] * X_test_pos.shape[2]))\n",
    "\n",
    "    X_train_prompt = pad_hierarchical_text_sequences(\n",
    "        train_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "    X_dev_prompt = pad_hierarchical_text_sequences(\n",
    "        dev_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "    X_test_prompt = pad_hierarchical_text_sequences(\n",
    "        test_data['prompt_words'], max_sentnum, max_sentlen)\n",
    "\n",
    "    X_train_prompt = X_train_prompt.reshape(\n",
    "        (X_train_prompt.shape[0], X_train_prompt.shape[1] * X_train_prompt.shape[2]))\n",
    "    X_dev_prompt = X_dev_prompt.reshape(\n",
    "        (X_dev_prompt.shape[0], X_dev_prompt.shape[1] * X_dev_prompt.shape[2]))\n",
    "    X_test_prompt = X_test_prompt.reshape(\n",
    "        (X_test_prompt.shape[0], X_test_prompt.shape[1] * X_test_prompt.shape[2]))\n",
    "\n",
    "    X_train_prompt_pos = pad_hierarchical_text_sequences(\n",
    "        train_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "    X_dev_prompt_pos = pad_hierarchical_text_sequences(\n",
    "        dev_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "    X_test_prompt_pos = pad_hierarchical_text_sequences(\n",
    "        test_data['prompt_pos'], max_sentnum, max_sentlen)\n",
    "\n",
    "    X_train_prompt_pos = X_train_prompt_pos.reshape(\n",
    "        (X_train_prompt_pos.shape[0], X_train_prompt_pos.shape[1] * X_train_prompt_pos.shape[2]))\n",
    "    X_dev_prompt_pos = X_dev_prompt_pos.reshape(\n",
    "        (X_dev_prompt_pos.shape[0], X_dev_prompt_pos.shape[1] * X_dev_prompt_pos.shape[2]))\n",
    "    X_test_prompt_pos = X_test_prompt_pos.reshape(\n",
    "        (X_test_prompt_pos.shape[0], X_test_prompt_pos.shape[1] * X_test_prompt_pos.shape[2]))\n",
    "\n",
    "    X_train_linguistic_features = np.array(train_data['features_x'])\n",
    "    X_dev_linguistic_features = np.array(dev_data['features_x'])\n",
    "    X_test_linguistic_features = np.array(test_data['features_x'])\n",
    "\n",
    "    X_train_readability = np.array(train_data['readability_x'])\n",
    "    X_dev_readability = np.array(dev_data['readability_x'])\n",
    "    X_test_readability = np.array(test_data['readability_x'])\n",
    "\n",
    "    Y_train = np.array(train_data['y_scaled'])\n",
    "    Y_dev = np.array(dev_data['y_scaled'])\n",
    "    Y_test = np.array(test_data['y_scaled'])\n",
    "\n",
    "    X_train_attribute_rel = get_attribute_masks(Y_train)\n",
    "    X_dev_attribute_rel = get_attribute_masks(Y_dev)\n",
    "    X_test_attribute_rel = get_attribute_masks(Y_test)\n",
    "\n",
    "    print('================================')\n",
    "    print('X_train_pos: ', X_train_pos.shape)\n",
    "    print('X_train_prompt_words: ', X_train_prompt.shape)\n",
    "    print('X_train_prompt_pos: ', X_train_prompt_pos.shape)\n",
    "    print('X_train_readability: ', X_train_readability.shape)\n",
    "    print('X_train_ling: ', X_train_linguistic_features.shape)\n",
    "    print('X_train_attribute_rel: ', X_train_attribute_rel.shape)\n",
    "    print('Y_train: ', Y_train.shape)\n",
    "\n",
    "    print('================================')\n",
    "    print('X_dev_pos: ', X_dev_pos.shape)\n",
    "    print('X_dev_prompt_words: ', X_dev_prompt.shape)\n",
    "    print('X_dev_prompt_pos: ', X_dev_prompt_pos.shape)\n",
    "    print('X_dev_readability: ', X_dev_readability.shape)\n",
    "    print('X_dev_ling: ', X_dev_linguistic_features.shape)\n",
    "    print('X_dev_attribute_rel: ', X_dev_attribute_rel.shape)\n",
    "    print('Y_dev: ', Y_dev.shape)\n",
    "\n",
    "    print('================================')\n",
    "    print('X_test_pos: ', X_test_pos.shape)\n",
    "    print('X_test_prompt_words: ', X_test_prompt.shape)\n",
    "    print('X_test_prompt_pos: ', X_test_prompt_pos.shape)\n",
    "    print('X_test_readability: ', X_test_readability.shape)\n",
    "    print('X_test_ling: ', X_test_linguistic_features.shape)\n",
    "    print('X_test_attribute_rel: ', X_test_attribute_rel.shape)\n",
    "    print('Y_test: ', Y_test.shape)\n",
    "    print('================================')\n",
    "\n",
    "    # to torch tensor\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_train_pos),\n",
    "        torch.from_numpy(X_train_prompt),\n",
    "        torch.from_numpy(X_train_prompt_pos),\n",
    "        torch.from_numpy(X_train_linguistic_features),\n",
    "        torch.from_numpy(X_train_readability),\n",
    "        torch.from_numpy(Y_train)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dev_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_dev_pos),\n",
    "        torch.from_numpy(X_dev_prompt),\n",
    "        torch.from_numpy(X_dev_prompt_pos),\n",
    "        torch.from_numpy(X_dev_linguistic_features),\n",
    "        torch.from_numpy(X_dev_readability),\n",
    "        torch.from_numpy(Y_dev)\n",
    "    )\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.from_numpy(X_test_pos),\n",
    "        torch.from_numpy(X_test_prompt),\n",
    "        torch.from_numpy(X_test_prompt_pos),\n",
    "        torch.from_numpy(X_test_linguistic_features),\n",
    "        torch.from_numpy(X_test_readability),\n",
    "        torch.from_numpy(Y_test)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # build model\n",
    "    model = ProTACT(\n",
    "        len(pos_vocab), len(word_vocab), max_sentnum, max_sentlen,\n",
    "        X_train_readability.shape[1], X_train_linguistic_features.shape[1],\n",
    "        configs, Y_train.shape[1], num_heads, embed_table\n",
    "    )\n",
    "    for param in model.parameters():\n",
    "        print(param.requires_grad)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    criterion = LossFunctions(alpha=0.7)\n",
    "    # optimizer = torch.optim.RMSprop(\n",
    "    #     model.parameters(), lr=configs.LEARNING_RATE, alpha=0.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    evaluator = AllAttEvaluator(\n",
    "        dev_data['prompt_ids'], test_data['prompt_ids'],\n",
    "        dev_loader, test_loader,\n",
    "        Y_dev, Y_test, seed, device\n",
    "    )\n",
    "\n",
    "    evaluator.evaluate(model, -1, print_info=True)\n",
    "\n",
    "    custom_hist = CustomHistory()\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    #     start_time = time.time()\n",
    "\n",
    "    #     # train\n",
    "    #     model.train()\n",
    "    #     train_loss = 0.0\n",
    "    #     for batch_data in train_loader:\n",
    "    #         optimizer.zero_grad()\n",
    "    #         batch_data = [x.to(device) for x in batch_data]\n",
    "    #         inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "    #         outputs = model(*inputs)\n",
    "    #         # print(targets.dtype)\n",
    "    #         # print(outputs.dtype)\n",
    "    #         targets = targets.float()\n",
    "    #         loss = criterion(outputs, targets)\n",
    "    #         # print(type(loss))\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         train_loss += loss.item() * batch_data[0].size(0)\n",
    "    #     train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    #     # validate\n",
    "    #     model.eval()\n",
    "    #     val_loss = 0.0\n",
    "    #     with torch.no_grad():\n",
    "    #         for batch_data in dev_loader:\n",
    "    #             batch_data = [x.to(device) for x in batch_data]\n",
    "    #             inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "    #             outputs = model(*inputs)\n",
    "    #             # loss = criterion(outputs, targets)\n",
    "    #             loss = criterion(outputs, targets)\n",
    "    #             val_loss += loss.item() * batch_data[0].size(0)\n",
    "    #         val_loss /= len(dev_loader.dataset)\n",
    "\n",
    "    #     custom_hist.update(train_loss, val_loss)\n",
    "\n",
    "    #     # evaluate\n",
    "    #     tt_time = time.time() - start_time\n",
    "    #     print(f\"Training one epoch in {tt_time:.3f} s\")\n",
    "    #     evaluator.evaluate(model, epoch + 1)\n",
    "    #     print(f\"Train Loss: {train_loss:.4f} || Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # add tqdm\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1} - Training')\n",
    "        for batch_data in train_pbar:\n",
    "            # 각 layer의 이름과 파라미터 출력\n",
    "            # for name, child in model.named_children():\n",
    "            #     for param in child.parameters():\n",
    "            #         print(name, param)\n",
    "            optimizer.zero_grad()\n",
    "            batch_data = [x.to(device) for x in batch_data]\n",
    "            inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "            outputs = model(*inputs)\n",
    "            targets = targets.float()\n",
    "            loss = criterion(targets, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_data[0].size(0)\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_pbar.close()\n",
    "\n",
    "        # validate\n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # val_pdar = tqdm(dev_loader, desc=f'Epoch {epoch + 1} - Training')\n",
    "        # with torch.no_grad():\n",
    "        #     for batch_data in dev_loader:\n",
    "        #         batch_data = [x.to(device) for x in batch_data]\n",
    "        #         inputs, targets = batch_data[:-1], batch_data[-1]\n",
    "        #         outputs = model(*inputs)\n",
    "        #         loss = criterion(outputs, targets.float())\n",
    "        #         val_loss += loss.item() * batch_data[0].size(0)\n",
    "        #     val_loss /= len(dev_loader.dataset)\n",
    "        #     val_pdar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        # val_pdar.close()\n",
    "        # custom_hist.update(train_loss, val_loss)\n",
    "\n",
    "        # evaluate\n",
    "        tt_time = time.time() - start_time\n",
    "        print(f\"Training one epoch in {tt_time:.3f} s\")\n",
    "        evaluator.evaluate(model, epoch + 1)\n",
    "        # print(f\"Train Loss: {train_loss:.4f} || Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    evaluator.print_final_info()\n",
    "\n",
    "    '''# show the loss as the graph\n",
    "    fig, loss_graph = plt.subplots()\n",
    "    loss_graph.plot(custom_hist.train_loss,'y',label='train loss')\n",
    "    loss_graph.plot(custom_hist.val_loss,'r',label='val loss')\n",
    "    loss_graph.set_xlabel('epoch')\n",
    "    loss_graph.set_ylabel('loss')\n",
    "    plt.savefig(str('images/protact/test_prompt_'+ str(test_prompt_id) + '_seed_' + str(seed) + '_loss.png'))'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
